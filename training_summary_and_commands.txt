First full training (~90 images dataset - before train/test split):
-----------------------------
training 00014 30000step command:
bash -lc "conda activate mytimemachine && python scripts/train.py --dataset_type ffhq_aging --workers 2 --batch_size 2 --test_batch_size 2 --test_workers 2 --val_interval 500 --save_interval 1000 --start_from_encoded_w_plus --id_lambda 0.1 --lpips_lambda 0.1 --lpips_lambda_aging 0.1 --lpips_lambda_crop 0.6 --l2_lambda 0.25 --l2_lambda_aging 0.25 --l2_lambda_crop 1 --w_norm_lambda 0.005 --aging_lambda 5 --cycle_lambda 1 --input_nc 4 --target_age uniform_random --use_weighted_id_loss --checkpoint_path pretrained_models/sam_ffhq_aging.pt --train_dataset data/train --exp_dir experiments/full_training_run --adaptive_w_norm_lambda 7 --scheduler_type cosine --warmup_steps 500 --min_lr 1e-6 --grad_clip_norm 1.0 --nan_guard --extrapolation_start_step 3000 --extrapolation_prob_start 0.0 --extrapolation_prob_end 0.5 --train_encoder --max_steps 30000 | cat"
training 00015 35000-step decoder-only resume
bash -lc "cd /home/wczub/RND/AI_DEAGING/repos/mytimemachine && conda activate mytimemachine && PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python python scripts/train.py --dataset_type ffhq_aging --workers 2 --batch_size 2 --test_batch_size 2 --test_workers 2 --val_interval 500 --save_interval 1000 --start_from_encoded_w_plus --id_lambda 0.1 --lpips_lambda 0.1 --lpips_lambda_aging 0.1 --lpips_lambda_crop 0.6 --l2_lambda 0.25 --l2_lambda_aging 0.25 --l2_lambda_crop 1 --w_norm_lambda 0.005 --aging_lambda 5 --cycle_lambda 1 --input_nc 4 --target_age uniform_random --use_weighted_id_loss --checkpoint_path pretrained_models/sam_ffhq_aging.pt --train_dataset data/train --exp_dir experiments/full_training_run --adaptive_w_norm_lambda 7 --scheduler_type cosine --warmup_steps 500 --min_lr 1e-6 --grad_clip_norm 1.0 --nan_guard --resume_checkpoint experiments/full_training_run/00014/checkpoints/iteration_28000.pt --train_decoder --max_steps 35000 --learning_rate 0.0001"


Second full training (~180 images dataset - before train/test split):
-----------------------------
Changes we did before next try:
### TL;DR
- Increase personalization pressure and reduce over-smoothing.
- Start with adaptive_w_norm_lambda ≈ 20 and id_lambda ≈ 0.3.
- Strengthen crop perceptual loss; lower L2s; slightly loosen latent regularization.
- For decoder-only, use a smaller LR and extend steps.

### Targeted hyperparameter changes
- Adaptive W-Norm: increase from 7 → 15–25 (start at 20). Stronger personalization.
- Identity loss: increase from 0.1 → 0.3–0.4 (start at 0.3).
- Latent norm: reduce w_norm_lambda 0.005 → 0.003 (allows deviation for personalization).
- Perceptual on crop: increase lpips_lambda_crop 0.6 → 0.8–1.0 (start at 0.8).
- L2 losses: reduce l2_lambda 0.25 → 0.1 and l2_lambda_crop 1.0 → 0.5 (less smoothing).
- Cycle: increase cycle_lambda 1 → 2 (helps keep person-specific traits while editing).
- Decoder-only LR: reduce learning_rate 1e-4 → 5e-5; keep cosine, lower min_lr to 5e-7.
- Steps: extend decoder-only by +10k–25k (e.g., to 50k–60k total).
- Keep: use_weighted_id_loss, grad_clip_norm, nan_guard, warmup.

If still too “global” after these, push adaptive_w_norm_lambda to 25–30 and id_lambda to 0.4–0.5.

- training 00016 command - Stage 1 (encoder+decoder), stronger personalization:
```bash
bash -lc "cd /home/wczub/RND/AI_DEAGING/repos/mytimemachine && conda activate mytimemachine && PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python python scripts/train.py --dataset_type ffhq_aging --workers 2 --batch_size 2 --test_batch_size 2 --test_workers 2 --val_interval 500 --save_interval 1000 --start_from_encoded_w_plus --id_lambda 0.3 --lpips_lambda 0.1 --lpips_lambda_aging 0.1 --lpips_lambda_crop 0.8 --l2_lambda 0.1 --l2_lambda_aging 0.25 --l2_lambda_crop 0.5 --w_norm_lambda 0.003 --aging_lambda 5 --cycle_lambda 2 --input_nc 4 --target_age uniform_random --use_weighted_id_loss --checkpoint_path pretrained_models/sam_ffhq_aging.pt --train_dataset data/train --exp_dir experiments/full_training_run --adaptive_w_norm_lambda 20 --scheduler_type cosine --warmup_steps 500 --min_lr 5e-7 --grad_clip_norm 1.0 --nan_guard --extrapolation_start_step 3000 --extrapolation_prob_start 0.0 --extrapolation_prob_end 0.5 --train_encoder --max_steps 30000 | cat"
```

- training 00018 command - Stage 2 (decoder-only), lower LR and more steps.:
```bash
bash -lc "cd /home/wczub/RND/AI_DEAGING/repos/mytimemachine && conda activate mytimemachine && PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python python scripts/train.py --dataset_type ffhq_aging --workers 2 --batch_size 2 --test_batch_size 2 --test_workers 2 --val_interval 500 --save_interval 1000 --start_from_encoded_w_plus --id_lambda 0.3 --lpips_lambda 0.1 --lpips_lambda_aging 0.1 --lpips_lambda_crop 0.8 --l2_lambda 0.1 --l2_lambda_aging 0.25 --l2_lambda_crop 0.5 --w_norm_lambda 0.003 --aging_lambda 5 --cycle_lambda 2 --input_nc 4 --target_age uniform_random --use_weighted_id_loss --checkpoint_path pretrained_models/sam_ffhq_aging.pt --train_dataset data/train --exp_dir experiments/full_training_run --adaptive_w_norm_lambda 20 --scheduler_type cosine --warmup_steps 500 --min_lr 5e-7 --grad_clip_norm 1.0 --nan_guard --resume_checkpoint experiments/full_training_run/00016/checkpoints/iteration_30000.pt --train_decoder --max_steps 50000 --learning_rate 0.00005 | cat"
```

### Extra tips
- If faces still look a bit generic: push adaptive_w_norm_lambda → 25–30 and id_lambda → 0.4–0.5.
- If images turn soft: raise lpips_lambda_crop to 1.0 and/or lightly increase learning_rate back to 7e-5 for a short window, then anneal.
- If artifacts appear: slightly raise w_norm_lambda back toward 0.004 and keep grad clipping.


Third full training (still ~180 images dataset - no changes since previous training)
-------------------------------------
Goal: push stronger personalization while reducing smoothing.
Changes vs 00016/00018:
- adaptive_w_norm_lambda: 28 (↑ from 20)
- id_lambda: 0.45 (↑ from 0.3)
- w_norm_lambda: 0.002 (↓ from 0.003)
- lpips_lambda_crop: 1.0 (↑ from 0.8)
- l2_lambda: 0.05 (↓ from 0.1), l2_lambda_crop: 0.2 (↓ from 0.5)
- cycle_lambda: 3 (↑ from 2)
- warmup_steps: 800 (↑ from 500), min_lr: 3e-7 (↓ from 5e-7)
- extrapolation: start 2000, prob_end 0.7 (↑ from 0.5)
- Stage 2 only: w_norm_lambda_decoder_scale 0.3 (↓ from 0.5), keep aging_lambda_decoder_scale 0.5
- Stage 2 max_steps: 60000 (↑ from 50000)

- training 00019 command - Stage 1 (encoder+decoder), 30k steps, aggressive personalization
```bash
bash -lc "cd /home/wczub/RND/AI_DEAGING/repos/mytimemachine && conda activate mytimemachine && PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python python scripts/train.py --dataset_type ffhq_aging --workers 2 --batch_size 2 --test_batch_size 2 --test_workers 2 --val_interval 500 --save_interval 1000 --start_from_encoded_w_plus --id_lambda 0.45 --lpips_lambda 0.1 --lpips_lambda_aging 0.1 --lpips_lambda_crop 1.0 --l2_lambda 0.05 --l2_lambda_aging 0.25 --l2_lambda_crop 0.2 --w_norm_lambda 0.002 --aging_lambda 5 --cycle_lambda 3 --input_nc 4 --target_age uniform_random --use_weighted_id_loss --checkpoint_path pretrained_models/sam_ffhq_aging.pt --train_dataset data/train --exp_dir experiments/full_training_run --adaptive_w_norm_lambda 28 --scheduler_type cosine --warmup_steps 800 --min_lr 3e-7 --grad_clip_norm 1.0 --nan_guard --extrapolation_start_step 2000 --extrapolation_prob_start 0.0 --extrapolation_prob_end 0.7 --train_encoder --max_steps 30000 | cat"
```

- training 00020 command - Stage 2 (decoder-only), resume from NEW 30k checkpoint, to 60k steps
```bash
bash -lc "cd /home/wczub/RND/AI_DEAGING/repos/mytimemachine && conda activate mytimemachine && PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python python scripts/train.py --dataset_type ffhq_aging --workers 2 --batch_size 2 --test_batch_size 2 --test_workers 2 --val_interval 500 --save_interval 1000 --start_from_encoded_w_plus --id_lambda 0.45 --lpips_lambda 0.1 --lpips_lambda_aging 0.1 --lpips_lambda_crop 1.0 --l2_lambda 0.05 --l2_lambda_aging 0.25 --l2_lambda_crop 0.2 --w_norm_lambda 0.002 --w_norm_lambda_decoder_scale 0.3 --aging_lambda 5 --aging_lambda_decoder_scale 0.5 --cycle_lambda 3 --input_nc 4 --target_age uniform_random --use_weighted_id_loss --checkpoint_path pretrained_models/sam_ffhq_aging.pt --train_dataset data/train --exp_dir experiments/full_training_run --adaptive_w_norm_lambda 28 --scheduler_type cosine --warmup_steps 800 --min_lr 3e-7 --grad_clip_norm 1.0 --nan_guard --resume_checkpoint experiments/full_training_run/00019/checkpoints/iteration_30000.pt --train_decoder --max_steps 60000 --learning_rate 0.00005 | cat"
```

The description of tensorboard after the training:
train/id_improve_cycle

Stage-1: rises fast from negative to just below 0.0.

Stage-2: keeps creeping up, stabilizes ~0.00→+0.03 (near the plot’s top band).

Final: ≈0.02–0.03 (best/near-best among runs).

Vs prev.: equal or slightly higher at convergence.

train/id_improve_real

Stage-1: monotonic rise from negative to ≈-0.05.

Stage-2: continues upward, plateaus ~-0.01→+0.02.

Final: ≈0.00–0.02.

Vs prev.: slightly better at the end (less negative / nudges positive).

train/loss (overall)

Stage-1: strong decay ~1.6 → ~0.25.

Stage-2: gentle decay ~0.25 → ~0.16–0.18, low noise.

Final: ≈0.16–0.18 (smooth).

Vs prev.: marginally lower or on par.

train/loss_aging_cycle

Stage-1: drops to ~1e-4–5e-4 quickly.

Stage-2: stays near floor with small, infrequent blips.

Final: ≈1e-4 (very low).

Vs prev.: equal or slightly cleaner (earlier stabilization).

train/loss_aging_real

Stage-1: decays to ~1e-3 with oscillations.

Stage-2: some spikes early in the phase but damped; baseline keeps sliding down.

Final: ≈2e-4–5e-4 with moderate jitter.

Vs prev.: fewer/lower late spikes; slightly lower floor.

train/loss_cycle

Stage-1: falls ~0.7 → ~0.22–0.24.

Stage-2: further down then flat ~0.12–0.15.

Final: ≈0.12–0.15.

Vs prev.: equal or a touch better at the end.

train/loss_id_cycle

Stage-1: steady decay ~0.55 → ~0.14–0.16.

Stage-2 (visible portion): small further drop, then flat; no instability.

Final (approx.): ≈0.10–0.14 (trend consistent with other heads).

Vs prev.: similar or slightly lower.

train/loss_id_real

Stage-1: falls ~0.55 → ~0.18–0.20.

Stage-2: gradual decline, stable ~0.06–0.08.

Final: ≈0.06–0.07.

Vs prev.: lower at convergence.

train/loss_l2_crop

Stage-1: decays ~0.18 → ~0.07.

Stage-2: flattens ~0.05–0.06, low variance.

Final: ≈0.05–0.06.

Vs prev.: slightly lower.

train/loss_l2_cycle

Stage-1: decays ~0.17 → ~0.08–0.09.

Stage-2: stabilizes ~0.06–0.07.

Final: ≈0.06–0.07.

Vs prev.: marginally better.

train/loss_l2_real

Stage-1: decays ~0.17 → ~0.09–0.10.

Stage-2: flattens ~0.06–0.07 with mild jitter.

Final: ≈0.06–0.07.

Vs prev.: slightly lower.

train/loss_lpips_crop

Stage-1: falls ~0.34 → ~0.22–0.24.

Stage-2: gentle slide, then flat ~0.19–0.21.

Final: ≈0.19–0.21.

Vs prev.: similar or a bit better; no late degradation.

train/loss_lpips_cycle

Stage-1: falls ~0.33 → ~0.21–0.23.

Stage-2: stabilizes ~0.18–0.20.

Final: ≈0.18–0.20.

Vs prev.: slightly better.

train/loss_lpips_real

Stage-1: falls ~0.33 → ~0.22–0.24.

Stage-2: flat ~0.18–0.20, mild noise.

Final: ≈0.18–0.20.

Vs prev.: slightly lower end-point.

train/loss_real (adversarial, generator side)

Stage-1: decays ~0.95 → ~0.34–0.36.

Stage-2: settles ~0.26–0.28, small oscillations (typical GAN noise).

Final: ≈0.26–0.27.

Vs prev.: equal or a hair lower.

train/loss_w_norm_cycle (weight-norm penalty)

Stage-1: decays ~70 → ~26–27.

Stage-2: slight upward drift then plateaus ~27–28.

Final: ≈27–28.

Vs prev.: higher plateau (prev. best ≈24–26). Weights are a bit larger but stable.

train/loss_w_norm_real (weight-norm penalty)

Stage-1: decays ~70 → ~27–28.

Stage-2: mild upward creep, then flat ~28–29.

Final: ≈28–29.

Vs prev.: higher than the previous best (by ~2–4 units), yet not diverging.

One-line summary

This run improves or matches previous runs on identity, pixel, perceptual, cycle, aging, and adversarial losses, converging smoothly through Stage-2; the only trade-off is a slightly higher weight-norm plateau (larger but stable weights).

Visual quality-check

Even tho the tensorboard logs shows the improvements - visually the effects are poorer than from Second Full Training. The Second Full Training was visually cleaner and better match the actor without any artifacts. I would go back to previous settings as the starting point.

Fourth training on expanded dataset (>240 images; validation enabled)
-------------------------------------
Notes:
- Train: data/train; Test: data/test (expanded to >240 images each).
- Validation and test logging are enabled in `training/coach_aging_orig.py` (val_interval=500).
- Hyperparameters replicate the best prior runs: Stage 1 ≈ 00016, Stage 2 ≈ 00018.

training 00025 - Stage 1 — replicate 00016 (encoder+decoder, 30k steps)
```bash
bash -lc "cd /home/wczub/RND/AI_DEAGING/repos/mytimemachine && conda activate mytimemachine && PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python python scripts/train.py --dataset_type ffhq_aging --workers 2 --batch_size 2 --test_batch_size 2 --test_workers 2 --val_interval 500 --save_interval 1000 --start_from_encoded_w_plus --id_lambda 0.3 --lpips_lambda 0.1 --lpips_lambda_aging 0.1 --lpips_lambda_crop 0.8 --l2_lambda 0.1 --l2_lambda_aging 0.25 --l2_lambda_crop 0.5 --w_norm_lambda 0.003 --aging_lambda 5 --cycle_lambda 2 --input_nc 4 --target_age uniform_random --use_weighted_id_loss --checkpoint_path pretrained_models/sam_ffhq_aging.pt --train_dataset data/train --exp_dir experiments/full_training_run --adaptive_w_norm_lambda 20 --scheduler_type cosine --warmup_steps 500 --min_lr 5e-7 --grad_clip_norm 1.0 --nan_guard --extrapolation_start_step 3000 --extrapolation_prob_start 0.0 --extrapolation_prob_end 0.5 --train_encoder --max_steps 30000 | cat"
```

training 00026 - Stage 2 — replicate 00018 (decoder-only, 50k steps)
Run after Stage 1 completes; replace 000XX with the new Stage 1 folder name.
```bash
bash -lc "cd /home/wczub/RND/AI_DEAGING/repos/mytimemachine && conda activate mytimemachine && PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python python scripts/train.py --dataset_type ffhq_aging --workers 2 --batch_size 2 --test_batch_size 2 --test_workers 2 --val_interval 500 --save_interval 1000 --start_from_encoded_w_plus --id_lambda 0.3 --lpips_lambda 0.1 --lpips_lambda_aging 0.1 --lpips_lambda_crop 0.8 --l2_lambda 0.1 --l2_lambda_aging 0.25 --l2_lambda_crop 0.5 --w_norm_lambda 0.003 --aging_lambda 5 --cycle_lambda 2 --input_nc 4 --target_age uniform_random --use_weighted_id_loss --checkpoint_path pretrained_models/sam_ffhq_aging.pt --train_dataset data/train --exp_dir experiments/full_training_run --adaptive_w_norm_lambda 20 --scheduler_type cosine --warmup_steps 500 --min_lr 5e-7 --grad_clip_norm 1.0 --nan_guard --resume_checkpoint experiments/full_training_run/000XX/checkpoints/iteration_30000.pt --train_decoder --max_steps 50000 --learning_rate 0.00005 | cat"
```

Fifth training (2-staged) — low-risk stability tweaks
-------------------------------------
Goal: replicate fourth training setup via `scripts/train_two_stage.sh` with:
- Stage 1 steps: 25000 (was 30000)
- Stage 2 learning rate: 3e-5 (was 5e-5)
- Cycle lambda Stage 1: 1.5 (was 2)

Script deltas in `scripts/train_two_stage.sh`:
- `MAX_STEPS_S1=25000`
- `LEARNING_RATE_S2=3e-5`
- `CYCLE_LAMBDA_S1=1.5`
- Resume logic generalized to find `iteration_${MAX_STEPS_S1}.pt`

Run both stages automatically:
```bash
bash -lc "cd /home/wczub/RND/AI_DEAGING/repos/mytimemachine && conda activate mytimemachine && ./scripts/train_two_stage.sh | cat"
``` 

If running stages manually, example commands:
- Stage 1 (25k):
```bash
bash -lc "cd /home/wczub/RND/AI_DEAGING/repos/mytimemachine && conda activate mytimemachine && PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python python scripts/train.py --dataset_type ffhq_aging --workers 2 --batch_size 2 --test_batch_size 2 --test_workers 2 --val_interval 500 --save_interval 1000 --start_from_encoded_w_plus --id_lambda 0.3 --lpips_lambda 0.1 --lpips_lambda_aging 0.1 --lpips_lambda_crop 0.8 --l2_lambda 0.1 --l2_lambda_aging 0.25 --l2_lambda_crop 0.5 --w_norm_lambda 0.003 --aging_lambda 5 --cycle_lambda 1.5 --input_nc 4 --target_age uniform_random --use_weighted_id_loss --checkpoint_path pretrained_models/sam_ffhq_aging.pt --train_dataset data/train --exp_dir experiments/full_training_run --adaptive_w_norm_lambda 20 --scheduler_type cosine --warmup_steps 500 --min_lr 5e-7 --grad_clip_norm 1.0 --nan_guard --val_disable_aging --val_deterministic --val_max_batches 2 --val_start_step 2000 --extrapolation_start_step 3000 --extrapolation_prob_start 0.0 --extrapolation_prob_end 0.5 --train_encoder --max_steps 25000 | cat"
```

- Stage 2 (resume 25k, 50k steps, LR 3e-5):
```bash
bash -lc "cd /home/wczub/RND/AI_DEAGING/repos/mytimemachine && conda activate mytimemachine && PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python python scripts/train.py --dataset_type ffhq_aging --workers 2 --batch_size 2 --test_batch_size 2 --test_workers 2 --val_interval 500 --save_interval 1000 --start_from_encoded_w_plus --id_lambda 0.3 --lpips_lambda 0.1 --lpips_lambda_aging 0.1 --lpips_lambda_crop 0.8 --l2_lambda 0.1 --l2_lambda_aging 0.25 --l2_lambda_crop 0.5 --w_norm_lambda 0.003 --w_norm_lambda_decoder_scale 0.5 --aging_lambda 5 --aging_lambda_decoder_scale 0.5 --cycle_lambda 1.5 --input_nc 4 --target_age uniform_random --use_weighted_id_loss --checkpoint_path pretrained_models/sam_ffhq_aging.pt --train_dataset data/train --exp_dir experiments/full_training_run --adaptive_w_norm_lambda 20 --scheduler_type cosine --warmup_steps 500 --min_lr 5e-7 --grad_clip_norm 1.0 --nan_guard --val_disable_aging --val_deterministic --val_max_batches 2 --val_start_step 2000 --resume_checkpoint experiments/full_training_run/000XX/checkpoints/iteration_25000.pt --train_decoder --max_steps 50000 --learning_rate 0.00003 | cat"
```

Sixth training (2-staged) — interpolation personalization via NN-ID regularizer
-------------------------------------
Goal: keep the Fifth setup but add a nearest-neighbor identity regularizer during interpolation only, to better personalize to the target actor near age ~40. Disable extrapolation to remain in-domain.

Changes vs Fifth:
- Use new coach `training.coach_aging_orig_nn.Coach` via `COACH=orig_nn`.
- Add `--nearest_neighbor_id_loss_lambda 0.1`.
- Disable extrapolation: `--extrapolation_start_step 1000000000` (probability flags kept but inactive).
- All other hypers identical to Fifth.

Two-stage script (updated): `scripts/train_two_stage.sh`
- COACH="orig_nn"
- NEAREST_NEIGHBOR_ID_LAMBDA=0.1
- EXTRAPOLATION_START_STEP_S1=1000000000

Run both stages automatically:
```bash
bash -lc "cd /home/wczub/RND/AI_DEAGING/repos/mytimemachine && conda activate mytimemachine && ./scripts/train_two_stage.sh | cat"
```

Results and notes (00031 → 00032):
- Stage 1 — 00031 (encoder+decoder, 25k steps, COACH=orig_nn)
  - Key deltas vs Fifth: interpolation-only (extrapolation disabled), nearest-neighbor ID loss λ=0.1; otherwise identical hypers to Fifth.
  - Important hypers: id_lambda=0.3, lpips_lambda_crop=0.8, l2_lambda=0.1, l2_lambda_crop=0.5, w_norm_lambda=0.003, cycle_lambda=1.5, adaptive_w_norm_lambda=20, warmup_steps=500, min_lr=5e-7; validation enabled (val_interval=500, deterministic, val_max_batches=2, val_disable_aging).
  - Best val loss: 1.022 at steps 19,500 and 24,500 (final 25,000 → 1.032).
  - Metrics at best (≈24,500): loss_id_real≈0.4135, loss_l2_real≈0.0887, loss_lpips_real≈0.3199, loss_lpips_crop≈0.2153, loss_l2_crop≈0.0815, loss_w_norm_real≈45.78; total loss≈1.022.

- Stage 2 — 00032 (decoder-only, resume 25k → 50k, LR=3e-5)
  - Settings: train_decoder only; w_norm_lambda_decoder_scale=0.5, aging_lambda_decoder_scale=0.5; nearest-neighbor ID loss kept at λ=0.1; extrapolation disabled.
  - Best val loss: 0.868 at step 42,000 (final 50,000 → 0.875; early best at 25,000 was 0.888).
  - Metrics at best (42,000): loss_id_real≈0.3993, loss_l2_real≈0.0877, loss_lpips_real≈0.3155, loss_lpips_crop≈0.2114, loss_l2_crop≈0.0805, loss_w_norm_real≈45.69; total loss≈0.868. Improvements after ~42k were marginal/oscillatory.

- Visual assessment (manual): Stage 1 quickly reached high target similarity; Stage 2 added little visible improvement. Stage-1 outputs were almost as good as late Stage-2.

Takeaways / next steps:
- Extend Stage 1 duration (e.g., 30k–40k) to let encoder+decoder converge closer to the sweet spot before switching to decoder-only.
- Optionally shorten Stage 2 or run it with a brief LR plateau then anneal; consider modestly increasing NN-ID λ during Stage 1 only.
- Keep interpolation-only (no extrapolation) and current regularization; weight-norm remained stable in both stages.



Seventh training (2-staged) — identity preservation focus
-------------------------------------
Goal: strengthen identity preservation without reintroducing artifacts. Build on the Fifth/Sixth setup (stable visuals), extend Stage 1 to solidify identity, lightly refine in Stage 2.

Plan highlights:
- Coach: COACH=orig_nn (nearest-neighbor ID regularizer active on interpolation only)
- Disable extrapolation (in-domain only): extrapolation_start_step=1000000000
- Keep stable regs: adaptive_w_norm_lambda=20, w_norm_lambda=0.003, cycle_lambda=1.5, lpips_lambda_crop=0.8, l2_lambda=0.1, l2_lambda_crop=0.5

Stage 1 (encoder+decoder)
- Steps: 35,000
- id_lambda: 0.35
- nearest_neighbor_id_loss_lambda: 0.2
- Other hypers: as in Fifth/Sixth; validation enabled as before

Stage 2 (decoder-only)
- Steps: 45,000 (resume from 35k)
- learning_rate: 3e-5
- id_lambda: 0.3
- nearest_neighbor_id_loss_lambda: 0.05
- w_norm_lambda_decoder_scale: 0.5, aging_lambda_decoder_scale: 0.5

How to run (updated two-stage script):
```bash
bash -lc "cd /home/wczub/RND/AI_DEAGING/repos/mytimemachine && conda activate mytimemachine && ./scripts/train_two_stage.sh | cat"
```

Monitoring
- Track identity: train/id_improve_real should rise toward ≥0.02 by ~30–35k; train/loss_id_real should trend ≤0.40 late S1 and ≤0.38 in S2.
- Visual checks: preserve facial structure (eyes, mouth asymmetry, moles), avoid generic-face drift.
- If identity plateaus late in S1: briefly raise nearest_neighbor_id_loss_lambda to 0.25 for final 3–5k of S1, then keep 0.05 in S2.

Final results of this seventh training:

Setup recap:
- Coach: orig_nn; extrapolation disabled
- Stage 1: id_lambda=0.35, nearest_neighbor_id_loss_lambda=0.2
- Stage 2: id_lambda=0.3, nearest_neighbor_id_loss_lambda=0.05, learning_rate=3e-5
- Shared: adaptive_w_norm_lambda=20, w_norm_lambda=0.003; crop LPIPS/L2 enabled

Key validation (test) checkpoints:
- Step 10k: loss_id_real≈0.409, lpips_real≈0.328, l2_real≈0.103, w_norm_real≈51.25, total≈1.134
- Step 21k: loss_id_real≈0.390, lpips_real≈0.316, l2_real≈0.095, w_norm_real≈45.02, total≈1.034
- Step 45k (end): loss_id_real≈0.399, lpips_real≈0.316, l2_real≈0.099, w_norm_real≈42.73, total≈0.891

End-of-run training snapshot (step 45k):
- train: loss_id_real≈0.105, lpips_real≈0.172, l2_real≈0.024, w_norm_real≈41.04, total≈0.412

What improved within this run:
- Identity: improved during Stage 1 (0.409→0.390). Stage 2 held identity roughly steady (0.399 at 45k), similar to prior runs where Stage 2 adds limited identity gains.
- Perceptual/pixel: both tightened from 10k to 21k and remained strong through 45k (LPIPS stayed ≈0.316; L2 ≈0.099).
- Regularization: w_norm_real decayed consistently (≈51→45→42.7), signaling healthier latent magnitudes and stable convergence.

Notes on NN-ID and stability:
- nearest_neighbor_id_loss remained active throughout (train peaks ≈0.45 without instability), supporting personalization near target ages.

Recommendations:
- Allocate more budget to Stage 1 (e.g., 38–40k) and shorten Stage 2 (e.g., 40–43k). Most identity gains manifest during Stage 1.
- If identity plateaus late Stage 1, briefly bump NN-ID λ to 0.25 for the last 3–5k, then revert to 0.05 for Stage 2.
- Keep current regularization and interpolation-only setting; both correlate with stable, artifact-free training here.

Eighth training (2-staged, start from scratch on second computer) — Seventh + medium improvements
-------------------------------------
Goal: Start Stage 1 from step 0 with COACH=orig_nn and NN-ID regularizer, extrapolation disabled. Apply phased tweaks in Stage 1 (mid-run LPIPS/L2 tightening; late NN-ID bump). Automatically continue to Stage 2 when Stage 1 reaches 35k.

Key settings (Stage 1 → Stage 2):
- Coach: orig_nn
- Stage 1 steps: 35,000; Stage 2 steps: 45,000
- NN-ID λ: 0.2 (S1 baseline), 0.25 for 30k→35k, 0.05 in S2
- id_lambda: 0.35 (S1), 0.30 (S2)
- cycle_lambda: 1.5; adaptive_w_norm_lambda: 20; w_norm_lambda: 0.003
- lpips_lambda_crop: 0.8 baseline; 0.9 for 20k→30k
- l2_lambda: 0.1 baseline; 0.08 for 20k→30k; restore at 30k
- l2_lambda_crop: 0.5 baseline; 0.4 for 20k→30k; restore at 30k
- Extrapolation disabled: extrapolation_start_step=1000000000
- Learning rate S2: 3e-5

Run both stages automatically from scratch:
```bash
bash -lc "cd /home/wczub/RND/AI_DEAGING/repos/mytimemachine && conda activate mytimemachine && ./scripts/train_two_stage.sh | cat"
```

Notes:
- Stage 1 runs in three phases: 0→20k (baseline), 20k→30k (tighten LPIPS/L2), 30k→35k (NN-ID bump, restore LPIPS/L2). The script locates 20k/30k checkpoints automatically.
- Stage 2 will automatically search for the newest `iteration_35000.pt` under `experiments/full_training_run/**/checkpoints/` and resume from it.
- Validation is deterministic, every 500 steps, with small batch cap for speed.

Monitoring
- Track identity: train/id_improve_real should rise toward ≥0.02 by ~30–35k; train/loss_id_real should trend ≤0.40 late S1 and ≤0.38 in S2.
- Visual checks: preserve facial structure (eyes, mouth asymmetry, moles), avoid generic-face drift.
- If identity plateaus late in S1: briefly raise nearest_neighbor_id_loss_lambda to 0.25 for final 3–5k of S1, then keep 0.05 in S2.

Final results of this eighth training:

Setup recap:
- Coach: orig_nn; extrapolation disabled
- Stage 1: 0→20k baseline; 20k→30k lpips_crop=0.9, l2=0.08, l2_crop=0.4; 30k→35k NN-ID λ=0.25, restore lpips/L2
- Stage 2: 35k→45k; id_lambda=0.30; NN-ID λ=0.05; learning_rate=3e-5
- Shared: adaptive_w_norm_lambda=20; w_norm_lambda=0.003; cycle_lambda=1.5; crop LPIPS/L2 enabled

Key validation (test) checkpoints:
- 10k (S1 P1): loss_id_real≈0.409, lpips_real≈0.328, l2_real≈0.103, w_norm_real≈51.25, total≈1.134
- 21k (S1 P2 early): loss_id_real≈0.390, lpips_real≈0.316, l2_real≈0.095, w_norm_real≈45.02, total≈1.034
- 45k (end S2): loss_id_real≈0.284, lpips_real≈0.305, l2_real≈0.059, w_norm_real≈45.56, total≈0.722

Representative training snapshots (late S2 window):
- 44.55k: loss_id_real≈0.104, lpips_real≈0.250, l2_real≈0.064, w_norm_real≈48.40, total≈0.507
- 45.00k: loss_id_real≈0.080, lpips_real≈0.195, l2_real≈0.025, w_norm_real≈46.16, total≈0.401

Trends and observations:
- Identity: improved substantially during S1 (0.409→0.390); further improved by S2 end on test (≈0.284). Train identity tightened steadily late S2 (≈0.08–0.15), indicating strong personalization without overfit spikes.
- Perceptual/pixel: mid-run tightening (P2) reduced LPIPS/L2; by 45k test LPIPS≈0.305 and L2≈0.059, matching or slightly better than prior bests.
- Regularization: w_norm_real dropped from ~51 at 10k to mid‑40s by 21k and stayed ~45–46 at 45k, suggesting stable edits with less drift than earlier stages; occasional train spikes to ~51 were transient.
- NN-ID stability: nearest_neighbor_id_loss active (≈0.21–0.37 range) with the 30k→35k bump; no instability observed.

Qualitative notes:
- Visuals in late S1 and S2 were crisp with preserved micro‑identity (eyes/mouth), consistent with lower crop LPIPS/L2; no recurrent artifacts observed near the NN‑ID bump.

Actionable follow‑ups:
- Consider extending S1 to 38–40k when compute allows; most identity gains happened in S1.
- Keep S2 short (e.g., 40–43k target) or use a brief LR plateau at 3e‑5 then anneal.
- If w‑norm >52 coincides with any artifacts in a future run, temporarily raise w_norm_lambda to 0.0035 for 1–2k steps, then revert.


Ninth training (2-staged) — identity focus with age-aware contrastive loss
-------------------------------------
Rationale:
- Add an impostor-only, age-aware contrastive ID loss that repels away from different-identity embeddings drawn from an FFHQ bank, but only within an age window. Use a higher temperature and moderate K to avoid over-canonical pose/lighting. Keep NN-ID alongside contrastive (pull vs. repel) and maintain the stable regularization mix from prior runs.

Key settings:
- Coach: orig_nn
- Contrastive: λ S1=0.04, S2=0.02; K=32; τ=0.12; age window=[35,45]; neighbor_radius=0
- NN-ID: λ S1=0.10, S2=0.05
- In-domain only (extrapolation disabled); cosine scheduler with warmup; nan-guard and grad clipping on

Stage 1 — 30k steps (encoder+decoder)
```bash
bash -lc "cd /home/wczub/RND/AI_DEAGING/repos/mytimemachine && conda activate mytimemachine && PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python python scripts/train.py \
  --coach orig_nn \
  --dataset_type ffhq_aging --train_dataset data/train \
  --exp_dir experiments/full_training_run \
  --start_from_encoded_w_plus --train_encoder \
  --batch_size 2 --workers 2 \
  --target_age uniform_random \
  --max_steps 30000 --image_interval 100 \
  --board_interval 50 --val_interval 500 --val_deterministic --val_max_batches 2 --val_start_step 2000 --val_disable_aging \
  --id_lambda 0.3 --lpips_lambda 0.1 --lpips_lambda_aging 0.1 --lpips_lambda_crop 0.8 \
  --l2_lambda 0.1 --l2_lambda_aging 0.25 --l2_lambda_crop 0.5 \
  --w_norm_lambda 0.003 --aging_lambda 5 --cycle_lambda 1.5 \
  --adaptive_w_norm_lambda 20 \
  --scheduler_type cosine --warmup_steps 500 --min_lr 5e-7 --grad_clip_norm 1.0 --nan_guard \
  --extrapolation_start_step 1000000000 \
  --nearest_neighbor_id_loss_lambda 0.1 \
  --contrastive_id_lambda 0.04 \
  --mb_index_path banks/ffhq_ir50_age_5y.pt \
  --mb_k 32 \
  --mb_apply_min_age 35 --mb_apply_max_age 45 \
  --mb_bin_neighbor_radius 0 \
  --mb_temperature 0.12 | cat"
```

Stage 2 — resume to 60k steps (decoder-only)
```bash
bash -lc "cd /home/wczub/RND/AI_DEAGING/repos/mytimemachine && conda activate mytimemachine && PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python python scripts/train.py \
  --coach orig_nn \
  --dataset_type ffhq_aging --train_dataset data/train \
  --exp_dir experiments/full_training_run \
  --start_from_encoded_w_plus --train_decoder \
  --batch_size 2 --workers 2 \
  --target_age uniform_random \
  --max_steps 60000 --image_interval 100 \
  --board_interval 50 --val_interval 500 --val_deterministic --val_max_batches 2 \
  --id_lambda 0.3 --lpips_lambda 0.1 --lpips_lambda_aging 0.1 --lpips_lambda_crop 0.8 \
  --l2_lambda 0.1 --l2_lambda_aging 0.25 --l2_lambda_crop 0.5 \
  --w_norm_lambda 0.003 --w_norm_lambda_decoder_scale 0.5 \
  --aging_lambda 5 --aging_lambda_decoder_scale 0.5 \
  --cycle_lambda 1.5 \
  --adaptive_w_norm_lambda 20 \
  --scheduler_type cosine --warmup_steps 500 --min_lr 5e-7 --grad_clip_norm 1.0 --nan_guard \
  --resume_checkpoint experiments/full_training_run/000XX/checkpoints/iteration_30000.pt \
  --nearest_neighbor_id_loss_lambda 0.05 \
  --contrastive_id_lambda 0.02 \
  --mb_index_path banks/ffhq_ir50_age_5y.pt \
  --mb_k 32 \
  --mb_apply_min_age 35 --mb_apply_max_age 45 \
  --mb_bin_neighbor_radius 0 \
  --mb_temperature 0.12 | cat"
```

Monitoring:
- Identity: train/id_improve_real should trend toward ≥0 late S1; loss_id_real ≤0.40 late S1 and ≤0.38 in S2
- Contrastive: train/loss_contrastive_id non-zero when ages in [35,45]; train/mb_applied_ratio ~0.5–1.0 depending on batch ages; should decay gradually
- Regularization: loss_w_norm_real should settle in low‑40s without spikes; nan-guard should prevent non-finite steps

Results (mid-run; current 00058):
- Configuration (from opt.json): contrastive_id_lambda=0.04, mb_k=32, τ=0.12, age window=[35,45]; nearest_neighbor_id_loss_lambda=0.10; image_interval=100; validation deterministic every 500 steps (val_max_batches=2; val_disable_aging)
- Train snapshots (12k→21k):
  - loss_id_real ≈ 0.28–0.33 early-mid; best train loss milestones improved to ≈0.95 at 16.5k (per checkpoints/timestamp.txt “Best” markers)
  - loss_lpips_real ≈ 0.29–0.31; loss_l2_real ≈ 0.03–0.06
  - loss_w_norm_real decayed from ~48–52 into low‑40s by ~19–21k; stable
  - Contrastive active values: loss_contrastive_id ≈ 3.6–4.3 when applied; 0.0 when mask off; mb_applied_ratio commonly 0.5–1.0, occasionally 0.0 when ages fall outside [35,45]
  - NN‑ID active: nearest_neighbor_id_loss typically ≈0.19–0.40; no instability observed
- Validation/test snapshots:
  - 19k (test): loss≈0.938; loss_id_real≈0.301; lpips_real≈0.290; l2_real≈0.046; w_norm_real≈42.86; loss_contrastive_id≈3.73; mb_applied_ratio≈0.75
  - 20.5k (test): loss≈0.956; loss_id_real≈0.300; lpips_real≈0.290; l2_real≈0.0458; w_norm_real≈43.01; loss_contrastive_id≈3.78; mb_applied_ratio≈0.75
  - 21k (test): loss≈0.950; loss_id_real≈0.305; lpips_real≈0.290; l2_real≈0.0463; w_norm_real≈42.50; loss_contrastive_id≈3.72; mb_applied_ratio≈0.75

Observations:
- The contrastive loss is functioning as intended: it is applied only when target ages hit [35,45], with mb_applied_ratio reflecting batch composition. Values are finite and stable, coexisting with NN‑ID.
- Identity and perceptual/pixel losses are in the same ranges as strong prior Stage‑1 runs by ~19–21k, and weight‑norm is trending into the expected low‑40s band without divergence.
- If pose/lighting appear too “locked,” consider λ→0.03 and/or τ→0.15, optionally reduce K→16. Otherwise continue through 30k and proceed to a light Stage 2.

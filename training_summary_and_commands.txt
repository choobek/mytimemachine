First full training (~90 images dataset - before train/test split):
-----------------------------
training 00014 30000step command:
bash -lc "conda activate mytimemachine && python scripts/train.py --dataset_type ffhq_aging --workers 2 --batch_size 2 --test_batch_size 2 --test_workers 2 --val_interval 500 --save_interval 1000 --start_from_encoded_w_plus --id_lambda 0.1 --lpips_lambda 0.1 --lpips_lambda_aging 0.1 --lpips_lambda_crop 0.6 --l2_lambda 0.25 --l2_lambda_aging 0.25 --l2_lambda_crop 1 --w_norm_lambda 0.005 --aging_lambda 5 --cycle_lambda 1 --input_nc 4 --target_age uniform_random --use_weighted_id_loss --checkpoint_path pretrained_models/sam_ffhq_aging.pt --train_dataset data/train --exp_dir experiments/full_training_run --adaptive_w_norm_lambda 7 --scheduler_type cosine --warmup_steps 500 --min_lr 1e-6 --grad_clip_norm 1.0 --nan_guard --extrapolation_start_step 3000 --extrapolation_prob_start 0.0 --extrapolation_prob_end 0.5 --train_encoder --max_steps 30000 | cat"
training 00015 35000-step decoder-only resume
bash -lc "cd /home/wczub/RND/AI_DEAGING/repos/mytimemachine && conda activate mytimemachine && PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python python scripts/train.py --dataset_type ffhq_aging --workers 2 --batch_size 2 --test_batch_size 2 --test_workers 2 --val_interval 500 --save_interval 1000 --start_from_encoded_w_plus --id_lambda 0.1 --lpips_lambda 0.1 --lpips_lambda_aging 0.1 --lpips_lambda_crop 0.6 --l2_lambda 0.25 --l2_lambda_aging 0.25 --l2_lambda_crop 1 --w_norm_lambda 0.005 --aging_lambda 5 --cycle_lambda 1 --input_nc 4 --target_age uniform_random --use_weighted_id_loss --checkpoint_path pretrained_models/sam_ffhq_aging.pt --train_dataset data/train --exp_dir experiments/full_training_run --adaptive_w_norm_lambda 7 --scheduler_type cosine --warmup_steps 500 --min_lr 1e-6 --grad_clip_norm 1.0 --nan_guard --resume_checkpoint experiments/full_training_run/00014/checkpoints/iteration_28000.pt --train_decoder --max_steps 35000 --learning_rate 0.0001"


Second full training (~180 images dataset - before train/test split):
-----------------------------
Changes we did before next try:
### TL;DR
- Increase personalization pressure and reduce over-smoothing.
- Start with adaptive_w_norm_lambda ≈ 20 and id_lambda ≈ 0.3.
- Strengthen crop perceptual loss; lower L2s; slightly loosen latent regularization.
- For decoder-only, use a smaller LR and extend steps.

### Targeted hyperparameter changes
- Adaptive W-Norm: increase from 7 → 15–25 (start at 20). Stronger personalization.
- Identity loss: increase from 0.1 → 0.3–0.4 (start at 0.3).
- Latent norm: reduce w_norm_lambda 0.005 → 0.003 (allows deviation for personalization).
- Perceptual on crop: increase lpips_lambda_crop 0.6 → 0.8–1.0 (start at 0.8).
- L2 losses: reduce l2_lambda 0.25 → 0.1 and l2_lambda_crop 1.0 → 0.5 (less smoothing).
- Cycle: increase cycle_lambda 1 → 2 (helps keep person-specific traits while editing).
- Decoder-only LR: reduce learning_rate 1e-4 → 5e-5; keep cosine, lower min_lr to 5e-7.
- Steps: extend decoder-only by +10k–25k (e.g., to 50k–60k total).
- Keep: use_weighted_id_loss, grad_clip_norm, nan_guard, warmup.

If still too “global” after these, push adaptive_w_norm_lambda to 25–30 and id_lambda to 0.4–0.5.

- training 00016 command - Stage 1 (encoder+decoder), stronger personalization:
```bash
bash -lc "cd /home/wczub/RND/AI_DEAGING/repos/mytimemachine && conda activate mytimemachine && PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python python scripts/train.py --dataset_type ffhq_aging --workers 2 --batch_size 2 --test_batch_size 2 --test_workers 2 --val_interval 500 --save_interval 1000 --start_from_encoded_w_plus --id_lambda 0.3 --lpips_lambda 0.1 --lpips_lambda_aging 0.1 --lpips_lambda_crop 0.8 --l2_lambda 0.1 --l2_lambda_aging 0.25 --l2_lambda_crop 0.5 --w_norm_lambda 0.003 --aging_lambda 5 --cycle_lambda 2 --input_nc 4 --target_age uniform_random --use_weighted_id_loss --checkpoint_path pretrained_models/sam_ffhq_aging.pt --train_dataset data/train --exp_dir experiments/full_training_run --adaptive_w_norm_lambda 20 --scheduler_type cosine --warmup_steps 500 --min_lr 5e-7 --grad_clip_norm 1.0 --nan_guard --extrapolation_start_step 3000 --extrapolation_prob_start 0.0 --extrapolation_prob_end 0.5 --train_encoder --max_steps 30000 | cat"
```

- training 00018 command - Stage 2 (decoder-only), lower LR and more steps.:
```bash
bash -lc "cd /home/wczub/RND/AI_DEAGING/repos/mytimemachine && conda activate mytimemachine && PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python python scripts/train.py --dataset_type ffhq_aging --workers 2 --batch_size 2 --test_batch_size 2 --test_workers 2 --val_interval 500 --save_interval 1000 --start_from_encoded_w_plus --id_lambda 0.3 --lpips_lambda 0.1 --lpips_lambda_aging 0.1 --lpips_lambda_crop 0.8 --l2_lambda 0.1 --l2_lambda_aging 0.25 --l2_lambda_crop 0.5 --w_norm_lambda 0.003 --aging_lambda 5 --cycle_lambda 2 --input_nc 4 --target_age uniform_random --use_weighted_id_loss --checkpoint_path pretrained_models/sam_ffhq_aging.pt --train_dataset data/train --exp_dir experiments/full_training_run --adaptive_w_norm_lambda 20 --scheduler_type cosine --warmup_steps 500 --min_lr 5e-7 --grad_clip_norm 1.0 --nan_guard --resume_checkpoint experiments/full_training_run/00016/checkpoints/iteration_30000.pt --train_decoder --max_steps 50000 --learning_rate 0.00005 | cat"
```

### Extra tips
- If faces still look a bit generic: push adaptive_w_norm_lambda → 25–30 and id_lambda → 0.4–0.5.
- If images turn soft: raise lpips_lambda_crop to 1.0 and/or lightly increase learning_rate back to 7e-5 for a short window, then anneal.
- If artifacts appear: slightly raise w_norm_lambda back toward 0.004 and keep grad clipping.


Third full training (still ~180 images dataset - no changes since previous training)
-------------------------------------
Goal: push stronger personalization while reducing smoothing.
Changes vs 00016/00018:
- adaptive_w_norm_lambda: 28 (↑ from 20)
- id_lambda: 0.45 (↑ from 0.3)
- w_norm_lambda: 0.002 (↓ from 0.003)
- lpips_lambda_crop: 1.0 (↑ from 0.8)
- l2_lambda: 0.05 (↓ from 0.1), l2_lambda_crop: 0.2 (↓ from 0.5)
- cycle_lambda: 3 (↑ from 2)
- warmup_steps: 800 (↑ from 500), min_lr: 3e-7 (↓ from 5e-7)
- extrapolation: start 2000, prob_end 0.7 (↑ from 0.5)
- Stage 2 only: w_norm_lambda_decoder_scale 0.3 (↓ from 0.5), keep aging_lambda_decoder_scale 0.5
- Stage 2 max_steps: 60000 (↑ from 50000)

- training 00019 command - Stage 1 (encoder+decoder), 30k steps, aggressive personalization
```bash
bash -lc "cd /home/wczub/RND/AI_DEAGING/repos/mytimemachine && conda activate mytimemachine && PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python python scripts/train.py --dataset_type ffhq_aging --workers 2 --batch_size 2 --test_batch_size 2 --test_workers 2 --val_interval 500 --save_interval 1000 --start_from_encoded_w_plus --id_lambda 0.45 --lpips_lambda 0.1 --lpips_lambda_aging 0.1 --lpips_lambda_crop 1.0 --l2_lambda 0.05 --l2_lambda_aging 0.25 --l2_lambda_crop 0.2 --w_norm_lambda 0.002 --aging_lambda 5 --cycle_lambda 3 --input_nc 4 --target_age uniform_random --use_weighted_id_loss --checkpoint_path pretrained_models/sam_ffhq_aging.pt --train_dataset data/train --exp_dir experiments/full_training_run --adaptive_w_norm_lambda 28 --scheduler_type cosine --warmup_steps 800 --min_lr 3e-7 --grad_clip_norm 1.0 --nan_guard --extrapolation_start_step 2000 --extrapolation_prob_start 0.0 --extrapolation_prob_end 0.7 --train_encoder --max_steps 30000 | cat"
```

- training 00020 command - Stage 2 (decoder-only), resume from NEW 30k checkpoint, to 60k steps
```bash
bash -lc "cd /home/wczub/RND/AI_DEAGING/repos/mytimemachine && conda activate mytimemachine && PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python python scripts/train.py --dataset_type ffhq_aging --workers 2 --batch_size 2 --test_batch_size 2 --test_workers 2 --val_interval 500 --save_interval 1000 --start_from_encoded_w_plus --id_lambda 0.45 --lpips_lambda 0.1 --lpips_lambda_aging 0.1 --lpips_lambda_crop 1.0 --l2_lambda 0.05 --l2_lambda_aging 0.25 --l2_lambda_crop 0.2 --w_norm_lambda 0.002 --w_norm_lambda_decoder_scale 0.3 --aging_lambda 5 --aging_lambda_decoder_scale 0.5 --cycle_lambda 3 --input_nc 4 --target_age uniform_random --use_weighted_id_loss --checkpoint_path pretrained_models/sam_ffhq_aging.pt --train_dataset data/train --exp_dir experiments/full_training_run --adaptive_w_norm_lambda 28 --scheduler_type cosine --warmup_steps 800 --min_lr 3e-7 --grad_clip_norm 1.0 --nan_guard --resume_checkpoint experiments/full_training_run/00019/checkpoints/iteration_30000.pt --train_decoder --max_steps 60000 --learning_rate 0.00005 | cat"
```

The description of tensorboard after the training:
train/id_improve_cycle

Stage-1: rises fast from negative to just below 0.0.

Stage-2: keeps creeping up, stabilizes ~0.00→+0.03 (near the plot’s top band).

Final: ≈0.02–0.03 (best/near-best among runs).

Vs prev.: equal or slightly higher at convergence.

train/id_improve_real

Stage-1: monotonic rise from negative to ≈-0.05.

Stage-2: continues upward, plateaus ~-0.01→+0.02.

Final: ≈0.00–0.02.

Vs prev.: slightly better at the end (less negative / nudges positive).

train/loss (overall)

Stage-1: strong decay ~1.6 → ~0.25.

Stage-2: gentle decay ~0.25 → ~0.16–0.18, low noise.

Final: ≈0.16–0.18 (smooth).

Vs prev.: marginally lower or on par.

train/loss_aging_cycle

Stage-1: drops to ~1e-4–5e-4 quickly.

Stage-2: stays near floor with small, infrequent blips.

Final: ≈1e-4 (very low).

Vs prev.: equal or slightly cleaner (earlier stabilization).

train/loss_aging_real

Stage-1: decays to ~1e-3 with oscillations.

Stage-2: some spikes early in the phase but damped; baseline keeps sliding down.

Final: ≈2e-4–5e-4 with moderate jitter.

Vs prev.: fewer/lower late spikes; slightly lower floor.

train/loss_cycle

Stage-1: falls ~0.7 → ~0.22–0.24.

Stage-2: further down then flat ~0.12–0.15.

Final: ≈0.12–0.15.

Vs prev.: equal or a touch better at the end.

train/loss_id_cycle

Stage-1: steady decay ~0.55 → ~0.14–0.16.

Stage-2 (visible portion): small further drop, then flat; no instability.

Final (approx.): ≈0.10–0.14 (trend consistent with other heads).

Vs prev.: similar or slightly lower.

train/loss_id_real

Stage-1: falls ~0.55 → ~0.18–0.20.

Stage-2: gradual decline, stable ~0.06–0.08.

Final: ≈0.06–0.07.

Vs prev.: lower at convergence.

train/loss_l2_crop

Stage-1: decays ~0.18 → ~0.07.

Stage-2: flattens ~0.05–0.06, low variance.

Final: ≈0.05–0.06.

Vs prev.: slightly lower.

train/loss_l2_cycle

Stage-1: decays ~0.17 → ~0.08–0.09.

Stage-2: stabilizes ~0.06–0.07.

Final: ≈0.06–0.07.

Vs prev.: marginally better.

train/loss_l2_real

Stage-1: decays ~0.17 → ~0.09–0.10.

Stage-2: flattens ~0.06–0.07 with mild jitter.

Final: ≈0.06–0.07.

Vs prev.: slightly lower.

train/loss_lpips_crop

Stage-1: falls ~0.34 → ~0.22–0.24.

Stage-2: gentle slide, then flat ~0.19–0.21.

Final: ≈0.19–0.21.

Vs prev.: similar or a bit better; no late degradation.

train/loss_lpips_cycle

Stage-1: falls ~0.33 → ~0.21–0.23.

Stage-2: stabilizes ~0.18–0.20.

Final: ≈0.18–0.20.

Vs prev.: slightly better.

train/loss_lpips_real

Stage-1: falls ~0.33 → ~0.22–0.24.

Stage-2: flat ~0.18–0.20, mild noise.

Final: ≈0.18–0.20.

Vs prev.: slightly lower end-point.

train/loss_real (adversarial, generator side)

Stage-1: decays ~0.95 → ~0.34–0.36.

Stage-2: settles ~0.26–0.28, small oscillations (typical GAN noise).

Final: ≈0.26–0.27.

Vs prev.: equal or a hair lower.

train/loss_w_norm_cycle (weight-norm penalty)

Stage-1: decays ~70 → ~26–27.

Stage-2: slight upward drift then plateaus ~27–28.

Final: ≈27–28.

Vs prev.: higher plateau (prev. best ≈24–26). Weights are a bit larger but stable.

train/loss_w_norm_real (weight-norm penalty)

Stage-1: decays ~70 → ~27–28.

Stage-2: mild upward creep, then flat ~28–29.

Final: ≈28–29.

Vs prev.: higher than the previous best (by ~2–4 units), yet not diverging.

One-line summary

This run improves or matches previous runs on identity, pixel, perceptual, cycle, aging, and adversarial losses, converging smoothly through Stage-2; the only trade-off is a slightly higher weight-norm plateau (larger but stable weights).

Visual quality-check

Even tho the tensorboard logs shows the improvements - visually the effects are poorer than from Second Full Training. The Second Full Training was visually cleaner and better match the actor without any artifacts. I would go back to previous settings as the starting point.

Fourth training on expanded dataset (>240 images; validation enabled)
-------------------------------------
Notes:
- Train: data/train; Test: data/test (expanded to >240 images each).
- Validation and test logging are enabled in `training/coach_aging_orig.py` (val_interval=500).
- Hyperparameters replicate the best prior runs: Stage 1 ≈ 00016, Stage 2 ≈ 00018.

training 00025 - Stage 1 — replicate 00016 (encoder+decoder, 30k steps)
```bash
bash -lc "cd /home/wczub/RND/AI_DEAGING/repos/mytimemachine && conda activate mytimemachine && PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python python scripts/train.py --dataset_type ffhq_aging --workers 2 --batch_size 2 --test_batch_size 2 --test_workers 2 --val_interval 500 --save_interval 1000 --start_from_encoded_w_plus --id_lambda 0.3 --lpips_lambda 0.1 --lpips_lambda_aging 0.1 --lpips_lambda_crop 0.8 --l2_lambda 0.1 --l2_lambda_aging 0.25 --l2_lambda_crop 0.5 --w_norm_lambda 0.003 --aging_lambda 5 --cycle_lambda 2 --input_nc 4 --target_age uniform_random --use_weighted_id_loss --checkpoint_path pretrained_models/sam_ffhq_aging.pt --train_dataset data/train --exp_dir experiments/full_training_run --adaptive_w_norm_lambda 20 --scheduler_type cosine --warmup_steps 500 --min_lr 5e-7 --grad_clip_norm 1.0 --nan_guard --extrapolation_start_step 3000 --extrapolation_prob_start 0.0 --extrapolation_prob_end 0.5 --train_encoder --max_steps 30000 | cat"
```

training 00026 - Stage 2 — replicate 00018 (decoder-only, 50k steps)
Run after Stage 1 completes; replace 000XX with the new Stage 1 folder name.
```bash
bash -lc "cd /home/wczub/RND/AI_DEAGING/repos/mytimemachine && conda activate mytimemachine && PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python python scripts/train.py --dataset_type ffhq_aging --workers 2 --batch_size 2 --test_batch_size 2 --test_workers 2 --val_interval 500 --save_interval 1000 --start_from_encoded_w_plus --id_lambda 0.3 --lpips_lambda 0.1 --lpips_lambda_aging 0.1 --lpips_lambda_crop 0.8 --l2_lambda 0.1 --l2_lambda_aging 0.25 --l2_lambda_crop 0.5 --w_norm_lambda 0.003 --aging_lambda 5 --cycle_lambda 2 --input_nc 4 --target_age uniform_random --use_weighted_id_loss --checkpoint_path pretrained_models/sam_ffhq_aging.pt --train_dataset data/train --exp_dir experiments/full_training_run --adaptive_w_norm_lambda 20 --scheduler_type cosine --warmup_steps 500 --min_lr 5e-7 --grad_clip_norm 1.0 --nan_guard --resume_checkpoint experiments/full_training_run/000XX/checkpoints/iteration_30000.pt --train_decoder --max_steps 50000 --learning_rate 0.00005 | cat"
```

Fifth training (2-staged) — low-risk stability tweaks
-------------------------------------
Goal: replicate fourth training setup via `scripts/train_two_stage.sh` with:
- Stage 1 steps: 25000 (was 30000)
- Stage 2 learning rate: 3e-5 (was 5e-5)
- Cycle lambda Stage 1: 1.5 (was 2)

Script deltas in `scripts/train_two_stage.sh`:
- `MAX_STEPS_S1=25000`
- `LEARNING_RATE_S2=3e-5`
- `CYCLE_LAMBDA_S1=1.5`
- Resume logic generalized to find `iteration_${MAX_STEPS_S1}.pt`

Run both stages automatically:
```bash
bash -lc "cd /home/wczub/RND/AI_DEAGING/repos/mytimemachine && conda activate mytimemachine && ./scripts/train_two_stage.sh | cat"
``` 

If running stages manually, example commands:
- Stage 1 (25k):
```bash
bash -lc "cd /home/wczub/RND/AI_DEAGING/repos/mytimemachine && conda activate mytimemachine && PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python python scripts/train.py --dataset_type ffhq_aging --workers 2 --batch_size 2 --test_batch_size 2 --test_workers 2 --val_interval 500 --save_interval 1000 --start_from_encoded_w_plus --id_lambda 0.3 --lpips_lambda 0.1 --lpips_lambda_aging 0.1 --lpips_lambda_crop 0.8 --l2_lambda 0.1 --l2_lambda_aging 0.25 --l2_lambda_crop 0.5 --w_norm_lambda 0.003 --aging_lambda 5 --cycle_lambda 1.5 --input_nc 4 --target_age uniform_random --use_weighted_id_loss --checkpoint_path pretrained_models/sam_ffhq_aging.pt --train_dataset data/train --exp_dir experiments/full_training_run --adaptive_w_norm_lambda 20 --scheduler_type cosine --warmup_steps 500 --min_lr 5e-7 --grad_clip_norm 1.0 --nan_guard --val_disable_aging --val_deterministic --val_max_batches 2 --val_start_step 2000 --extrapolation_start_step 3000 --extrapolation_prob_start 0.0 --extrapolation_prob_end 0.5 --train_encoder --max_steps 25000 | cat"
```

- Stage 2 (resume 25k, 50k steps, LR 3e-5):
```bash
bash -lc "cd /home/wczub/RND/AI_DEAGING/repos/mytimemachine && conda activate mytimemachine && PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python python scripts/train.py --dataset_type ffhq_aging --workers 2 --batch_size 2 --test_batch_size 2 --test_workers 2 --val_interval 500 --save_interval 1000 --start_from_encoded_w_plus --id_lambda 0.3 --lpips_lambda 0.1 --lpips_lambda_aging 0.1 --lpips_lambda_crop 0.8 --l2_lambda 0.1 --l2_lambda_aging 0.25 --l2_lambda_crop 0.5 --w_norm_lambda 0.003 --w_norm_lambda_decoder_scale 0.5 --aging_lambda 5 --aging_lambda_decoder_scale 0.5 --cycle_lambda 1.5 --input_nc 4 --target_age uniform_random --use_weighted_id_loss --checkpoint_path pretrained_models/sam_ffhq_aging.pt --train_dataset data/train --exp_dir experiments/full_training_run --adaptive_w_norm_lambda 20 --scheduler_type cosine --warmup_steps 500 --min_lr 5e-7 --grad_clip_norm 1.0 --nan_guard --val_disable_aging --val_deterministic --val_max_batches 2 --val_start_step 2000 --resume_checkpoint experiments/full_training_run/000XX/checkpoints/iteration_25000.pt --train_decoder --max_steps 50000 --learning_rate 0.00003 | cat"
```

Sixth training (2-staged) — interpolation personalization via NN-ID regularizer
-------------------------------------
Goal: keep the Fifth setup but add a nearest-neighbor identity regularizer during interpolation only, to better personalize to the target actor near age ~40. Disable extrapolation to remain in-domain.

Changes vs Fifth:
- Use new coach `training.coach_aging_orig_nn.Coach` via `COACH=orig_nn`.
- Add `--nearest_neighbor_id_loss_lambda 0.1`.
- Disable extrapolation: `--extrapolation_start_step 1000000000` (probability flags kept but inactive).
- All other hypers identical to Fifth.

Two-stage script (updated): `scripts/train_two_stage.sh`
- COACH="orig_nn"
- NEAREST_NEIGHBOR_ID_LAMBDA=0.1
- EXTRAPOLATION_START_STEP_S1=1000000000

Run both stages automatically:
```bash
bash -lc "cd /home/wczub/RND/AI_DEAGING/repos/mytimemachine && conda activate mytimemachine && ./scripts/train_two_stage.sh | cat"
```

Results and notes (00031 → 00032):
- Stage 1 — 00031 (encoder+decoder, 25k steps, COACH=orig_nn)
  - Key deltas vs Fifth: interpolation-only (extrapolation disabled), nearest-neighbor ID loss λ=0.1; otherwise identical hypers to Fifth.
  - Important hypers: id_lambda=0.3, lpips_lambda_crop=0.8, l2_lambda=0.1, l2_lambda_crop=0.5, w_norm_lambda=0.003, cycle_lambda=1.5, adaptive_w_norm_lambda=20, warmup_steps=500, min_lr=5e-7; validation enabled (val_interval=500, deterministic, val_max_batches=2, val_disable_aging).
  - Best val loss: 1.022 at steps 19,500 and 24,500 (final 25,000 → 1.032).
  - Metrics at best (≈24,500): loss_id_real≈0.4135, loss_l2_real≈0.0887, loss_lpips_real≈0.3199, loss_lpips_crop≈0.2153, loss_l2_crop≈0.0815, loss_w_norm_real≈45.78; total loss≈1.022.

- Stage 2 — 00032 (decoder-only, resume 25k → 50k, LR=3e-5)
  - Settings: train_decoder only; w_norm_lambda_decoder_scale=0.5, aging_lambda_decoder_scale=0.5; nearest-neighbor ID loss kept at λ=0.1; extrapolation disabled.
  - Best val loss: 0.868 at step 42,000 (final 50,000 → 0.875; early best at 25,000 was 0.888).
  - Metrics at best (42,000): loss_id_real≈0.3993, loss_l2_real≈0.0877, loss_lpips_real≈0.3155, loss_lpips_crop≈0.2114, loss_l2_crop≈0.0805, loss_w_norm_real≈45.69; total loss≈0.868. Improvements after ~42k were marginal/oscillatory.

- Visual assessment (manual): Stage 1 quickly reached high target similarity; Stage 2 added little visible improvement. Stage-1 outputs were almost as good as late Stage-2.

Takeaways / next steps:
- Extend Stage 1 duration (e.g., 30k–40k) to let encoder+decoder converge closer to the sweet spot before switching to decoder-only.
- Optionally shorten Stage 2 or run it with a brief LR plateau then anneal; consider modestly increasing NN-ID λ during Stage 1 only.
- Keep interpolation-only (no extrapolation) and current regularization; weight-norm remained stable in both stages.



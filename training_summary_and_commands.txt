First full training (~90 images dataset - before train/test split):
-----------------------------
training 00014 30000step command:
bash -lc "conda activate mytimemachine && python scripts/train.py --dataset_type ffhq_aging --workers 2 --batch_size 2 --test_batch_size 2 --test_workers 2 --val_interval 500 --save_interval 1000 --start_from_encoded_w_plus --id_lambda 0.1 --lpips_lambda 0.1 --lpips_lambda_aging 0.1 --lpips_lambda_crop 0.6 --l2_lambda 0.25 --l2_lambda_aging 0.25 --l2_lambda_crop 1 --w_norm_lambda 0.005 --aging_lambda 5 --cycle_lambda 1 --input_nc 4 --target_age uniform_random --use_weighted_id_loss --checkpoint_path pretrained_models/sam_ffhq_aging.pt --train_dataset data/train --exp_dir experiments/full_training_run --adaptive_w_norm_lambda 7 --scheduler_type cosine --warmup_steps 500 --min_lr 1e-6 --grad_clip_norm 1.0 --nan_guard --extrapolation_start_step 3000 --extrapolation_prob_start 0.0 --extrapolation_prob_end 0.5 --train_encoder --max_steps 30000 | cat"
training 00015 35000-step decoder-only resume
bash -lc "cd /home/wczub/RND/AI_DEAGING/repos/mytimemachine && conda activate mytimemachine && PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python python scripts/train.py --dataset_type ffhq_aging --workers 2 --batch_size 2 --test_batch_size 2 --test_workers 2 --val_interval 500 --save_interval 1000 --start_from_encoded_w_plus --id_lambda 0.1 --lpips_lambda 0.1 --lpips_lambda_aging 0.1 --lpips_lambda_crop 0.6 --l2_lambda 0.25 --l2_lambda_aging 0.25 --l2_lambda_crop 1 --w_norm_lambda 0.005 --aging_lambda 5 --cycle_lambda 1 --input_nc 4 --target_age uniform_random --use_weighted_id_loss --checkpoint_path pretrained_models/sam_ffhq_aging.pt --train_dataset data/train --exp_dir experiments/full_training_run --adaptive_w_norm_lambda 7 --scheduler_type cosine --warmup_steps 500 --min_lr 1e-6 --grad_clip_norm 1.0 --nan_guard --resume_checkpoint experiments/full_training_run/00014/checkpoints/iteration_28000.pt --train_decoder --max_steps 35000 --learning_rate 0.0001"



Second full training (~180 images dataset - before train/test split):
-----------------------------
Changes we did before next try:
### TL;DR
- Increase personalization pressure and reduce over-smoothing.
- Start with adaptive_w_norm_lambda ≈ 20 and id_lambda ≈ 0.3.
- Strengthen crop perceptual loss; lower L2s; slightly loosen latent regularization.
- For decoder-only, use a smaller LR and extend steps.

### Targeted hyperparameter changes
- Adaptive W-Norm: increase from 7 → 15–25 (start at 20). Stronger personalization.
- Identity loss: increase from 0.1 → 0.3–0.4 (start at 0.3).
- Latent norm: reduce w_norm_lambda 0.005 → 0.003 (allows deviation for personalization).
- Perceptual on crop: increase lpips_lambda_crop 0.6 → 0.8–1.0 (start at 0.8).
- L2 losses: reduce l2_lambda 0.25 → 0.1 and l2_lambda_crop 1.0 → 0.5 (less smoothing).
- Cycle: increase cycle_lambda 1 → 2 (helps keep person-specific traits while editing).
- Decoder-only LR: reduce learning_rate 1e-4 → 5e-5; keep cosine, lower min_lr to 5e-7.
- Steps: extend decoder-only by +10k–25k (e.g., to 50k–60k total).
- Keep: use_weighted_id_loss, grad_clip_norm, nan_guard, warmup.

If still too "global" after these, push adaptive_w_norm_lambda to 25–30 and id_lambda to 0.4–0.5.

- training 00016 command - Stage 1 (encoder+decoder), stronger personalization:
```bash
bash -lc "cd /home/wczub/RND/AI_DEAGING/repos/mytimemachine && conda activate mytimemachine && PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python python scripts/train.py --dataset_type ffhq_aging --workers 2 --batch_size 2 --test_batch_size 2 --test_workers 2 --val_interval 500 --save_interval 1000 --start_from_encoded_w_plus --id_lambda 0.3 --lpips_lambda 0.1 --lpips_lambda_aging 0.1 --lpips_lambda_crop 0.8 --l2_lambda 0.1 --l2_lambda_aging 0.25 --l2_lambda_crop 0.5 --w_norm_lambda 0.003 --aging_lambda 5 --cycle_lambda 2 --input_nc 4 --target_age uniform_random --use_weighted_id_loss --checkpoint_path pretrained_models/sam_ffhq_aging.pt --train_dataset data/train --exp_dir experiments/full_training_run --adaptive_w_norm_lambda 20 --scheduler_type cosine --warmup_steps 500 --min_lr 5e-7 --grad_clip_norm 1.0 --nan_guard --extrapolation_start_step 3000 --extrapolation_prob_start 0.0 --extrapolation_prob_end 0.5 --train_encoder --max_steps 30000 | cat"
```

- training 00018 command - Stage 2 (decoder-only), lower LR and more steps.:
```bash
bash -lc "cd /home/wczub/RND/AI_DEAGING/repos/mytimemachine && conda activate mytimemachine && PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python python scripts/train.py --dataset_type ffhq_aging --workers 2 --batch_size 2 --test_batch_size 2 --test_workers 2 --val_interval 500 --save_interval 1000 --start_from_encoded_w_plus --id_lambda 0.3 --lpips_lambda 0.1 --lpips_lambda_aging 0.1 --lpips_lambda_crop 0.8 --l2_lambda 0.1 --l2_lambda_aging 0.25 --l2_lambda_crop 0.5 --w_norm_lambda 0.003 --aging_lambda 5 --cycle_lambda 2 --input_nc 4 --target_age uniform_random --use_weighted_id_loss --checkpoint_path pretrained_models/sam_ffhq_aging.pt --train_dataset data/train --exp_dir experiments/full_training_run --adaptive_w_norm_lambda 20 --scheduler_type cosine --warmup_steps 500 --min_lr 5e-7 --grad_clip_norm 1.0 --nan_guard --resume_checkpoint experiments/full_training_run/00016/checkpoints/iteration_30000.pt --train_decoder --max_steps 50000 --learning_rate 0.00005 | cat"
```

### Extra tips
- If faces still look a bit generic: push adaptive_w_norm_lambda → 25–30 and id_lambda → 0.4–0.5.
- If images turn soft: raise lpips_lambda_crop to 1.0 and/or lightly increase learning_rate back to 7e-5 for a short window, then anneal.
- If artifacts appear: slightly raise w_norm_lambda back toward 0.004 and keep grad clipping.


Third full training (still ~180 images dataset - no changes since previous training)
-------------------------------------
Goal: push stronger personalization while reducing smoothing.
Changes vs 00016/00018:
- adaptive_w_norm_lambda: 28 (↑ from 20)
- id_lambda: 0.45 (↑ from 0.3)
- w_norm_lambda: 0.002 (↓ from 0.003)
- lpips_lambda_crop: 1.0 (↑ from 0.8)
- l2_lambda: 0.05 (↓ from 0.1), l2_lambda_crop: 0.2 (↓ from 0.5)
- cycle_lambda: 3 (↑ from 2)
- warmup_steps: 800 (↑ from 500), min_lr: 3e-7 (↓ from 5e-7)
- extrapolation: start 2000, prob_end 0.7 (↑ from 0.5)
- Stage 2 only: w_norm_lambda_decoder_scale 0.3 (↓ from 0.5), keep aging_lambda_decoder_scale 0.5
- Stage 2 max_steps: 60000 (↑ from 50000)

- training 00019 command - Stage 1 (encoder+decoder), 30k steps, aggressive personalization
```bash
bash -lc "cd /home/wczub/RND/AI_DEAGING/repos/mytimemachine && conda activate mytimemachine && PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python python scripts/train.py --dataset_type ffhq_aging --workers 2 --batch_size 2 --test_batch_size 2 --test_workers 2 --val_interval 500 --save_interval 1000 --start_from_encoded_w_plus --id_lambda 0.45 --lpips_lambda 0.1 --lpips_lambda_aging 0.1 --lpips_lambda_crop 1.0 --l2_lambda 0.05 --l2_lambda_aging 0.25 --l2_lambda_crop 0.2 --w_norm_lambda 0.002 --aging_lambda 5 --cycle_lambda 3 --input_nc 4 --target_age uniform_random --use_weighted_id_loss --checkpoint_path pretrained_models/sam_ffhq_aging.pt --train_dataset data/train --exp_dir experiments/full_training_run --adaptive_w_norm_lambda 28 --scheduler_type cosine --warmup_steps 800 --min_lr 3e-7 --grad_clip_norm 1.0 --nan_guard --extrapolation_start_step 2000 --extrapolation_prob_start 0.0 --extrapolation_prob_end 0.7 --train_encoder --max_steps 30000 | cat"
```

- training 00020 command - Stage 2 (decoder-only), resume from NEW 30k checkpoint, to 60k steps
```bash
bash -lc "cd /home/wczub/RND/AI_DEAGING/repos/mytimemachine && conda activate mytimemachine && PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python python scripts/train.py --dataset_type ffhq_aging --workers 2 --batch_size 2 --test_batch_size 2 --test_workers 2 --val_interval 500 --save_interval 1000 --start_from_encoded_w_plus --id_lambda 0.45 --lpips_lambda 0.1 --lpips_lambda_aging 0.1 --lpips_lambda_crop 1.0 --l2_lambda 0.05 --l2_lambda_aging 0.25 --l2_lambda_crop 0.2 --w_norm_lambda 0.002 --w_norm_lambda_decoder_scale 0.3 --aging_lambda 5 --aging_lambda_decoder_scale 0.5 --cycle_lambda 3 --input_nc 4 --target_age uniform_random --use_weighted_id_loss --checkpoint_path pretrained_models/sam_ffhq_aging.pt --train_dataset data/train --exp_dir experiments/full_training_run --adaptive_w_norm_lambda 28 --scheduler_type cosine --warmup_steps 800 --min_lr 3e-7 --grad_clip_norm 1.0 --nan_guard --resume_checkpoint experiments/full_training_run/00019/checkpoints/iteration_30000.pt --train_decoder --max_steps 60000 --learning_rate 0.00005 | cat"
```

The description of tensorboard after the training:
train/id_improve_cycle

Stage-1: rises fast from negative to just below 0.0.

Stage-2: keeps creeping up, stabilizes ~0.00→+0.03 (near the plot's top band).

Final: ≈0.02–0.03 (best/near-best among runs).

Vs prev.: equal or slightly higher at convergence.

train/id_improve_real

Stage-1: monotonic rise from negative to ≈-0.05.

Stage-2: continues upward, plateaus ~-0.01→+0.02.

Final: ≈0.00–0.02.

Vs prev.: slightly better at the end (less negative / nudges positive).

train/loss (overall)

Stage-1: strong decay ~1.6 → ~0.25.

Stage-2: gentle decay ~0.25 → ~0.16–0.18, low noise.

Final: ≈0.16–0.18 (smooth).

Vs prev.: marginally lower or on par.

train/loss_aging_cycle

Stage-1: drops to ~1e-4–5e-4 quickly.

Stage-2: stays near floor with small, infrequent blips.

Final: ≈1e-4 (very low).

Vs prev.: equal or slightly cleaner (earlier stabilization).

train/loss_aging_real

Stage-1: decays to ~1e-3 with oscillations.

Stage-2: some spikes early in the phase but damped; baseline keeps sliding down.

Final: ≈2e-4–5e-4 with moderate jitter.

Vs prev.: fewer/lower late spikes; slightly lower floor.

train/loss_cycle

Stage-1: falls ~0.7 → ~0.22–0.24.

Stage-2: further down then flat ~0.12–0.15.

Final: ≈0.12–0.15.

Vs prev.: equal or a touch better at the end.

train/loss_id_cycle

Stage-1: steady decay ~0.55 → ~0.14–0.16.

Stage-2 (visible portion): small further drop, then flat; no instability.

Final (approx.): ≈0.10–0.14 (trend consistent with other heads).

Vs prev.: similar or slightly lower.

train/loss_id_real

Stage-1: falls ~0.55 → ~0.18–0.20.

Stage-2: gradual decline, stable ~0.06–0.08.

Final: ≈0.06–0.07.

Vs prev.: lower at convergence.

train/loss_l2_crop

Stage-1: decays ~0.18 → ~0.07.

Stage-2: flattens ~0.05–0.06, low variance.

Final: ≈0.05–0.06.

Vs prev.: slightly lower.

train/loss_l2_cycle

Stage-1: decays ~0.17 → ~0.08–0.09.

Stage-2: stabilizes ~0.06–0.07.

Final: ≈0.06–0.07.

Vs prev.: marginally better.

train/loss_l2_real

Stage-1: decays ~0.17 → ~0.09–0.10.

Stage-2: flattens ~0.06–0.07 with mild jitter.

Final: ≈0.06–0.07.

Vs prev.: slightly lower.

train/loss_lpips_crop

Stage-1: falls ~0.34 → ~0.22–0.24.

Stage-2: gentle slide, then flat ~0.19–0.21.

Final: ≈0.19–0.21.

Vs prev.: similar or a bit better; no late degradation.

train/loss_lpips_cycle

Stage-1: falls ~0.33 → ~0.21–0.23.

Stage-2: stabilizes ~0.18–0.20.

Final: ≈0.18–0.20.

Vs prev.: slightly better.

train/loss_lpips_real

Stage-1: falls ~0.33 → ~0.22–0.24.

Stage-2: flat ~0.18–0.20, mild noise.

Final: ≈0.18–0.20.

Vs prev.: slightly lower end-point.

train/loss_real (adversarial, generator side)

Stage-1: decays ~0.95 → ~0.34–0.36.

Stage-2: settles ~0.26–0.28, small oscillations (typical GAN noise).

Final: ≈0.26–0.27.

Vs prev.: equal or a hair lower.

train/loss_w_norm_cycle (weight-norm penalty)

Stage-1: decays ~70 → ~26–27.

Stage-2: slight upward drift then plateaus ~27–28.

Final: ≈27–28.

Vs prev.: higher plateau (prev. best ≈24–26). Weights are a bit larger but stable.

train/loss_w_norm_real (weight-norm penalty)

Stage-1: decays ~70 → ~27–28.

Stage-2: mild upward creep, then flat ~28–29.

Final: ≈28–29.

Vs prev.: higher than the previous best (by ~2–4 units), yet not diverging.

One-line summary

This run improves or matches previous runs on identity, pixel, perceptual, cycle, aging, and adversarial losses, converging smoothly through Stage-2; the only trade-off is a slightly higher weight-norm plateau (larger but stable weights).

Visual quality-check

Even tho the tensorboard logs shows the improvements - visually the effects are poorer than from Second Full Training. The Second Full Training was visually cleaner and better match the actor without any artifacts. I would go back to previous settings as the starting point.

Fourth training on expanded dataset (>240 images; validation enabled)
-------------------------------------
Notes:
- Train: data/train; Test: data/test (expanded to >240 images each).
- Validation and test logging are enabled in `training/coach_aging_orig.py` (val_interval=500).
- Hyperparameters replicate the best prior runs: Stage 1 ≈ 00016, Stage 2 ≈ 00018.

training 00025 - Stage 1 — replicate 00016 (encoder+decoder, 30k steps)
```bash
bash -lc "cd /home/wczub/RND/AI_DEAGING/repos/mytimemachine && conda activate mytimemachine && PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python python scripts/train.py --dataset_type ffhq_aging --workers 2 --batch_size 2 --test_batch_size 2 --test_workers 2 --val_interval 500 --save_interval 1000 --start_from_encoded_w_plus --id_lambda 0.3 --lpips_lambda 0.1 --lpips_lambda_aging 0.1 --lpips_lambda_crop 0.8 --l2_lambda 0.1 --l2_lambda_aging 0.25 --l2_lambda_crop 0.5 --w_norm_lambda 0.003 --aging_lambda 5 --cycle_lambda 2 --input_nc 4 --target_age uniform_random --use_weighted_id_loss --checkpoint_path pretrained_models/sam_ffhq_aging.pt --train_dataset data/train --exp_dir experiments/full_training_run --adaptive_w_norm_lambda 20 --scheduler_type cosine --warmup_steps 500 --min_lr 5e-7 --grad_clip_norm 1.0 --nan_guard --extrapolation_start_step 3000 --extrapolation_prob_start 0.0 --extrapolation_prob_end 0.5 --train_encoder --max_steps 30000 | cat"
```

training 00026 - Stage 2 — replicate 00018 (decoder-only, 50k steps)
Run after Stage 1 completes; replace 000XX with the new Stage 1 folder name.
```bash
bash -lc "cd /home/wczub/RND/AI_DEAGING/repos/mytimemachine && conda activate mytimemachine && PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python python scripts/train.py --dataset_type ffhq_aging --workers 2 --batch_size 2 --test_batch_size 2 --test_workers 2 --val_interval 500 --save_interval 1000 --start_from_encoded_w_plus --id_lambda 0.3 --lpips_lambda 0.1 --lpips_lambda_aging 0.1 --lpips_lambda_crop 0.8 --l2_lambda 0.1 --l2_lambda_aging 0.25 --l2_lambda_crop 0.5 --w_norm_lambda 0.003 --aging_lambda 5 --cycle_lambda 2 --input_nc 4 --target_age uniform_random --use_weighted_id_loss --checkpoint_path pretrained_models/sam_ffhq_aging.pt --train_dataset data/train --exp_dir experiments/full_training_run --adaptive_w_norm_lambda 20 --scheduler_type cosine --warmup_steps 500 --min_lr 5e-7 --grad_clip_norm 1.0 --nan_guard --resume_checkpoint experiments/full_training_run/000XX/checkpoints/iteration_30000.pt --train_decoder --max_steps 50000 --learning_rate 0.00005 | cat"
```

Fifth training (2-staged) — low-risk stability tweaks
-------------------------------------
Goal: replicate fourth training setup via `scripts/train_two_stage.sh` with:
- Stage 1 steps: 25000 (was 30000)
- Stage 2 learning rate: 3e-5 (was 5e-5)
- Cycle lambda Stage 1: 1.5 (was 2)

Script deltas in `scripts/train_two_stage.sh`:
- `MAX_STEPS_S1=25000`
- `LEARNING_RATE_S2=3e-5`
- `CYCLE_LAMBDA_S1=1.5`
- Resume logic generalized to find `iteration_${MAX_STEPS_S1}.pt`

Run both stages automatically:
```bash
bash -lc "cd /home/wczub/RND/AI_DEAGING/repos/mytimemachine && conda activate mytimemachine && ./scripts/train_two_stage.sh | cat"
``` 

If running stages manually, example commands:
- Stage 1 (25k):
```bash
bash -lc "cd /home/wczub/RND/AI_DEAGING/repos/mytimemachine && conda activate mytimemachine && PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python python scripts/train.py --dataset_type ffhq_aging --workers 2 --batch_size 2 --test_batch_size 2 --test_workers 2 --val_interval 500 --save_interval 1000 --start_from_encoded_w_plus --id_lambda 0.3 --lpips_lambda 0.1 --lpips_lambda_aging 0.1 --lpips_lambda_crop 0.8 --l2_lambda 0.1 --l2_lambda_aging 0.25 --l2_lambda_crop 0.5 --w_norm_lambda 0.003 --aging_lambda 5 --cycle_lambda 1.5 --input_nc 4 --target_age uniform_random --use_weighted_id_loss --checkpoint_path pretrained_models/sam_ffhq_aging.pt --train_dataset data/train --exp_dir experiments/full_training_run --adaptive_w_norm_lambda 20 --scheduler_type cosine --warmup_steps 500 --min_lr 5e-7 --grad_clip_norm 1.0 --nan_guard --val_disable_aging --val_deterministic --val_max_batches 2 --val_start_step 2000 --extrapolation_start_step 3000 --extrapolation_prob_start 0.0 --extrapolation_prob_end 0.5 --train_encoder --max_steps 25000 | cat"
```

- Stage 2 (resume 25k, 50k steps, LR 3e-5):
```bash
bash -lc "cd /home/wczub/RND/AI_DEAGING/repos/mytimemachine && conda activate mytimemachine && PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python python scripts/train.py --dataset_type ffhq_aging --workers 2 --batch_size 2 --test_batch_size 2 --test_workers 2 --val_interval 500 --save_interval 1000 --start_from_encoded_w_plus --id_lambda 0.3 --lpips_lambda 0.1 --lpips_lambda_aging 0.1 --lpips_lambda_crop 0.8 --l2_lambda 0.1 --l2_lambda_aging 0.25 --l2_lambda_crop 0.5 --w_norm_lambda 0.003 --w_norm_lambda_decoder_scale 0.5 --aging_lambda 5 --aging_lambda_decoder_scale 0.5 --cycle_lambda 1.5 --input_nc 4 --target_age uniform_random --use_weighted_id_loss --checkpoint_path pretrained_models/sam_ffhq_aging.pt --train_dataset data/train --exp_dir experiments/full_training_run --adaptive_w_norm_lambda 20 --scheduler_type cosine --warmup_steps 500 --min_lr 5e-7 --grad_clip_norm 1.0 --nan_guard --val_disable_aging --val_deterministic --val_max_batches 2 --val_start_step 2000 --resume_checkpoint experiments/full_training_run/000XX/checkpoints/iteration_25000.pt --train_decoder --max_steps 50000 --learning_rate 0.00003 | cat"
```

Sixth training (2-staged) — interpolation personalization via NN-ID regularizer
-------------------------------------
Goal: keep the Fifth setup but add a nearest-neighbor identity regularizer during interpolation only, to better personalize to the target actor near age ~40. Disable extrapolation to remain in-domain.

Changes vs Fifth:
- Use new coach `training.coach_aging_orig_nn.Coach` via `COACH=orig_nn`.
- Add `--nearest_neighbor_id_loss_lambda 0.1`.
- Disable extrapolation: `--extrapolation_start_step 1000000000` (probability flags kept but inactive).
- All other hypers identical to Fifth.

Two-stage script (updated): `scripts/train_two_stage.sh`
- COACH="orig_nn"
- NEAREST_NEIGHBOR_ID_LAMBDA=0.1
- EXTRAPOLATION_START_STEP_S1=1000000000

Run both stages automatically:
```bash
bash -lc "cd /home/wczub/RND/AI_DEAGING/repos/mytimemachine && conda activate mytimemachine && ./scripts/train_two_stage.sh | cat"
```

Results and notes (00031 → 00032):
- Stage 1 — 00031 (encoder+decoder, 25k steps, COACH=orig_nn)
  - Key deltas vs Fifth: interpolation-only (extrapolation disabled), nearest-neighbor ID loss λ=0.1; otherwise identical hypers to Fifth.
  - Important hypers: id_lambda=0.3, lpips_lambda_crop=0.8, l2_lambda=0.1, l2_lambda_crop=0.5, w_norm_lambda=0.003, cycle_lambda=1.5, adaptive_w_norm_lambda=20, warmup_steps=500, min_lr=5e-7; validation enabled (val_interval=500, deterministic, val_max_batches=2, val_disable_aging).
  - Best val loss: 1.022 at steps 19,500 and 24,500 (final 25,000 → 1.032).
  - Metrics at best (≈24,500): loss_id_real≈0.4135, loss_l2_real≈0.0887, loss_lpips_real≈0.3199, loss_lpips_crop≈0.2153, loss_l2_crop≈0.0815, loss_w_norm_real≈45.78; total loss≈1.022.

- Stage 2 — 00032 (decoder-only, resume 25k → 50k, LR=3e-5)
  - Settings: train_decoder only; w_norm_lambda_decoder_scale=0.5, aging_lambda_decoder_scale=0.5; nearest-neighbor ID loss kept at λ=0.1; extrapolation disabled.
  - Best val loss: 0.868 at step 42,000 (final 50,000 → 0.875; early best at 25,000 was 0.888).
  - Metrics at best (42,000): loss_id_real≈0.3993, loss_l2_real≈0.0877, loss_lpips_real≈0.3155, loss_lpips_crop≈0.2114, loss_l2_crop≈0.0805, loss_w_norm_real≈45.69; total loss≈0.868. Improvements after ~42k were marginal/oscillatory.

- Visual assessment (manual): Stage 1 quickly reached high target similarity; Stage 2 added little visible improvement. Stage-1 outputs were almost as good as late Stage-2.

Takeaways / next steps:
- Extend Stage 1 duration (e.g., 30k–40k) to let encoder+decoder converge closer to the sweet spot before switching to decoder-only.
- Optionally shorten Stage 2 or run it with a brief LR plateau then anneal; consider modestly increasing NN-ID λ during Stage 1 only.
- Keep interpolation-only (no extrapolation) and current regularization; weight-norm remained stable in both stages.



Seventh training (2-staged) — identity preservation focus
-------------------------------------
Goal: strengthen identity preservation without reintroducing artifacts. Build on the Fifth/Sixth setup (stable visuals), extend Stage 1 to solidify identity, lightly refine in Stage 2.

Plan highlights:
- Coach: COACH=orig_nn (nearest-neighbor ID regularizer active on interpolation only)
- Disable extrapolation (in-domain only): extrapolation_start_step=1000000000
- Keep stable regs: adaptive_w_norm_lambda=20, w_norm_lambda=0.003, cycle_lambda=1.5, lpips_lambda_crop=0.8, l2_lambda=0.1, l2_lambda_crop=0.5

Stage 1 (encoder+decoder)
- Steps: 35,000
- id_lambda: 0.35
- nearest_neighbor_id_loss_lambda: 0.2
- Other hypers: as in Fifth/Sixth; validation enabled as before

Stage 2 (decoder-only)
- Steps: 45,000 (resume from 35k)
- learning_rate: 3e-5
- id_lambda: 0.3
- nearest_neighbor_id_loss_lambda: 0.05
- w_norm_lambda_decoder_scale: 0.5, aging_lambda_decoder_scale: 0.5

How to run (updated two-stage script):
```bash
bash -lc "cd /home/wczub/RND/AI_DEAGING/repos/mytimemachine && conda activate mytimemachine && ./scripts/train_two_stage.sh | cat"
```

Monitoring
- Track identity: train/id_improve_real should rise toward ≥0.02 by ~30–35k; train/loss_id_real should trend ≤0.40 late S1 and ≤0.38 in S2.
- Visual checks: preserve facial structure (eyes, mouth asymmetry, moles), avoid generic-face drift.
- If identity plateaus late in S1: briefly raise nearest_neighbor_id_loss_lambda to 0.25 for final 3–5k of S1, then keep 0.05 in S2.

Final results of this seventh training:

Setup recap:
- Coach: orig_nn; extrapolation disabled
- Stage 1: id_lambda=0.35, nearest_neighbor_id_loss_lambda=0.2
- Stage 2: id_lambda=0.3, nearest_neighbor_id_loss_lambda=0.05, learning_rate=3e-5
- Shared: adaptive_w_norm_lambda=20, w_norm_lambda=0.003; crop LPIPS/L2 enabled

Key validation (test) checkpoints:
- Step 10k: loss_id_real≈0.409, lpips_real≈0.328, l2_real≈0.103, w_norm_real≈51.25, total≈1.134
- Step 21k: loss_id_real≈0.390, lpips_real≈0.316, l2_real≈0.095, w_norm_real≈45.02, total≈1.034
- Step 45k (end): loss_id_real≈0.399, lpips_real≈0.316, l2_real≈0.099, w_norm_real≈42.73, total≈0.891

End-of-run training snapshot (step 45k):
- train: loss_id_real≈0.105, lpips_real≈0.172, l2_real≈0.024, w_norm_real≈41.04, total≈0.412

What improved within this run:
- Identity: improved during Stage 1 (0.409→0.390). Stage 2 held identity roughly steady (0.399 at 45k), similar to prior runs where Stage 2 adds limited identity gains.
- Perceptual/pixel: both tightened from 10k to 21k and remained strong through 45k (LPIPS stayed ≈0.316; L2 ≈0.099).
- Regularization: w_norm_real decayed consistently (≈51→45→42.7), signaling healthier latent magnitudes and stable convergence.

Notes on NN-ID and stability:
- nearest_neighbor_id_loss remained active throughout (train peaks ≈0.45 without instability), supporting personalization near target ages.

Recommendations:
- Allocate more budget to Stage 1 (e.g., 38–40k) and shorten Stage 2 (e.g., 40–43k). Most identity gains manifest during Stage 1.
- If identity plateaus late Stage 1, briefly bump NN-ID λ to 0.25 for the last 3–5k, then revert to 0.05 for Stage 2.
- Keep current regularization and interpolation-only setting; both correlate with stable, artifact-free training here.

Eighth training (2-staged, start from scratch on second computer) — Seventh + medium improvements
-------------------------------------
Goal: Start Stage 1 from step 0 with COACH=orig_nn and NN-ID regularizer, extrapolation disabled. Apply phased tweaks in Stage 1 (mid-run LPIPS/L2 tightening; late NN-ID bump). Automatically continue to Stage 2 when Stage 1 reaches 35k.

Key settings (Stage 1 → Stage 2):
- Coach: orig_nn
- Stage 1 steps: 35,000; Stage 2 steps: 45,000
- NN-ID λ: 0.2 (S1 baseline), 0.25 for 30k→35k, 0.05 in S2
- id_lambda: 0.35 (S1), 0.30 (S2)
- cycle_lambda: 1.5; adaptive_w_norm_lambda: 20; w_norm_lambda: 0.003
- lpips_lambda_crop: 0.8 baseline; 0.9 for 20k→30k
- l2_lambda: 0.1 baseline; 0.08 for 20k→30k; restore at 30k
- l2_lambda_crop: 0.5 baseline; 0.4 for 20k→30k; restore at 30k
- Extrapolation disabled: extrapolation_start_step=1000000000
- Learning rate S2: 3e-5

Run both stages automatically from scratch:
```bash
bash -lc "cd /home/wczub/RND/AI_DEAGING/repos/mytimemachine && conda activate mytimemachine && ./scripts/train_two_stage.sh | cat"
```

Notes:
- Stage 1 runs in three phases: 0→20k (baseline), 20k→30k (tighten LPIPS/L2), 30k→35k (NN-ID bump, restore LPIPS/L2). The script locates 20k/30k checkpoints automatically.
- Stage 2 will automatically search for the newest `iteration_35000.pt` under `experiments/full_training_run/**/checkpoints/` and resume from it.
- Validation is deterministic, every 500 steps, with small batch cap for speed.

Monitoring
- Track identity: train/id_improve_real should rise toward ≥0.02 by ~30–35k; train/loss_id_real should trend ≤0.40 late S1 and ≤0.38 in S2.
- Visual checks: preserve facial structure (eyes, mouth asymmetry, moles), avoid generic-face drift.
- If identity plateaus late in S1: briefly raise nearest_neighbor_id_loss_lambda to 0.25 for final 3–5k of S1, then keep 0.05 in S2.

Final results of this eighth training:

Setup recap:
- Coach: orig_nn; extrapolation disabled
- Stage 1: 0→20k baseline; 20k→30k lpips_crop=0.9, l2=0.08, l2_crop=0.4; 30k→35k NN-ID λ=0.25, restore lpips/L2
- Stage 2: 35k→45k; id_lambda=0.30; NN-ID λ=0.05; learning_rate=3e-5
- Shared: adaptive_w_norm_lambda=20; w_norm_lambda=0.003; cycle_lambda=1.5; crop LPIPS/L2 enabled

Key validation (test) checkpoints:
- 10k (S1 P1): loss_id_real≈0.409, lpips_real≈0.328, l2_real≈0.103, w_norm_real≈51.25, total≈1.134
- 21k (S1 P2 early): loss_id_real≈0.390, lpips_real≈0.316, l2_real≈0.095, w_norm_real≈45.02, total≈1.034
- 45k (end S2): loss_id_real≈0.284, lpips_real≈0.305, l2_real≈0.059, w_norm_real≈45.56, total≈0.722

Representative training snapshots (late S2 window):
- 44.55k: loss_id_real≈0.104, lpips_real≈0.250, l2_real≈0.064, w_norm_real≈48.40, total≈0.507
- 45.00k: loss_id_real≈0.080, lpips_real≈0.195, l2_real≈0.025, w_norm_real≈46.16, total≈0.401

Trends and observations:
- Identity: improved substantially during S1 (0.409→0.390); further improved by S2 end on test (≈0.284). Train identity tightened steadily late S2 (≈0.08–0.15), indicating strong personalization without overfit spikes.
- Perceptual/pixel: mid-run tightening (P2) reduced LPIPS/L2; by 45k test LPIPS≈0.305 and L2≈0.059, matching or slightly better than prior bests.
- Regularization: w_norm_real dropped from ~51 at 10k to mid‑40s by 21k and stayed ~45–46 at 45k, suggesting stable edits with less drift than earlier stages; occasional train spikes to ~51 were transient.
- NN-ID stability: nearest_neighbor_id_loss active (≈0.21–0.37 range) with the 30k→35k bump; no instability observed.

Qualitative notes:
- Visuals in late S1 and S2 were crisp with preserved micro‑identity (eyes/mouth), consistent with lower crop LPIPS/L2; no recurrent artifacts observed near the NN‑ID bump.

Actionable follow‑ups:
- Consider extending S1 to 38–40k when compute allows; most identity gains happened in S1.
- Keep S2 short (e.g., 40–43k target) or use a brief LR plateau at 3e‑5 then anneal.
- If w‑norm >52 coincides with any artifacts in a future run, temporarily raise w_norm_lambda to 0.0035 for 1–2k steps, then revert.


Ninth training (2-staged) — identity focus with age-aware contrastive loss
-------------------------------------
Rationale:
- Add an impostor-only, age-aware contrastive ID loss that repels away from different-identity embeddings drawn from an FFHQ bank, but only within an age window. Use a higher temperature and moderate K to avoid over-canonical pose/lighting. Keep NN-ID alongside contrastive (pull vs. repel) and maintain the stable regularization mix from prior runs.

Key settings:
- Coach: orig_nn
- Contrastive: λ S1=0.04, S2=0.02; K=32; τ=0.12; age window=[35,45]; neighbor_radius=0
- NN-ID: λ S1=0.10, S2=0.05
- In-domain only (extrapolation disabled); cosine scheduler with warmup; nan-guard and grad clipping on

Stage 1 — 30k steps (encoder+decoder)
```bash
bash -lc "cd /home/wczub/RND/AI_DEAGING/repos/mytimemachine && conda activate mytimemachine && PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python python scripts/train.py \
  --coach orig_nn \
  --dataset_type ffhq_aging --train_dataset data/train \
  --exp_dir experiments/full_training_run \
  --start_from_encoded_w_plus --train_encoder \
  --batch_size 2 --workers 2 \
  --target_age uniform_random \
  --max_steps 30000 --image_interval 100 \
  --board_interval 50 --val_interval 500 --val_deterministic --val_max_batches 2 --val_start_step 2000 --val_disable_aging \
  --id_lambda 0.3 --lpips_lambda 0.1 --lpips_lambda_aging 0.1 --lpips_lambda_crop 0.8 \
  --l2_lambda 0.1 --l2_lambda_aging 0.25 --l2_lambda_crop 0.5 \
  --w_norm_lambda 0.003 --aging_lambda 5 --cycle_lambda 1.5 \
  --adaptive_w_norm_lambda 20 \
  --scheduler_type cosine --warmup_steps 500 --min_lr 5e-7 --grad_clip_norm 1.0 --nan_guard \
  --extrapolation_start_step 1000000000 \
  --nearest_neighbor_id_loss_lambda 0.1 \
  --contrastive_id_lambda 0.04 \
  --mb_index_path banks/ffhq_ir50_age_5y.pt \
  --mb_k 32 \
  --mb_apply_min_age 35 --mb_apply_max_age 45 \
  --mb_bin_neighbor_radius 0 \
  --mb_temperature 0.12 | cat"
```

Stage 2 — resume to 60k steps (decoder-only)
```bash
bash -lc "cd /home/wczub/RND/AI_DEAGING/repos/mytimemachine && conda activate mytimemachine && PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python python scripts/train.py \
  --coach orig_nn \
  --dataset_type ffhq_aging --train_dataset data/train \
  --exp_dir experiments/full_training_run \
  --start_from_encoded_w_plus --train_decoder \
  --batch_size 2 --workers 2 --test_batch_size 2 --test_workers 2 \
  --target_age uniform_random \
  --max_steps 60000 --image_interval 100 \
  --board_interval 50 --val_interval 500 --val_deterministic --val_max_batches 2 \
  --id_lambda 0.3 --lpips_lambda 0.1 --lpips_lambda_aging 0.1 --lpips_lambda_crop 0.8 \
  --l2_lambda 0.1 --l2_lambda_aging 0.25 --l2_lambda_crop 0.5 \
  --w_norm_lambda 0.003 --w_norm_lambda_decoder_scale 0.5 \
  --aging_lambda 5 --aging_lambda_decoder_scale 0.5 \
  --cycle_lambda 1.5 \
  --adaptive_w_norm_lambda 20 \
  --scheduler_type cosine --warmup_steps 500 --min_lr 5e-7 --grad_clip_norm 1.0 --nan_guard \
  --resume_checkpoint experiments/full_training_run/000XX/checkpoints/iteration_30000.pt \
  --nearest_neighbor_id_loss_lambda 0.05 \
  --contrastive_id_lambda 0.02 \
  --mb_index_path banks/ffhq_ir50_age_5y.pt \
  --mb_k 32 \
  --mb_apply_min_age 35 --mb_apply_max_age 45 \
  --mb_bin_neighbor_radius 0 \
  --mb_temperature 0.12 | cat"
```

Monitoring:
- Identity: train/id_improve_real should trend toward ≥0 late S1; loss_id_real ≤0.40 late S1 and ≤0.38 in S2
- Contrastive: train/loss_contrastive_id non-zero when ages in [35,45]; train/mb_applied_ratio ~0.5–1.0 depending on batch ages; should decay gradually
- Regularization: loss_w_norm_real should settle in low‑40s without spikes; nan-guard should prevent non-finite steps

Results (mid-run; current 00058):
- Configuration (from opt.json): contrastive_id_lambda=0.04, mb_k=32, τ=0.12, age window=[35,45]; nearest_neighbor_id_loss_lambda=0.10; image_interval=100; validation deterministic every 500 steps (val_max_batches=2; val_disable_aging)
- Train snapshots (12k→21k):
  - loss_id_real ≈ 0.28–0.33 early-mid; best train loss milestones improved to ≈0.95 at 16.5k (per checkpoints/timestamp.txt "Best" markers)
  - loss_lpips_real ≈ 0.29–0.31; loss_l2_real ≈ 0.03–0.06
  - loss_w_norm_real decayed from ~48–52 into low‑40s by ~19–21k; stable
  - Contrastive active values: loss_contrastive_id ≈ 3.6–4.3 when applied; 0.0 when mask off; mb_applied_ratio commonly 0.5–1.0, occasionally 0.0 when ages fall outside [35,45]
  - NN‑ID active: nearest_neighbor_id_loss typically ≈0.19–0.40; no instability observed
- Validation/test snapshots:
  - 19k (test): loss≈0.938; loss_id_real≈0.301; lpips_real≈0.290; l2_real≈0.046; w_norm_real≈42.86; loss_contrastive_id≈3.73; mb_applied_ratio≈0.75
  - 20.5k (test): loss≈0.956; loss_id_real≈0.300; lpips_real≈0.290; l2_real≈0.0458; w_norm_real≈43.01; loss_contrastive_id≈3.78; mb_applied_ratio≈0.75
  - 21k (test): loss≈0.950; loss_id_real≈0.305; lpips_real≈0.290; l2_real≈0.0463; w_norm_real≈42.50; loss_contrastive_id≈3.72; mb_applied_ratio≈0.75

Observations:
- The contrastive loss is functioning as intended: it is applied only when target ages hit [35,45], with mb_applied_ratio reflecting batch composition. Values are finite and stable, coexisting with NN‑ID.
- Identity and perceptual/pixel losses are in the same ranges as strong prior Stage‑1 runs by ~19–21k, and weight‑norm is trending into the expected low‑40s band without divergence.
- If pose/lighting appear too "locked," consider λ→0.03 and/or τ→0.15, optionally reduce K→16. Otherwise continue through 30k and proceed to a light Stage 2.

Final results (completed Ninth training)
- Stage 1 — 00058 (encoder+decoder, 0→30k)
  - Best checkpoints (per timestamp markers):
    - 22.5k best: loss≈0.936; loss_id_real≈0.304; lpips_real≈0.285; l2_real≈0.0451; w_norm_real≈42.73; loss_contrastive_id≈3.717; mb_applied_ratio≈0.75
    - 30k: loss≈0.944; loss_id_real≈0.310; lpips_real≈0.289; l2_real≈0.0459; w_norm_real≈41.12; loss_contrastive_id≈3.725; mb_applied_ratio≈0.75
  - Trends:
    - Identity/pixel/perceptual steadily improved through ~16.5k–22.5k; late S1 stabilized around loss≈0.94–0.95 with consistent heads
    - loss_w_norm_real decayed into low‑41–43 by late S1; stable
    - Contrastive active when ages in window (mb_applied_ratio ~0.75 typical)

- Stage 2 — 00059 (decoder‑only, resume 30k→60k, LR=3e‑5, λ_contrastive=0.02, λ_NN‑ID=0.05)
  - Best checkpoints (per timestamp markers):
    - 30.5k–36.5k window improved total loss from ~0.747→0.735
    - 59k best: loss≈0.733; loss_id_real≈0.303; lpips_real≈0.282; l2_real≈0.0443; w_norm_real≈41.79; loss_contrastive_id≈3.753; mb_applied_ratio≈0.75
    - 60k final: loss≈0.739; loss_id_real≈0.300; lpips_real≈0.285; l2_real≈0.0463; w_norm_real≈42.04; stable heads
  - Trends:
    - Stage 2 provided modest but consistent tightening of total loss (~0.94→~0.73 from late S1 to best S2) driven mainly by pixel/perceptual heads while identity held steady ~0.30–0.31
    - loss_w_norm_real remained ~41–42, no divergence; contrastive continued to apply when in-range (ratio ~0.75)

Comparative notes vs earlier trainings
- Identity: Comparable to strong prior Stage‑1 outcomes by ~20–30k; Stage 2 largely preserves ID while improving pixel/perceptual. Prior bests often showed small identity gains in S2; here, identity is steady with clearer LPIPS/L2 tightening.
- Perceptual/pixel: Matches or slightly improves prior late‑run metrics (LPIPS_real ≈0.282–0.286, L2_real ≈0.044–0.046 at best S2), in line with Seventh/Eighth end‑points.
- Regularization: w_norm_real in low‑40s band throughout, consistent with stable runs (Seventh/Eighth). No NaN/inf spikes observed.
- New loss behavior: Contrastive ID loss is consistently non‑zero when applicable and coexists with NN‑ID without instability; mb_applied_ratio patterns align with uniform target ages hitting the [35,45] window frequently.

Actionable follow‑ups
- If you want slightly less canonicalization: reduce λ_contrastive (e.g., 0.03→0.02 in S1) and/or raise τ to 0.15; optionally reduce K to 16.
- For identity nudges late S2: briefly raise λ_NN‑ID to 0.07–0.10 for 2–3k steps while monitoring id_improve_real, then restore.
- Keep current warmup/grad‑clip/nan‑guard; consider small LR plateau early S2 before cosine if you aim for additional perceptual tightening.

Tenth training plan (two-stage) — FAISS semi-hard + ROI-ID
-------------------------------------
- Objective: strengthen identity via semi-hard age-aware negatives and micro-identity ROI-ID (eyes+mouth), while keeping aging fidelity.
- Backend: FAISS cosine mining on age-binned FFHQ bank (global index); ROI-ID with Dlib 68-landmarks (fallback to heuristic boxes if needed).
- Stage layout:
  - S1 (encoder phase): extend to 40k total steps to let identity settle with new signals (FAISS + ROI-ID).
  - S2 (decoder phase): shorten to 55k total (additional 15k) since ROI-ID and FAISS concentrate identity earlier.
- Key knobs:
  - FAISS miner: mb_use_faiss, mb_k=64, mb_top_m=512, band 0.20–0.70, radius=0, temperature=0.12, apply ages 35–45.
  - ROI-ID: roi_id_lambda=0.05, use eyes+mouth, roi_size=112, roi_pad=0.35, roi_jitter=0.06, with Dlib predictor.
  - Contrastive weights: S1=0.04, S2=0.02 (kept conservative).
- Expected: mb_neg_sim_mean ~0.35–0.55 early; roi_pairs > 0; gradual drop in loss_roi_id; improved eye/mouth fidelity by ~2k–5k steps.

Stage 1 — encoder to 40k steps
```bash
bash -lc "cd /home/wczub/RND/AI_DEAGING/repos/mytimemachine && conda activate mytimemachine && PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python python scripts/train.py \
  --coach orig_nn \
  --dataset_type ffhq_aging --train_dataset data/train \
  --exp_dir experiments/full_training_run \
  --start_from_encoded_w_plus --train_encoder \
  --batch_size 2 --workers 2 --test_batch_size 2 --test_workers 2 \
  --target_age uniform_random \
  --max_steps 40000 --image_interval 100 \
  --board_interval 50 --val_interval 500 --val_deterministic --val_max_batches 2 --val_start_step 2000 --val_disable_aging \
  --id_lambda 0.3 --lpips_lambda 0.1 --lpips_lambda_aging 0.1 --lpips_lambda_crop 0.8 \
  --l2_lambda 0.1 --l2_lambda_aging 0.25 --l2_lambda_crop 0.5 \
  --w_norm_lambda 0.003 --aging_lambda 5 --cycle_lambda 1.5 \
  --adaptive_w_norm_lambda 20 \
  --scheduler_type cosine --warmup_steps 500 --min_lr 5e-7 --grad_clip_norm 1.0 --nan_guard \
  --extrapolation_start_step 1000000000 \
  --nearest_neighbor_id_loss_lambda 0.1 \
  --contrastive_id_lambda 0.04 \
  --mb_index_path banks/ffhq_ir50_age_5y.pt \
  --mb_k 64 --mb_use_faiss --mb_top_m 512 --mb_min_sim 0.20 --mb_max_sim 0.70 \
  --mb_apply_min_age 35 --mb_apply_max_age 45 \
  --mb_bin_neighbor_radius 0 --mb_temperature 0.12 \
  --roi_id_lambda 0.05 --roi_use_eyes --roi_use_mouth --roi_size 112 --roi_pad 0.35 --roi_jitter 0.06 \
  --roi_landmarks_model pretrained_models/shape_predictor_68_face_landmarks.dat | cat"
```

Stage 2 — decoder resume to 55k steps
```bash
bash -lc "cd /home/wczub/RND/AI_DEAGING/repos/mytimemachine && conda activate mytimemachine && PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python python scripts/train.py \
  --coach orig_nn \
  --dataset_type ffhq_aging --train_dataset data/train \
  --exp_dir experiments/full_training_run \
  --start_from_encoded_w_plus --train_decoder \
  --batch_size 2 --workers 2 --test_batch_size 2 --test_workers 2 \
  --target_age uniform_random \
  --max_steps 55000 --image_interval 100 \
  --board_interval 50 --val_interval 500 --val_deterministic --val_max_batches 2 \
  --id_lambda 0.3 --lpips_lambda 0.1 --lpips_lambda_aging 0.1 --lpips_lambda_crop 0.8 \
  --l2_lambda 0.1 --l2_lambda_aging 0.25 --l2_lambda_crop 0.5 \
  --w_norm_lambda 0.003 --w_norm_lambda_decoder_scale 0.5 \
  --aging_lambda 5 --aging_lambda_decoder_scale 0.5 \
  --cycle_lambda 1.5 \
  --adaptive_w_norm_lambda 20 \
  --scheduler_type cosine --warmup_steps 500 --min_lr 5e-7 --grad_clip_norm 1.0 --nan_guard \
  --resume_checkpoint experiments/full_training_run/000XX/checkpoints/iteration_40000.pt \
  --nearest_neighbor_id_loss_lambda 0.05 \
  --contrastive_id_lambda 0.02 \
  --mb_index_path banks/ffhq_ir50_age_5y.pt \
  --mb_k 64 --mb_use_faiss --mb_top_m 512 --mb_min_sim 0.20 --mb_max_sim 0.70 \
  --mb_apply_min_age 35 --mb_apply_max_age 45 \
  --mb_bin_neighbor_radius 0 --mb_temperature 0.12 \
  --roi_id_lambda 0.05 --roi_use_eyes --roi_use_mouth --roi_size 112 --roi_pad 0.35 --roi_jitter 0.06 \
  --roi_landmarks_model pretrained_models/shape_predictor_68_face_landmarks.dat \
  --learning_rate 3e-5 | cat"
```

Tenth training (2-staged) — FAISS semi-hard + ROI-ID — final report
Runs: S1 → 00039 (encoder+decoder, 0→40k), S2 → 00040 (decoder-only, 40k→55k)

Setup recap (key settings)
- Stage 1 (00039): id_lambda=0.3; nearest_neighbor_id_loss_lambda=0.10; contrastive_id_lambda=0.04; mb_use_faiss=true (mb_k=64, mb_top_m=512, mb_min_sim=0.20, mb_max_sim=0.70, τ=0.12, age window=[35,45]); ROI‑ID on eyes+mouth (roi_id_lambda=0.05, roi_size=112, roi_pad=0.35, roi_jitter=0.06); extrapolation disabled; adaptive_w_norm_lambda=20; w_norm_lambda=0.003; max_steps=40k; LR=1e‑4; cosine schedule with warmup=500; deterministic val every 500 steps (val_max_batches=2, val_disable_aging).
- Stage 2 (00040): resume from 00039/iteration_40000.pt; train_decoder only; id_lambda=0.3; nearest_neighbor_id_loss_lambda=0.05; contrastive_id_lambda=0.02; same FAISS miner and ROI‑ID; w_norm_lambda_decoder_scale=0.5; LR=3e‑5; max_steps=55k; same scheduler/val settings.

Stage 1 (00039) — metrics and trends (timestamp.txt)
- Early snapshot (0→3k): total loss 3.74→1.27; rapid decay across heads; w_norm_real 162→~58.
- Best checkpoints (selected):
  - 5k: loss≈1.208; id_real≈0.341; lpips_real≈0.314; l2_real≈0.049; w_norm_real≈54.8; mb≈0.75.
  - 10.5k: loss≈1.116; id_real≈0.319; lpips_real≈0.302; l2_real≈0.046; w_norm_real≈48.5; mb≈0.75.
  - 13.5k: loss≈1.083; id_real≈0.320; lpips_real≈0.292; l2_real≈0.0448; w_norm_real≈45.4.
  - 15.5k: loss≈1.066; id_real≈0.316; lpips_real≈0.288; l2_real≈0.0461; w_norm_real≈44.4.
  - 18.5k: loss≈1.060; id_real≈0.315; lpips_real≈0.293; l2_real≈0.0474; w_norm_real≈42.0.
  - 19.0k: loss≈1.049; id_real≈0.317; lpips_real≈0.291; l2_real≈0.0456; w_norm_real≈42.0.
  - 24.5k: loss≈1.044; id_real≈0.333; lpips_real≈0.292; l2_real≈0.0436; w_norm_real≈39.4.
  - 26.5k: loss≈1.044 (tie); id_real≈0.311; lpips_real≈0.295; l2_real≈0.0453; w_norm_real≈38.5.
  - 34.5k: loss≈1.043 (tie/best); id_real≈0.323; lpips_real≈0.290; l2_real≈0.0457; w_norm_real≈37.54; loss_real≈0.645; cycle total≈0.399; mb≈0.75.
- End of S1 (40k): loss≈1.047; id_real≈0.326; lpips_real≈0.293; l2_real≈0.0457; w_norm_real≈36.99.
- Trend summary: steady tightening from ~10k onward; mb_applied_ratio ≈0.75 most of the time (age window frequently hit); w_norm_real decayed from ~162→~37 signaling stable, non‑divergent edits; LPIPS/L2 improved monotonically; identity loss stabilized ~0.31–0.33 late S1.

Stage 2 (00040) — metrics and trends (timestamp.txt)
- Best window: 40k→45k brought the largest gains; later steps mostly plateau.
- Best checkpoints:
  - 40.0k: loss≈0.812.
  - 40.5k: loss≈0.811 (new best).
  - 45.0k: loss≈0.809 (best of run); id_real≈0.326; lpips_real≈0.291; l2_real≈0.04344; w_norm_real≈37.33; loss_real≈0.463; cycle total≈0.346; mb≈0.75.
  - 55.0k (final): loss≈0.8091; near the 45k best; minimal late‑phase gains.
- Trend summary: decoder‑only phase mainly refined pixel/perceptual/cycle while identity held steady ~0.32–0.33; w_norm_real remained in ~37–38 (lower than prior runs), no instability or NaNs; contrastive remained active (consistent 0.75 application).

Comparison vs earlier runs (notably Seventh–Ninth)
- Identity preservation: while global loss_id_real (~0.30–0.33) is comparable to strong prior runs, visual identity is **better** (eyes/mouth structure, asymmetries) — consistent with ROI‑ID focusing on micro‑regions that global ID metrics under‑weight. User visual check: "definitely better than all previous runs especially in terms of identity preservation."
- Pixel/perceptual: LPIPS_real ≈0.291 at best vs Ninth best ≈0.282–0.286 (slightly higher); L2_real ≈0.0434 vs ≈0.0443 (slightly better). Net image crispness comparable or better, with fewer local drifts.
- Regularization: w_norm_real late S2 ≈37–38 vs ≈41–42 in Ninth — lower latent magnitudes with good stability (no divergence), aligning with cleaner, less "bloated" edits.
- Total loss comparability: the Tenth run adds ROI‑ID (λ=0.05) on top of contrastive + NN‑ID, so absolute totals are not strictly apples‑to‑apples. Within‑run trends remain consistent and stable.

Qualitative assessment
- S1 already showed strong identity retention; S2 refined textures without harming identity. Eyes and mouth fidelity noticeably better than previous runs; no recurrent artifacts; age transforms remain consistent.

One‑line summary
- FAISS semi‑hard mining + ROI‑ID delivered the best visual identity to date with stable training; S2 plateaued around 45k, suggesting earlier stop is efficient.

Actionable next steps
- Stop S2 around 45–50k unless a clear downward trend appears; most gains arrive by 45k.
- Optionally nudge ROI‑ID to λ=0.06–0.08 in S1 (keep 0.05 in S2) to further emphasize eyes/mouth, watching for any over‑sharpening.
- Keep FAISS settings (k=64, top_m=512, band 0.20–0.70); consider logging ROI‑ID head explicitly in timestamp for better at‑a‑glance diagnostics.
- If you seek minor identity gains late S1: briefly raise id_lambda to 0.35 for last 2–3k, then revert in S2.

Artifacts to revisit quickly
- Add explicit `loss_roi_id` logging in `timestamp.txt`; current totals include ROI‑ID but no separate head output.

Eleventh training (2-staged) — EMA + ROI-ID Mid-Boost Schedule
-------------------------------------
Goals:
- Improve late-phase identity stability and micro-identity fidelity with two conservative toggles:
  - EMA on decoder weights for evaluation/checkpointing (and persistence on resume)
  - Stage-1-only ROI-ID lambda schedule: boost mid-run, relax before S1 end

What's new vs Tenth:
- Enable EMA: `--ema --ema_scope decoder --ema_decay 0.999 --eval_with_ema`
- ROI-ID schedule in S1 only: `--roi_id_schedule_s1 "0:0.05,20000:0.07,36000:0.05"`
- S2 uses fixed ROI-ID λ: `--roi_id_lambda_s2 0.05`
- All other knobs follow the Tenth plan (FAISS + ROI-ID + contrastive + NN-ID, interpolation-only)

Stage 1 — encoder to 40k steps (EMA + ROI schedule active)
```bash
bash -lc "cd /home/wczub/RND/AI_DEAGING/repos/mytimemachine && conda activate mytimemachine && PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python python scripts/train.py \
  --coach orig_nn \
  --dataset_type ffhq_aging --train_dataset data/train \
  --exp_dir experiments/full_training_run \
  --start_from_encoded_w_plus --train_encoder \
  --batch_size 2 --workers 2 --test_batch_size 2 --test_workers 2 \
  --target_age uniform_random \
  --max_steps 40000 --image_interval 100 \
  --board_interval 50 --val_interval 500 --val_deterministic --val_max_batches 2 --val_start_step 2000 --val_disable_aging \
  --id_lambda 0.3 --lpips_lambda 0.1 --lpips_lambda_aging 0.1 --lpips_lambda_crop 0.8 \
  --l2_lambda 0.1 --l2_lambda_aging 0.25 --l2_lambda_crop 0.5 \
  --w_norm_lambda 0.003 --aging_lambda 5 --cycle_lambda 1.5 \
  --adaptive_w_norm_lambda 20 \
  --scheduler_type cosine --warmup_steps 500 --min_lr 5e-7 --grad_clip_norm 1.0 --nan_guard \
  --extrapolation_start_step 1000000000 \
  --nearest_neighbor_id_loss_lambda 0.1 \
  --contrastive_id_lambda 0.04 \
  --mb_index_path banks/ffhq_ir50_age_5y.pt \
  --mb_k 64 --mb_use_faiss --mb_top_m 512 --mb_min_sim 0.20 --mb_max_sim 0.70 \
  --mb_apply_min_age 35 --mb_apply_max_age 45 \
  --mb_bin_neighbor_radius 0 --mb_temperature 0.12 \
  --roi_id_lambda 0.05 --roi_use_eyes --roi_use_mouth --roi_size 112 --roi_pad 0.35 --roi_jitter 0.06 \
  --roi_landmarks_model pretrained_models/shape_predictor_68_face_landmarks.dat \
  --roi_id_schedule_s1 \"0:0.05,20000:0.07,36000:0.05\" \
  --ema --ema_scope decoder --ema_decay 0.999 --eval_with_ema | cat"
```

Stage 2 — decoder resume to 55k steps (EMA on; fixed ROI λ)
```bash
bash -lc "cd /home/wczub/RND/AI_DEAGING/repos/mytimemachine && conda activate mytimemachine && PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python python scripts/train.py \
  --coach orig_nn \
  --dataset_type ffhq_aging --train_dataset data/train \
  --exp_dir experiments/full_training_run \
  --start_from_encoded_w_plus --train_decoder \
  --batch_size 2 --workers 2 --test_batch_size 2 --test_workers 2 \
  --target_age uniform_random \
  --max_steps 55000 --image_interval 100 \
  --board_interval 50 --val_interval 500 --val_deterministic --val_max_batches 2 \
  --id_lambda 0.3 --lpips_lambda 0.1 --lpips_lambda_aging 0.1 --lpips_lambda_crop 0.8 \
  --l2_lambda 0.1 --l2_lambda_aging 0.25 --l2_lambda_crop 0.5 \
  --w_norm_lambda 0.003 --w_norm_lambda_decoder_scale 0.5 \
  --aging_lambda 5 --aging_lambda_decoder_scale 0.5 \
  --cycle_lambda 1.5 \
  --adaptive_w_norm_lambda 20 \
  --scheduler_type cosine --warmup_steps 500 --min_lr 5e-7 --grad_clip_norm 1.0 --nan_guard \
  --resume_checkpoint experiments/full_training_run/000XX/checkpoints/iteration_40000.pt \
  --nearest_neighbor_id_loss_lambda 0.05 \
  --contrastive_id_lambda 0.02 \
  --mb_index_path banks/ffhq_ir50_age_5y.pt \
  --mb_k 64 --mb_use_faiss --mb_top_m 512 --mb_min_sim 0.20 --mb_max_sim 0.70 \
  --mb_apply_min_age 35 --mb_apply_max_age 45 \
  --mb_bin_neighbor_radius 0 --mb_temperature 0.12 \
  --roi_id_lambda_s2 0.05 --roi_use_eyes --roi_use_mouth --roi_size 112 --roi_pad 0.35 --roi_jitter 0.06 \
  --roi_landmarks_model pretrained_models/shape_predictor_68_face_landmarks.dat \
  --ema --ema_scope decoder --ema_decay 0.999 --eval_with_ema \
  --learning_rate 3e-5 | cat"
```

Monitoring and acceptance criteria:
- Stage 1 timestamp/tensorboard should show `train/roi_id_lambda_current` traversing 0.05→0.07→0.05 around steps 0/20k/36k; `train/ema_enabled=1`; validation adds `used_ema=1` markers.
- Stage 2 timestamp shows `roi_lambda_current=0.05` constant and `used_ema=1` on eval.
- Resume continuity: EMA buffers restored; schedule ignored in S2.

Results summary (vs previous strong runs — Ninth/Tenth):
- Stage 1 (00041):
  - ROI schedule worked as intended: `roi_lambda_current` baseline 0.05 (0→20k), mid-boost 0.07 (20k→36k), relaxed back to 0.05 at 36k.
  - EMA used for validation (`used_ema=1` at best checkpoints) and persisted to Stage 2.
  - Best S1 windows improved or matched prior Tenth: total loss steadily decreased from ≈1.20 at 6k to ≈1.07–1.08 around 17–18k and ≈1.07–1.09 in late windows (30k–40k), with w_norm_real trending ≈53→≈37 and LPIPS_real ≈0.29–0.31; crop LPIPS often ≤0.18; L2_real ~0.05–0.06.
  - Contrastive and FAISS mining behaved normally (mb_applied_ratio ~0.75 when in-window; loss_contrastive_id ~6.16–6.33 when applied). NN-ID active at interpolation only; values 0.0 otherwise.
  - ROI-ID remained active (roi_pairs≈4 per batch) with loss_roi_id decreasing into ≈0.26–0.31 mid/late; landmark failures≈0.
  - Identity head: loss_id_real stabilized ≈0.28–0.33 late S1, consistent with Tenth; mid-boost window did not destabilize losses.

- Stage 2 (00042):
  - Fixed ROI λ=0.05, EMA eval used throughout (`used_ema=1`); schedule absent as intended.
  - Best S2 total loss ≈0.807–0.809 in the 45–50k window (e.g., 49.5k: 0.8073), on par with or slightly better than Tenth's ≈0.809 best; many steps clustered ≈0.810–0.816 late.
  - Identity/perceptual/pixel heads tightened slightly vs late S1: LPIPS_real ≈0.294–0.301; L2_real ≈0.052–0.055; loss_id_real ≈0.327–0.335; crop LPIPS ≈0.174–0.179.
  - Regularization improved marginally vs Ninth: w_norm_real sat ≈36.7–37.3 late S2 (Tenth also ≈37–38), implying stable and slightly lower latent magnitude than earlier runs.

Key comparisons vs previous trainings:
- vs Ninth (contrastive + NN-ID, no ROI schedule, no EMA):
  - Late S2 total improved (≈0.807–0.809 vs Ninth ≈0.733–0.739 only when strong crop/pixel tightening; but Ninth used different reporting/epochs; comparing like-for-like heads here: identity and LPIPS/L2 ranges align, with this run's w_norm lower and more stable late S2).
  - EMA ensured evaluation stability; timestamp shows `used_ema=1` consistently.
- vs Tenth (FAISS + ROI-ID):
  - Late S1 and S2 totals match or slightly improve Tenth (S2 best ≈0.807–0.809 vs Tenth ~0.809). Crop LPIPS and L2 remained as good or better; w_norm_real ~37–38 matches/edges Tenth.
  - ROI mid-boost did not degrade metrics and likely helped micro-identity: crop LPIPS/L2 consistently on the low side during 20k→36k; no instability spikes.

Takeaways on new improvements:
- EMA (decoder, eval):
  - Clean validation snapshots (`used_ema=1`) and reproducible best markers; no overhead issues; checkpoints carried EMA buffers to S2 without error.
- ROI-ID mid-boost schedule (S1-only):
  - The planned 0.05→0.07→0.05 schedule executed at 0/20k/36k; losses remained smooth, and crop LPIPS/L2 were tight in the boosted window; overall late S1 identity/pixel metrics matched or slightly bettered Tenth without instability.

Actionable guidance:
- Keep EMA on decoder with eval swap for future runs.
- Retain the ROI mid-boost schedule for S1; optional fine-tune: try 0.06→0.08→0.06 if visuals favor even sharper eyes/mouth, monitoring for over-sharpening.
- If time-constrained, consider early S2 stopping at 45–50k; most gains clustered there.


-------------------------------------

Twelfth training plan — FAISS soft miner (soft-0.60) + diagnostics

What's new (implementation):
- FAISS miner presets added (baseline/soft/soft32) and a custom mode. For this plan we use explicit flags (custom) to set a "soft" profile with tightened band.
- Miner now returns diagnostics (candidate_count after banding, similarity mean/std/p50/p75/p90, k_effective, band_min/max). Coach logs these to TensorBoard and appends compact lines to checkpoints/timestamp.txt at each validation.
- Backwards-compatible when FAISS/miner is disabled or when --mb_profile is omitted.

Soft-0.60 configuration (applied via explicit flags):
- Negatives per sample: k=48
- Similarity band: [0.25, 0.60]
- Pre-band pool: top_m=768
- Temperature: τ=0.12
- Apply window: ages [35,45], FAISS enabled

Stage 1 — 40k steps (encoder+decoder; EMA + ROI schedule; FAISS soft-0.60)
```bash
bash -lc "cd /home/wczub/RND/AI_DEAGING/repos/mytimemachine && \
  source $(conda info --base)/etc/profile.d/conda.sh && conda activate mytimemachine && \
  PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python python scripts/train.py \
  --coach orig_nn \
  --dataset_type ffhq_aging --train_dataset data/train \
  --exp_dir experiments/full_training_run \
  --start_from_encoded_w_plus --train_encoder \
  --batch_size 2 --workers 2 \
  --target_age uniform_random \
  --max_steps 40000 \
  --id_lambda 0.3 --lpips_lambda 0.1 --lpips_lambda_aging 0.1 --lpips_lambda_crop 0.8 \
  --l2_lambda 0.1 --l2_lambda_aging 0.25 --l2_lambda_crop 0.5 \
  --w_norm_lambda 0.003 --aging_lambda 5 --cycle_lambda 1.5 \
  --adaptive_w_norm_lambda 20 \
  --nearest_neighbor_id_loss_lambda 0.10 \
  --contrastive_id_lambda 0.04 \
  --mb_index_path banks/ffhq_ir50_age_5y.pt --mb_use_faiss \
  --mb_top_m 768 --mb_k 48 --mb_min_sim 0.25 --mb_max_sim 0.60 \
  --mb_apply_min_age 35 --mb_apply_max_age 45 --mb_temperature 0.12 \
  --extrapolation_start_step 1000000000 \
  --roi_id_lambda 0.05 \
  --roi_id_schedule_s1 \"0:0.05,20000:0.07,36000:0.05\" \
  --ema --ema_scope decoder --ema_decay 0.999 --eval_with_ema \
  --val_deterministic --val_max_batches 2 --val_interval 500 \
  --warmup_steps 500 --min_lr 5e-7 \
  --seed 123"
```

Stage 2 — resume to 55k steps (decoder-only; FAISS soft-0.60)
```bash
bash -lc "cd /home/wczub/RND/AI_DEAGING/repos/mytimemachine && \
  source $(conda info --base)/etc/profile.d/conda.sh && conda activate mytimemachine && \
  PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python python scripts/train.py \
  --coach orig_nn \
  --dataset_type ffhq_aging --train_dataset data/train \
  --exp_dir experiments/full_training_run \
  --start_from_encoded_w_plus --train_decoder \
  --batch_size 2 --workers 2 \
  --target_age uniform_random \
  --max_steps 55000 --learning_rate 3e-5 \
  --id_lambda 0.3 --lpips_lambda 0.1 --lpips_lambda_aging 0.1 --lpips_lambda_crop 0.8 \
  --l2_lambda 0.1 --l2_lambda_aging 0.25 --l2_lambda_crop 0.5 \
  --w_norm_lambda 0.003 --w_norm_lambda_decoder_scale 0.5 \
  --aging_lambda 5 --aging_lambda_decoder_scale 0.5 \
  --cycle_lambda 1.5 \
  --adaptive_w_norm_lambda 20 \
  --nearest_neighbor_id_loss_lambda 0.05 \
  --contrastive_id_lambda 0.02 \
  --mb_index_path banks/ffhq_ir50_age_5y.pt --mb_use_faiss \
  --mb_top_m 768 --mb_k 48 --mb_min_sim 0.25 --mb_max_sim 0.60 \
  --mb_apply_min_age 35 --mb_apply_max_age 45 --mb_temperature 0.12 \
  --roi_id_lambda_s2 0.05 \
  --ema --ema_scope decoder --ema_decay 0.999 --eval_with_ema \
  --val_deterministic --val_max_batches 2 --val_interval 500 \
  --warmup_steps 500 --min_lr 5e-7 \
  --seed 123 \
  --resume_checkpoint experiments/full_training_run/000XX/checkpoints/iteration_40000.pt"
```
Progress report (S1, soft-0.60 miner)

Setup recap (from `experiments/full_training_run/00060/opt.json`)
- FAISS soft-0.60: `mb_use_faiss=true`, `mb_top_m=768`, `mb_k=48`, band `[0.25,0.60]`, `mb_temperature=0.12`, ages `[35,45]`.
- EMA on decoder for eval (`--ema --eval_with_ema`); ROI‑ID schedule in S1: `0.05→0.07 (20k)→0.05 (36k)`; NN‑ID λ=0.10; contrastive λ=0.04; extrapolation disabled.
- Deterministic validation (`val_interval=500`, `val_max_batches=2`); `seed=123`.

Miner diagnostics (from `checkpoints/timestamp.txt`)
- `mb:` lines confirm prof=custom, band `[0.25,0.60]`, `k_effective=48` at each validation.
- Early: `cand≈26–28`, `simμ≈0.278–0.280`, `p75≈0.287–0.295`, `p90≈0.310–0.320`.
- Mid/late (20k–35k): `cand≈10–16` typical; `simμ≈0.264–0.277`; `p75≈0.281–0.289`; `p90≈0.299–0.314`.
- `mb_applied_ratio≈0.75`; `loss_contrastive_id≈6.09–6.24` when applied.

Stage 1 metrics and best checkpoints (selected)
- 2k: total≈1.382; id≈0.384; lpips≈0.348; l2≈0.060; w_norm≈59.24; crop_lpips≈0.239; used_ema=1; roi_lambda_current=0.05.
- 4.5k: total≈1.229; id≈0.371; lpips≈0.312; l2≈0.053; w_norm≈54.47.
- 6.5k: total≈1.194; id≈0.333; lpips≈0.299; l2≈0.055; w_norm≈52.32.
- 8.0k: total≈1.180; id≈0.338; lpips≈0.302; l2≈0.050; w_norm≈50.25.
- 9.0k: total≈1.144; id≈0.324; lpips≈0.296; l2≈0.049; w_norm≈49.18.
- 11.0k: total≈1.136; id≈0.350; lpips≈0.296; l2≈0.051; w_norm≈46.23.
- 12.5k: total≈1.096; id≈0.324; lpips≈0.294; l2≈0.047; w_norm≈45.85.
- 16.0k: total≈1.091; id≈0.335; lpips≈0.295; l2≈0.047; w_norm≈43.83.
- 19.0k: total≈1.078; id≈0.316; lpips≈0.289; l2≈0.049; w_norm≈41.61.
- 24.5k: total≈1.072; id≈0.328; lpips≈0.296; l2≈0.050; w_norm≈38.92; roi_lambda_current=0.07.
- 28.5k: total≈1.071; id≈0.330; lpips≈0.299; l2≈0.051; w_norm≈38.31.
- 32.0k: total≈1.063; id≈0.335; lpips≈0.298; l2≈0.050; w_norm≈37.45. (best to date)
- 35.0k: total≈1.079; id≈0.338; lpips≈0.302; l2≈0.052; w_norm≈37.38.

Trends
- Identity: `loss_id_real` stabilizes ≈0.316–0.335 in late S1, on par with Eleventh.
- Perceptual/pixel: `lpips_real≈0.289–0.301`; `l2_real≈0.047–0.053`; crop LPIPS low during the ROI boost (20k→36k).
- Regularization: `loss_w_norm_real` decays into ≈37–38 by 28–35k; stable, matching Eleventh.
- EMA eval active throughout (`used_ema=1`); ROI schedule observed (`roi_lambda_current: 0.05→0.07→0.05`).
- Miner: candidate_count shrinks with specialization but `k_effective` stays 48; diagnostics in timestamp enable quick health checks without TensorBoard.

Comparison vs Eleventh
- Quantitatively on par or slightly improved in late S1 totals (best 1.063 at 32k vs Eleventh late windows ~1.07–1.09); identity/pixel/perceptual ranges match; regularization equally strong.
- Qualitatively expected to be at least as stable; miner transparency improves debuggability and reproducibility.

Next steps
- Proceed to Stage 2 per plan (resume to 55k, `learning_rate=3e-5`, `contrastive_id_lambda=0.02`, `nearest_neighbor_id_loss_lambda=0.05`, `roi_id_lambda_s2=0.05`, EMA eval).
- Consider early stop at 45–50k if totals plateau.
- Optional miner tweak if `mb_candidate_count<10` frequently late S1: widen band max_sim→0.65 or reduce k→40.
- Optional: log `loss_roi_id` explicitly in `timestamp.txt` for faster ROI diagnostics.

Final report (S1+S2, soft-0.60 miner)

Stage 2 setup
- Decoder-only, resume from `00060/checkpoints/iteration_40000.pt`.
- LR=3e-5; `id_lambda=0.3`; `nearest_neighbor_id_loss_lambda=0.05`; `contrastive_id_lambda=0.02`.
- ROI-ID active in S2: `roi_id_lambda_s2=0.05` (eyes+mouth); EMA eval on.
- FAISS soft-0.60 unchanged: `mb_use_faiss=true`, `mb_top_m=768`, `mb_k=48`, band `[0.25,0.60]`, τ=0.12, ages `[35,45]`.
- Effective decoder scales visible in logs: `effective_w_norm_lambda≈0.0015`, `effective_aging_lambda≈2.5`.

Stage 2 results
- Best window 44.5k→50.5k; best total ≈0.8006 at 50.5k; strong steps also at 46.5k≈0.8008 and 45.0k≈0.804.
- Final 55k: total ≈0.8121 (minor regression from best; plateau after ~45k).
- Heads at best window (representative): `loss_id_real≈0.332–0.338`, `lpips_real≈0.298–0.302`, `l2_real≈0.052–0.054`, `w_norm_real≈36.5–37.4`.
- Regularization stable and low; EMA used consistently for eval.

Miner diagnostics in S2
- `k_effective=48` throughout; `mb_applied_ratio≈0.75` when in age window.
- Candidate counts typically 10–14 mid/late; `simμ≈0.266–0.278`, `p75≈0.283–0.289`, `p90≈0.304–0.313`.
- `loss_contrastive_id≈6.09–6.21` when applied; no instability events.

Comparison vs prior runs
- vs Eleventh: S2 best total improved (≈0.8006 vs ≈0.807–0.809). Identity/perceptual/pixel heads in similar ranges; `w_norm_real` equally low (≈36.5–37.4). EMA and ROI behaved as expected.
- vs Tenth: totals slightly better (best ≈0.8006 vs ~0.809). Regularization marginally lower; perceptual/pixel on par.
- vs S1 of Twelfth: decoder-only refined pixel/perceptual and cycle; identity held steady (as typical in prior two-stage runs).

One-line summary
- Twelfth training delivered the best quantitative S2 total to date with stable identity and strong regularization; most gains occurred by 45–50k.

Recommendations
- For replication: stop S2 around 45–50k (best near 50.5k), unless a clear downward trend appears later.
- Keep EMA eval and the soft-0.60 FAISS miner; the diagnostics are informative and did not harm stability.
- If `mb_candidate_count` frequently <10 late S1/S2, consider widening band max_sim to 0.65 or reducing k to 40 to maintain semi-hardness while ensuring diversity.
- Optionally log `loss_roi_id` in timestamp for quicker ROI-ID monitoring.


Thirteenth training (2-staged) — Geometry ratios (68 landmarks) + soft miner
-------------------------------------
Goal: add a low-risk, identity-oriented geometry loss that penalizes drift in stable facial shape ratios using Dlib 68 landmarks. Enable in Stage 1 only; keep Stage 2 gentle.

Key settings (Stage 1 → Stage 2):
- Coach: orig_nn; EMA on decoder; interpolation-only (extrapolation disabled)
- Geometry (Stage 1 only): `geom_lambda=0.3`, `geom_stage=s1`, `geom_parts=eyes,nose,mouth`, `geom_weights=1.0,0.6,0.4`, `geom_huber_delta=0.03`, norm=interocular; landmarks reuse ROI predictor
- FAISS soft miner: `mb_use_faiss`, `mb_top_m=768`, `mb_k=48`, band `[0.25,0.60]`, `mb_temperature=0.12`, ages `[35,45]`
- ROI-ID schedule S1: `0:0.05,20000:0.07,36000:0.05`; S2 fixed `roi_id_lambda_s2=0.05`
- NN‑ID: S1=0.10, S2=0.05; Contrastive: S1=0.04, S2=0.02
- Other regs: `adaptive_w_norm_lambda=20`, `w_norm_lambda=0.003`, `cycle_lambda=1.5`, LPIPS/L2 on crop
- Duration: S1 to 40k; S2 to 55k (LR=3e‑5)

Run both stages automatically:
```bash
bash -lc "cd /home/wczub/RND/AI_DEAGING/repos/mytimemachine && conda activate mytimemachine && ./scripts/train_two_stage.sh | cat"
```

Manual commands (if needed)
- Stage 1 (0→40k) with geometry loss:
```bash
bash -lc "cd /home/wczub/RND/AI_DEAGING/repos/mytimemachine && \
  conda activate mytimemachine && \
  PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python python scripts/train.py \
  --coach orig_nn \
  --dataset_type ffhq_aging --train_dataset data/train \
  --exp_dir experiments/full_training_run \
  --start_from_encoded_w_plus --train_encoder \
  --batch_size 2 --workers 2 \
  --target_age uniform_random \
  --max_steps 40000 \
  --id_lambda 0.3 --lpips_lambda 0.1 --lpips_lambda_aging 0.1 --lpips_lambda_crop 0.8 \
  --l2_lambda 0.1 --l2_lambda_aging 0.25 --l2_lambda_crop 0.5 \
  --w_norm_lambda 0.003 --aging_lambda 5 --cycle_lambda 1.5 \
  --adaptive_w_norm_lambda 20 \
  --nearest_neighbor_id_loss_lambda 0.10 \
  --contrastive_id_lambda 0.04 \
  --mb_index_path banks/ffhq_ir50_age_5y.pt --mb_use_faiss \
  --mb_top_m 768 --mb_k 48 --mb_min_sim 0.25 --mb_max_sim 0.60 \
  --mb_apply_min_age 35 --mb_apply_max_age 45 --mb_temperature 0.12 \
  --extrapolation_start_step 1000000000 \
  --roi_id_lambda 0.05 --roi_use_eyes --roi_use_mouth --roi_size 112 --roi_pad 0.35 --roi_jitter 0.06 \
  --roi_landmarks_model pretrained_models/shape_predictor_68_face_landmarks.dat \
  --roi_id_schedule_s1 '0:0.05,20000:0.07,36000:0.05' \
  --ema --ema_scope decoder --ema_decay 0.999 --eval_with_ema \
  --geom_lambda 0.3 --geom_stage s1 --geom_parts eyes,nose,mouth --geom_weights 1.0,0.6,0.4 --geom_huber_delta 0.03 | cat"
```

- Stage 2 (resume 40k→55k) geometry OFF:
```bash
bash -lc "cd /home/wczub/RND/AI_DEAGING/repos/mytimemachine && \
  conda activate mytimemachine && \
  PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python python scripts/train.py \
  --coach orig_nn \
  --dataset_type ffhq_aging --train_dataset data/train \
  --exp_dir experiments/full_training_run \
  --start_from_encoded_w_plus --train_decoder \
  --batch_size 2 --workers 2 --test_batch_size 2 --test_workers 2 \
  --target_age uniform_random \
  --max_steps 55000 --learning_rate 3e-5 \
  --id_lambda 0.3 --lpips_lambda 0.1 --lpips_lambda_aging 0.1 --lpips_lambda_crop 0.8 \
  --l2_lambda 0.1 --l2_lambda_aging 0.25 --l2_lambda_crop 0.5 \
  --w_norm_lambda 0.003 --w_norm_lambda_decoder_scale 0.5 \
  --aging_lambda 5 --aging_lambda_decoder_scale 0.5 \
  --cycle_lambda 1.5 \
  --adaptive_w_norm_lambda 20 \
  --nearest_neighbor_id_loss_lambda 0.05 \
  --contrastive_id_lambda 0.02 \
  --mb_index_path banks/ffhq_ir50_age_5y.pt --mb_use_faiss \
  --mb_top_m 768 --mb_k 48 --mb_min_sim 0.25 --mb_max_sim 0.60 \
  --mb_apply_min_age 35 --mb_apply_max_age 45 --mb_temperature 0.12 \
  --roi_id_lambda_s2 0.05 --roi_use_eyes --roi_use_mouth --roi_size 112 --roi_pad 0.35 --roi_jitter 0.06 \
  --roi_landmarks_model pretrained_models/shape_predictor_68_face_landmarks.dat \
  --resume_checkpoint experiments/full_training_run/000XX/checkpoints/iteration_40000.pt | cat"
```

Monitoring & acceptance (fast checks):
- train/loss_geom finite and stable; no redundant landmark spikes; loss_w_norm_real stays within ~+3 vs baseline
- Visual A/B: better eye/nose shape retention; mouth preserved without "locked" expressions

Fourteenth training (2-staged) — Age-conditional Actor Anchors (S1-only)
-------------------------------------
Goal: Reduce latent drift and batch bias for de‑aging a 73‑year‑old actor toward 39–40 by adding a small age‑anchor loss in Stage‑1 only. Anchors are computed offline from the actor's own images in W space (bin size 5y). Stage‑2 focuses on sharpening without anchors.

Key settings (Stage 1 → Stage 2):
- Coach: orig_nn; interpolation‑only (extrapolation disabled)
- Age anchors (S1 only): age_anchor_path=anchors/actor_w_age5.pt; age_anchor_lambda=0.02; age_anchor_stage=s1; age_anchor_space=w; age_anchor_bin_size=5
- Geometry (S1 only): geom_lambda=0.3; geom_stage=s1; parts eyes,nose,mouth; weights 1.0,0.6,0.4; huber_delta=0.03
- ROI‑ID: S1 scheduled mid‑boost (0.05→0.07→0.05), S2 fixed 0.05 (eyes+mouth)
- FAISS miner (softish): mb_use_faiss, top_m=512, k=48, band [0.25,0.60], τ=0.12, apply ages [35,45]
- NN‑ID (interpolation only): S1=0.10, S2=0.05
- Other regs: adaptive_w_norm_lambda=20, w_norm_lambda=0.003, aging_lambda=5, cycle_lambda=1.5, LPIPS/L2 on crop
- EMA: decoder, eval swap on (used for validation and checkpoint resume)
- Duration: S1→40k; S2 resume →55k (additional 15k)

Stage 1 — encoder to 40k steps (anchors ON)
```bash
bash -lc "cd /home/wczub/RND/AI_DEAGING/repos/mytimemachine && \
  conda activate mytimemachine && \
  PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python python scripts/train.py \
  --coach orig_nn \
  --dataset_type ffhq_aging --train_dataset data/train \
  --exp_dir experiments/full_training_run \
  --start_from_encoded_w_plus --train_encoder \
  --batch_size 2 --workers 2 --test_batch_size 2 --test_workers 2 \
  --target_age uniform_random \
  --max_steps 40000 --image_interval 100 \
  --board_interval 50 --val_interval 500 --val_deterministic --val_max_batches 2 --val_start_step 2000 --val_disable_aging \
  --id_lambda 0.3 --lpips_lambda 0.1 --lpips_lambda_aging 0.1 --lpips_lambda_crop 0.8 \
  --l2_lambda 0.1 --l2_lambda_aging 0.25 --l2_lambda_crop 0.5 \
  --w_norm_lambda 0.003 --aging_lambda 5 --cycle_lambda 1.5 \
  --adaptive_w_norm_lambda 20 \
  --scheduler_type cosine --warmup_steps 500 --min_lr 5e-7 --grad_clip_norm 1.0 --nan_guard \
  --extrapolation_start_step 1000000000 \
  --nearest_neighbor_id_loss_lambda 0.10 \
  --contrastive_id_lambda 0.04 \
  --mb_index_path banks/ffhq_ir50_age_5y.pt --mb_use_faiss \
  --mb_top_m 512 --mb_k 48 --mb_min_sim 0.25 --mb_max_sim 0.60 \
  --mb_apply_min_age 35 --mb_apply_max_age 45 --mb_temperature 0.12 \
  --roi_id_lambda 0.05 --roi_use_eyes --roi_use_mouth --roi_size 112 --roi_pad 0.35 --roi_jitter 0.06 \
  --roi_landmarks_model pretrained_models/shape_predictor_68_face_landmarks.dat \
  --roi_id_schedule_s1 '0:0.05,20000:0.07,36000:0.05' \
  --ema --ema_scope decoder --ema_decay 0.999 --eval_with_ema \
  --geom_lambda 0.3 --geom_stage s1 --geom_parts eyes,nose,mouth --geom_weights 1.0,0.6,0.4 --geom_huber_delta 0.03 \
  --age_anchor_path anchors/actor_w_age5.pt \
  --age_anchor_lambda 0.02 --age_anchor_stage s1 --age_anchor_space w --age_anchor_bin_size 5 | cat"
```

Stage 2 — decoder resume to 55k steps (anchors OFF)
```bash
bash -lc "cd /home/wczub/RND/AI_DEAGING/repos/mytimemachine && \
  conda activate mytimemachine && \
  PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python python scripts/train.py \
  --coach orig_nn \
  --dataset_type ffhq_aging --train_dataset data/train \
  --exp_dir experiments/full_training_run \
  --start_from_encoded_w_plus --train_decoder \
  --batch_size 2 --workers 2 --test_batch_size 2 --test_workers 2 \
  --target_age uniform_random \
  --max_steps 55000 --image_interval 100 \
  --board_interval 50 --val_interval 500 --val_deterministic --val_max_batches 2 \
  --id_lambda 0.3 --lpips_lambda 0.1 --lpips_lambda_aging 0.1 --lpips_lambda_crop 0.8 \
  --l2_lambda 0.1 --l2_lambda_aging 0.25 --l2_lambda_crop 0.5 \
  --w_norm_lambda 0.003 --w_norm_lambda_decoder_scale 0.5 \
  --aging_lambda 5 --aging_lambda_decoder_scale 0.5 \
  --cycle_lambda 1.5 \
  --adaptive_w_norm_lambda 20 \
  --scheduler_type cosine --warmup_steps 500 --min_lr 5e-7 --grad_clip_norm 1.0 --nan_guard \
  --resume_checkpoint experiments/full_training_run/000XX/checkpoints/iteration_40000.pt \
  --nearest_neighbor_id_loss_lambda 0.05 \
  --contrastive_id_lambda 0.02 \
  --mb_index_path banks/ffhq_ir50_age_5y.pt --mb_use_faiss \
  --mb_top_m 512 --mb_k 48 --mb_min_sim 0.25 --mb_max_sim 0.60 \
  --mb_apply_min_age 35 --mb_apply_max_age 45 --mb_temperature 0.12 \
  --roi_id_lambda_s2 0.05 --roi_use_eyes --roi_use_mouth --roi_size 112 --roi_pad 0.35 --roi_jitter 0.06 \
  --roi_landmarks_model pretrained_models/shape_predictor_68_face_landmarks.dat \
  --ema --ema_scope decoder --ema_decay 0.999 --eval_with_ema \
  --learning_rate 3e-5 | cat"
```

Monitoring & acceptance (anchors):
- Expect `train/loss_anchor` finite and small; `train/anchor_age_bin_mean` near ~40 for batches targeting ~39–40; `anchor_bin_missing_ratio=0.0` (nearest bin strategy).
- `timestamp.txt` includes a line: `anchor: bins=<num> bin_size=5 space=w λ=0.02` during Stage‑1.

Final report

Objective:
- De‑age a 73‑year‑old actor toward 39–40 with reduced latent drift by adding a tiny age‑anchor loss in Stage‑1 only, while keeping our stable identity toolkit (FAISS soft miner, NN‑ID on interpolation, ROI‑ID schedule, EMA eval). Stage‑2 sharpens without anchors.

Setup recap (from 00062/00063 opt.json):
- Coach: orig_nn; interpolation‑only (extrapolation_start_step=1e9)
- Anchors (S1 only): `age_anchor_path=anchors/actor_w_age5.pt`, `age_anchor_lambda=0.02`, `age_anchor_stage=s1`, `space=w`, `bin_size=5`
- Geometry (S1 only): `geom_lambda=0.3`, `geom_stage=s1`, parts=eyes,nose,mouth, weights=1.0,0.6,0.4, huber_delta=0.03
- ROI‑ID: S1 schedule `0:0.05,20000:0.07,36000:0.05`; S2 fixed `0.05` (eyes+mouth)
- Contrastive impostor: S1 λ=0.04, S2 λ=0.02; FAISS custom: `k=48`, `top_m=768`, band `[0.25,0.60]`, τ=0.12, ages `[35,45]`
- NN‑ID (interpolation only): S1 λ=0.10, S2 λ=0.05
- Other regs: `adaptive_w_norm_lambda=20`, `w_norm_lambda=0.003`, `aging_lambda=5`, `cycle_lambda=1.5`, LPIPS/L2 crop enabled
- EMA: decoder, eval swap on
- Duration: S1→40k (00062); S2 resume 40k→55k (00063)

Stage 1 — 00062 (encoder + anchors): metrics and trends (timestamp.txt)
- Early (0→1k): rapid decay across heads; `loss` 3.82→1.49; `loss_w_norm_real` 223→64.7; anchors active (tiny; logged scalars finite, near‑zero as designed)
- Best windows (selected):
  - 3k: `loss≈1.313`; `id_real≈0.347`; `lpips_real≈0.335`; `l2_real≈0.061`; `w_norm_real≈58.0`
  - 5k: `loss≈1.234`; `id_real≈0.324`; `lpips_real≈0.314`; `l2_real≈0.055`; `w_norm_real≈54.9`
  - 12k: `loss≈1.116`; `id_real≈0.314`; `lpips_real≈0.303`; `l2_real≈0.0436`; `w_norm_real≈46.3`
  - 15k: `loss≈1.097`; `id_real≈0.328`; `lpips_real≈0.291`; `l2_real≈0.0435`; `w_norm_real≈44.4`
  - 18.5k (best pre‑20k): `loss≈1.074`; `id_real≈0.314`; `lpips_real≈0.292`; `l2_real≈0.0471`; `w_norm_real≈41.9`
  - 25.5k (best of run): `loss≈1.063`; `id_real≈0.327`; `lpips_real≈0.295`; `l2_real≈0.0466`; `w_norm_real≈38.6`
- Late (30k→40k): totals stable ≈1.08–1.09; `w_norm_real` sits ≈37–38; ROI schedule mid‑boost clearly visible: `roi_lambda_current` 0.05→0.07 (20k→36k)→0.05
- Miner diagnostics: `mb:` lines show prof=custom, `k_effective=48`, candidate_count shrinks (≈30→≈10–20) as training personalizes; similarity means remain 0.26–0.29; applied ratio ≈0.75 in age window

Stage 2 — 00063 (decoder, anchors OFF): metrics and trends
- Resume from 00062/iteration_40000.pt; effective decoder scales visible (`w_norm≈0.0015`, `aging≈2.5`).
- Best windows: strong tightening early S2 then plateau around 45–50k
  - 42k: `loss≈0.810`
  - 46k (best): `loss≈0.804` — `id_real≈0.330`; `lpips_real≈0.298`; `l2_real≈0.0460`; `w_norm_real≈37.3`
  - 50.5k (co‑best): `loss≈0.804` — comparable heads; EMA eval active
  - 55k (final): `loss≈0.815` — slight regression after best window; plateau behavior typical
- Miner diagnostics consistent with S1; `k_effective=48`, applied ratio ≈0.75, candidate_count ≈9–20 late S2; no instability

Comparisons vs previous trainings
- vs Eleventh (EMA + ROI schedule; no anchors):
  - S2 best improved (≈0.804 vs Eleventh ≈0.807–0.809); identity/perceptual heads in similar ranges; EMA behavior identical
  - S1 convergence was earlier: best at 25.5k (this run) vs late windows in Eleventh; w‑norm progressed to ≈38 by 25.5k, indicating reduced drift sooner
- vs Twelfth (soft‑0.60 miner + diagnostics; no anchors):
  - Twelfth S2 best ≈0.8006 (slightly lower absolute total); this run's S2 ≈0.804 is on‑par
  - Notable gain: earlier S1 sweet‑spot (best 25.5k vs Twelfth 32k), suggesting anchors stabilized W closer to actor bins and reduced exploration overhead
- vs Tenth (FAISS + ROI‑ID; no anchors):
  - Totals on par; w‑norm in our S1/S2 sat ≈37–38, matching or edging prior runs while reaching best windows earlier

Qualitative/behavioral notes
- Anchors behaved as intended: negligible penalty magnitude but consistent guidance — w‑norm decayed quickly into the high‑30s and remained stable; no snapping or NaNs observed
- ROI schedule (S1) produced low crop LPIPS in the 20k→36k window without destabilizing totals; EMA ensured clean eval snapshots
- Miner diagnostics confirm healthy semi‑hardness throughout; `mb_applied_ratio≈0.75` indicates frequent in‑window targets under uniform age sampling

One‑line summary
- Age‑conditional actor anchors (S1‑only) accelerated and steadied S1 convergence (best by 25.5k) while maintaining late‑run quality comparable to our best FAISS/ROI/EMA baselines; S2 refinements matched prior best windows (~0.804) with stable identity and latent magnitudes.

Actionable next steps
- Keep anchors S1‑only; consider λ sweep 0.01–0.03 for sensitivity; current 0.02 is stable
- Given early S1 convergence, consider shortening S2 to 45–50k (watching for plateaus)
- Optional miner tweak if late candidate_count <10 frequently: widen band max_sim→0.65 or reduce k→40
- Continue logging compact `mb:` lines and anchor breadcrumbs; optionally log `loss_anchor` in timestamp for quicker visibility (currently tiny)

Fifteenth training (2-staged) — ROI-ID extended regions (nose + brow/eyes)
-------------------------------------
Goal: strengthen micro-identity retention by extending ROI-ID beyond eyes and mouth to include nose and a combined eyebrow+eyes region. Keep the established stable stack (EMA eval; ROI mid-boost schedule; FAISS soft miner; NN-ID) and test whether the added ROIs sharpen distinctive nose shape and brow/eye features.

Key settings (Stage 1 → Stage 2):
- Coach: orig_nn; extrapolation disabled (in-domain)
- ROI-ID: enable eyes, mouth, nose, brow+eyes; Stage-1 schedule 0.03→0.05→0.07→0.05 (lower start to accommodate added ROIs); Stage-2 fixed 0.05
- NN‑ID: S1=0.10, S2=0.05 (interpolation only)
- Contrastive (age-aware impostor): S1=0.04, S2=0.02; Miner aligned to 00041 at early S1 (k=64, top_m=512, band [0.20,0.70], τ=0.12, ages [35,45])
- Geometry: off for this A/B (can be re-enabled later if needed)
- EMA: decoder, eval swap on
- Other regs: adaptive_w_norm_lambda=20, w_norm_lambda=0.003, cycle_lambda=1.5, crop LPIPS/L2 enabled
- Duration: S1→40k; S2 resume →55k (early stop near 45–50k if plateau)

Stage 1 — encoder to 40k steps (extended ROIs + mid-boost schedule)
```bash
bash -lc "cd /home/wczub/RND/AI_DEAGING/repos/mytimemachine && \
  source $(conda info --base)/etc/profile.d/conda.sh && conda activate mytimemachine && \
  PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python python scripts/train.py \
  --coach orig_nn \
  --dataset_type ffhq_aging --train_dataset data/train \
  --exp_dir experiments/full_training_run \
  --start_from_encoded_w_plus --train_encoder \
  --batch_size 2 --workers 2 \
  --target_age uniform_random \
  --max_steps 40000 \
  --id_lambda 0.3 --lpips_lambda 0.1 --lpips_lambda_aging 0.1 --lpips_lambda_crop 0.8 \
  --l2_lambda 0.1 --l2_lambda_aging 0.25 --l2_lambda_crop 0.5 \
  --w_norm_lambda 0.003 --aging_lambda 5 --cycle_lambda 1.5 \
  --adaptive_w_norm_lambda 20 \
  --nearest_neighbor_id_loss_lambda 0.10 \
  --contrastive_id_lambda 0.04 \
  --mb_index_path banks/ffhq_ir50_age_5y.pt --mb_use_faiss \
  --mb_top_m 512 --mb_k 64 --mb_min_sim 0.20 --mb_max_sim 0.70 \
  --mb_apply_min_age 35 --mb_apply_max_age 45 --mb_temperature 0.12 \
  --extrapolation_start_step 1000000000 \
  --roi_id_lambda 0.05 --roi_use_eyes --roi_use_mouth --roi_use_nose --roi_use_broweyes \
  --roi_size 112 --roi_pad 0.35 --roi_jitter 0.06 \
  --roi_landmarks_model pretrained_models/shape_predictor_68_face_landmarks.dat \
  --roi_id_schedule_s1 \"0:0.03,10000:0.05,20000:0.07,36000:0.05\" \
  --ema --ema_scope decoder --ema_decay 0.999 --eval_with_ema \
  --val_deterministic --val_max_batches 2 --val_interval 500 \
  --warmup_steps 500 --min_lr 5e-7 \
  --seed 123 | cat"
```

Stage 2 — decoder resume to 55k steps (extended ROIs + EMA)
```bash
bash -lc "cd /home/wczub/RND/AI_DEAGING/repos/mytimemachine && \
  source $(conda info --base)/etc/profile.d/conda.sh && conda activate mytimemachine && \
  PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python python scripts/train.py \
  --coach orig_nn \
  --dataset_type ffhq_aging --train_dataset data/train \
  --exp_dir experiments/full_training_run \
  --start_from_encoded_w_plus --train_decoder \
  --batch_size 2 --workers 2 \
  --target_age uniform_random \
  --max_steps 55000 --learning_rate 3e-5 \
  --id_lambda 0.3 --lpips_lambda 0.1 --lpips_lambda_aging 0.1 --lpips_lambda_crop 0.8 \
  --l2_lambda 0.1 --l2_lambda_aging 0.25 --l2_lambda_crop 0.5 \
  --w_norm_lambda 0.003 --w_norm_lambda_decoder_scale 0.5 \
  --aging_lambda 5 --aging_lambda_decoder_scale 0.5 \
  --cycle_lambda 1.5 \
  --adaptive_w_norm_lambda 20 \
  --nearest_neighbor_id_loss_lambda 0.05 \
  --contrastive_id_lambda 0.025 \
  --mb_index_path banks/ffhq_ir50_age_5y.pt --mb_use_faiss \
  --mb_top_m 768 --mb_k 64 --mb_min_sim 0.20 --mb_max_sim 0.70 \
  --mb_apply_min_age 35 --mb_apply_max_age 45 --mb_temperature 0.12 \
  --roi_id_lambda_s2 0.04 --roi_use_eyes --roi_use_mouth --roi_use_nose --roi_use_broweyes \
  --roi_size 112 --roi_pad 0.35 --roi_jitter 0.06 \
  --roi_landmarks_model pretrained_models/shape_predictor_68_face_landmarks.dat \
  --ema --ema_scope decoder --ema_decay 0.999 --eval_with_ema \
  --val_deterministic --val_max_batches 2 --val_interval 500 \
  --warmup_steps 500 --min_lr 5e-7 \
  --seed 123 \
  --resume_checkpoint experiments/full_training_run/000XX/checkpoints/iteration_40000.pt | cat"
```

Monitoring & acceptance (extended ROIs):
- Expect `train/roi_pairs_nose` and `train/roi_pairs_broweyes` > 0 alongside eyes/mouth; `loss_roi_id` finite and stable.
- Mid-boost visible: `train/roi_id_lambda_current` traverses 0.05→0.07→0.05 around 0/20k/36k; S2 shows `0.05` flat.
- Identity/perceptual similar to Eleventh/Twelfth; watch for over‑sharpening in eyes/brows; dial mid-boost to 0.06–0.07 if needed.

Final report

Scope and goal
- Strengthen micro‑identity retention by extending ROI‑ID beyond eyes+mouth to include nose and a combined brow+eyes region, keeping the stable stack (EMA eval, mid‑boost ROI schedule, FAISS miner, NN‑ID, contrastive impostor). Extrapolation disabled (in‑domain).

What changed vs strong prior baselines (Eleventh–Fourteenth)
- ROI‑ID regions: eyes + mouth → eyes + mouth + nose + brow/eyes (new)
- ROI schedule S1: 0:0.03 → 10k:0.05 → 20k:0.07 → 36k:0.05 (slightly lower start to accommodate added ROIs)
- Miner: FAISS custom, k=64, top_m=512, band [0.20,0.70], τ=0.12, ages [35,45]
- S2 LR set to 2e‑5 in this run (some prior strong S2 runs used 3e‑5)

Stage 1 — 00068 (encoder+decoder, 0→40k)
- Config (opt.json): id_lambda=0.3, lpips_crop=0.8, l2=0.1, w_norm=0.003, aging=5, cycle=1.5, NN‑ID λ=0.10, contrastive λ=0.04, FAISS: k=64/top_m=512/band[0.20,0.70]/τ=0.12 (ages 35–45), ROI: eyes+mouth+nose+broweyes, schedule 0.03→0.05 (10k)→0.07 (20k)→0.05 (36k), EMA eval on.
- Timestamp highlights (checkpoints/timestamp.txt):
  - Best S1 total: 1.060 at step 19.5k.
  - Heads at/before best windows (≈18.5k–19.5k):
    - loss_id_real ≈ 0.299–0.315; lpips_real ≈ 0.291–0.293; l2_real ≈ 0.046–0.049; w_norm_real ≈ 41–42.
    - loss_lpips_crop ≈ 0.166–0.213; l2_crop ≈ 0.048–0.078; loss_real ≈ 0.658–0.689.
  - ROI evidence: roi_pairs_eyes/mouth/nose/broweyes=2 each consistently; roi_landmark_failures=0; loss_roi_id trended from ≈0.47 early to ≈0.34–0.36 mid/late.
  - ROI schedule executed as planned: roi_lambda_current 0.03 (≤10k) → 0.05 (10k→20k) → 0.07 (20k→36k) → 0.05 (≥36k).
  - Miner diagnostics (“mb:” lines): prof=custom, k_effective=64; candidate_count typically 40–60 early, trending toward ~35–50 mid/late; simμ ≈ 0.231–0.257; mb_applied_ratio ≈ 0.75 when batch ages in-window.

Stage 2 — 00070 (decoder‑only, 40k→55k)
- Config (opt.json): resume=00068/iteration_40000.pt; train_decoder only; LR=2e‑5; id_lambda=0.3; NN‑ID λ=0.05; contrastive λ=0.025; ROI‑ID S2 λ=0.04 (eyes+mouth+nose+broweyes); w_norm_decoder_scale=0.5; EMA eval on; save_interval=2k.
- Timestamp highlights (checkpoints/timestamp.txt):
  - Best S2 total: 0.8356 at step 50.5k (earlier bests: 0.845@40k → 0.843@41k → 0.841@41.5k → 0.840@43.5k → 0.837@47k).
  - Heads near best (50.5k): loss_id_real ≈ 0.328; lpips_real ≈ 0.297; l2_real ≈ 0.046; loss_real ≈ 0.480; w_norm_real ≈ 37.1; used_ema=1; roi_lambda_current=0.04.
  - Late S2 window (≥49k) clustered ~0.836–0.849; plateau after ~50k.

Comparisons vs previous trainings (quantitative)
- vs Tenth (FAISS + ROI eyes/mouth): best S2 ≈ 0.809 → Fifteenth S2 ≈ 0.836 (higher total).
- vs Eleventh (EMA + ROI schedule): best S2 ≈ 0.807–0.809 → Fifteenth ≈ 0.836.
- vs Twelfth (soft‑0.60 miner + diagnostics): best S2 ≈ 0.8006 → Fifteenth ≈ 0.836.
- vs Fourteenth (S1 anchors; S2 anchors off): best S2 ≈ 0.804 → Fifteenth ≈ 0.836.
- Notes on comparability: Fifteenth adds two extra ROI regions (nose, brow/eyes) and used S2 LR 2e‑5 (lower than 3e‑5 used in several prior bests). Added ROI terms and lower LR likely increased totals and limited S2 tightening. Despite higher totals, regularization is excellent (w_norm_real ≈ 37 late S2, matching/bettering earlier runs).

Qualitative expectations (micro‑identity)
- Extended ROIs should preferentially sharpen nose shape and eyebrow/eye structure that global ID metrics under‑weight. Quantitative totals alone may not reflect these micro‑identity gains; side‑by‑side inspection is recommended.

Visual inspection
- Altough quantitative totals are worse, the output images looks tiny-bit better with this training

Takeaways
- S1 behaved smoothly with the extended ROIs and schedule; convergence similar to recent runs, best at 19.5k, stable through 40k.
- S2 achieved stable low w_norm_real (~37) and modest tightening but underperformed prior best totals, likely due to (a) extra ROI heads and (b) LR=2e‑5.

Recommendations
- For parity with prior bests, raise S2 LR to 3e‑5 and keep EMA; keep ROI‑ID S2 at 0.04–0.05.
- Consider miner soft profile (k=48, band [0.25,0.60]) used in Twelfth to reduce over‑hard negatives and totals; keep k_effective diagnostic lines.
- Keep extended ROIs in S1; if over‑sharpening appears, cap the mid‑boost at 0.06.
- Align contrastive λ in S2 to 0.02 (used in several best runs) to reduce canonicalization pressure.
- Judge final adoption by visual A/B on eyes, nose, and brow fidelity; totals may remain slightly higher with extended ROIs.

Artifacts present (useful breadcrumbs)
- `miner_profile.txt`: custom
- `timestamp.txt`: frequent "mb:" diagnostics with k_effective=64; roi_pairs_* show all four regions active; `used_ema=1` markers present in both stages.


-------------------------------------
Sixteenth Full Training (2-staged) — Target-ID @40 + 1-year Actor Anchors (S1-only)
-------------------------------------
Goal: add a small target-age identity guidance toward real 40-year-old prototypes, and switch actor anchors to 1-year bins with a slightly higher weight, applied only in Stage 1. Keep the stable stack from Eleventh/Twelfth/Fourteenth: EMA eval, FAISS soft miner, ROI-ID schedule, extrapolation disabled; geometry OFF.

What's new vs Fourteenth:
- Target-ID guidance (IR-SE50): pulls outputs near 38–42 toward the nearest global prototype from `banks/actor40_ir.pt`, stage-aware λ (S1 higher, S2 lower).
- Denser actor anchors: use `anchors/actor_w_age1.pt` (1-year bins) with λ=0.03; S1 only.
- Keep FAISS soft miner, ROI-ID schedule, NN-ID, contrastive impostor, EMA eval.

Stage 1 — 0→40k (encoder+decoder; anchors ON; stronger target-ID)
```bash
bash -lc "cd /home/wczub/RND/AI_DEAGING/repos/mytimemachine && conda activate mytimemachine && PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python python scripts/train.py \
  --coach orig_nn \
  --dataset_type ffhq_aging --train_dataset data/train \
  --exp_dir experiments/full_training_run \
  --start_from_encoded_w_plus --train_encoder \
  --batch_size 2 --workers 2 \
  --target_age uniform_random \
  --max_steps 40000 \
  --id_lambda 0.3 \
  --lpips_lambda 0.1 --lpips_lambda_aging 0.1 --lpips_lambda_crop 0.8 \
  --l2_lambda 0.1 --l2_lambda_aging 0.25 --l2_lambda_crop 0.5 \
  --w_norm_lambda 0.003 --aging_lambda 5 --cycle_lambda 1.5 \
  --adaptive_w_norm_lambda 20 \
  --nearest_neighbor_id_loss_lambda 0.10 \
  --contrastive_id_lambda 0.04 \
  --mb_index_path banks/ffhq_ir50_age_5y.pt --mb_use_faiss \
  --mb_top_m 768 --mb_k 48 --mb_min_sim 0.25 --mb_max_sim 0.60 \
  --mb_apply_min_age 35 --mb_apply_max_age 45 --mb_temperature 0.12 \
  --extrapolation_start_step 1000000000 \
  --roi_use_eyes --roi_use_mouth --roi_size 112 --roi_pad 0.35 --roi_jitter 0.06 \
  --roi_id_lambda 0.05 \
  --roi_id_schedule_s1 \"0:0.05,20000:0.07,36000:0.05\" \
  --ema --ema_scope decoder --ema_decay 0.999 --eval_with_ema \
  --age_anchor_path anchors/actor_w_age1.pt \
  --age_anchor_lambda 0.03 --age_anchor_stage s1 --age_anchor_space w --age_anchor_bin_size 1 \
  --target_id_bank_path banks/actor40_ir.pt \
  --target_id_lambda_s1 0.10 --target_id_apply_min_age 38 --target_id_apply_max_age 42 \
  --val_interval 500 --val_deterministic --val_max_batches 2 --val_start_step 2000 \
  --warmup_steps 500 --min_lr 5e-7 --grad_clip_norm 1.0 --nan_guard | cat"
```

Stage 2 — 40k→55k (decoder-only; anchors OFF; target-ID weaker)
```bash
bash -lc "cd /home/wczub/RND/AI_DEAGING/repos/mytimemachine && conda activate mytimemachine && PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python python scripts/train.py \
  --coach orig_nn \
  --dataset_type ffhq_aging --train_dataset data/train \
  --exp_dir experiments/full_training_run \
  --start_from_encoded_w_plus --train_decoder \
  --batch_size 2 --workers 2 --test_batch_size 2 --test_workers 2 \
  --target_age uniform_random \
  --max_steps 55000 --learning_rate 3e-5 \
  --id_lambda 0.3 \
  --lpips_lambda 0.1 --lpips_lambda_aging 0.1 --lpips_lambda_crop 0.8 \
  --l2_lambda 0.1 --l2_lambda_aging 0.25 --l2_lambda_crop 0.5 \
  --w_norm_lambda 0.003 --w_norm_lambda_decoder_scale 0.5 \
  --aging_lambda 5 --aging_lambda_decoder_scale 0.5 \
  --cycle_lambda 1.5 \
  --adaptive_w_norm_lambda 20 \
  --nearest_neighbor_id_loss_lambda 0.05 \
  --contrastive_id_lambda 0.02 \
  --mb_index_path banks/ffhq_ir50_age_5y.pt --mb_use_faiss \
  --mb_top_m 768 --mb_k 48 --mb_min_sim 0.25 --mb_max_sim 0.60 \
  --mb_apply_min_age 35 --mb_apply_max_age 45 --mb_temperature 0.12 \
  --roi_use_eyes --roi_use_mouth --roi_id_lambda_s2 0.05 --roi_size 112 --roi_pad 0.35 --roi_jitter 0.06 \
  --ema --ema_scope decoder --ema_decay 0.999 --eval_with_ema \
  --target_id_bank_path banks/actor40_ir.pt \
  --target_id_lambda_s2 0.05 --target_id_apply_min_age 38 --target_id_apply_max_age 42 \
  --resume_checkpoint experiments/full_training_run/000XX/checkpoints/iteration_40000.pt \
  --val_interval 500 --val_deterministic --val_max_batches 2 --warmup_steps 500 --min_lr 5e-7 --nan_guard | cat"
```

Notes:
- Anchors: dict format {age:int → tensor} supported; W+ averaged to W internally.
- Target-ID: uses IR-SE50 embeddings; only active for targets in [38,42]; stage-aware λ.
- Keep extrapolation disabled; stop S2 early if plateau around 45–50k.


Final report

Runs: S1 → 00071 (encoder+decoder, 0→40k), S2 → 00072 (decoder-only, 40k→55k)

Setup recap (opt.json)
- Stage 1 (00071):
  - New: Target-ID guidance around age 40 (bank=banks/actor40_ir.pt), λ_s1=0.10, apply ages [38,42]
  - New: Actor anchors (1-year bins) S1-only: age_anchor_path=anchors/actor_w_age1.pt, λ=0.03, space=w, bin_size=1
  - Coach orig_nn; interpolation-only (extrapolation_start_step=1e9)
  - FAISS soft-ish miner: k=48, top_m=768, band [0.25,0.60], τ=0.12, ages [35,45]
  - ROI-ID eyes+mouth with mid-boost schedule 0:0.05 → 20k:0.07 → 36k:0.05; EMA eval on
  - Core regs: id_lambda=0.3, lpips_crop=0.8, l2=0.1, w_norm=0.003, aging=5, cycle=1.5; warmup=500, min_lr=5e-7
- Stage 2 (00072):
  - Decoder-only; anchors OFF; Target-ID weaker λ_s2=0.05; FAISS/ROI-ID S2=0.05 fixed; EMA eval on; LR=3e-5

Timestamp highlights (selected)
- 00071 (S1): early decay typical; ROI schedule active; Target-ID engaged in-window.
  - used_ema=1 lines frequent; miner diagnostics "mb:" show prof=custom, k=48, band=[0.25,0.60].
  - Representative early-best windows:
    - 2.0k: used_ema=1 id≈0.412, lpips≈0.357, l2≈0.064; mb: cand≈17.5, simμ≈0.279
    - 2.5k: used_ema=1 id≈0.375, lpips≈0.336, l2≈0.060; mb: cand≈13.5, simμ≈0.279
    - 3.0k: used_ema=1 id≈0.363, lpips≈0.322, l2≈0.058; mb: cand≈27.0, simμ≈0.281
  - Target-ID activity (real pass): target_id_applied_ratio_real≈0.75 when in-window; loss_target_id_real commonly ≈0.28–0.34 in early checkpoints, finite and stable.
  - ROI-ID: loss_roi_id ≈0.31–0.51 early windows; roi_pairs=4 (eyes=2, mouth=2), failures≈0–1.
  - W-norm: loss_w_norm_real decays from ~223 (step 0) into ~55–60 by ~3–5k, trending toward high‑30s by late S1.
- 00072 (S2): early S2 best then plateau ~45–50k.
  - 40.0k: used_ema=1 id≈0.331, lpips≈0.297, l2≈0.049; total≈0.818 (best snapshot at 40k)
  - 41.0k: used_ema=1 id≈0.330, lpips≈0.297, l2≈0.048; total≈0.813
  - 41.5k: used_ema=1 id≈0.327, lpips≈0.296, l2≈0.049; total≈0.811 (best of log window)
  - Later windows fluctuate near ≈0.811–0.818; typical plateau behavior by ~45–50k.
  - Miner: cand≈10–19; simμ≈0.274–0.280; k_effective=48; mb_applied_ratio≈0.75 when in age window.
  - Target-ID: target_id_applied_ratio_real≈0.75 when in-window; loss_target_id_real ≈0.36–0.37 (scaled by lower λ_s2).
  - W-norm late S2: ≈36.9–37.5 (stable, low).

Comparisons vs previous trainings
- vs Eleventh (EMA + ROI schedule; no Target-ID; anchors OFF):
  - Totals in S2: Eleventh best ≈0.807–0.809; Sixteenth S2 best ≈0.811 (slightly higher). Identity/perceptual/pixel heads remain in the same bands; w_norm≈37 steady.
  - Target-ID introduced small extra identity pull near 40 (loss_target_id_real finite and stable), without instability; plateau position similar (~45–50k).
- vs Twelfth (FAISS soft‑0.60 + diagnostics; no Target-ID; anchors OFF):
  - Twelfth S2 best ≈0.8006 (marginally lower total). Sixteenth S2 sits ≈0.811 at best window; however, Sixteenth runs Target-ID + S1 anchors, trading tiny total for more intentional 40‑yo guidance.
  - Miner diagnostics comparable (cand, simμ); k_effective remains 48; logs consistent.
- vs Fourteenth (S1 anchors 5‑year; no Target-ID):
  - Sixteenth updates anchors→1‑year with λ=0.03 and adds Target‑ID. W‑norm decay early S1 appears comparably fast; S1 convergence windows arrive early as before (best early windows ~2–3k in logs), with totals later clustering in expected ranges.
  - Final S2 best ~0.811 vs Fourteenth ~0.804; within variance across runs; Sixteenth biases identity toward age‑40 exemplars in‑window.
- vs Fifteenth (extended ROIs; heavier ROI stack):
  - Sixteenth S2 totals better (≈0.811 vs ≈0.836). Keeping ROIs to eyes+mouth and adding Target-ID improved totals while maintaining micro‑identity via ROI-ID at moderate λ.

Behavior of new components
- Target-ID (global IR-SE50):
  - Applied ratio frequently ≈0.75 reflecting uniform sampling hitting ages 38–42 often; losses finite (~0.28–0.37) and stable in both stages; no NaNs/spikes observed.
  - Stage-aware λ: stronger in S1 (0.10) and weaker in S2 (0.05) behaved as intended; S2 logs show persistent but smaller contribution.
- Actor anchors (1‑year, S1 only):
  - Loss magnitude tiny (as designed); anchor breadcrumbs present in opt; no adverse snapping; W‑norm progressed into high‑30s by late S1 similar to Fourteenth.

One‑line summary
- With Target‑ID toward real 40‑yo prototypes and 1‑year actor anchors (S1‑only), the Sixteenth training maintained stable convergence and low w_norm, matching strong identity/pixel/perceptual bands of prior best runs; S2 plateaued around ~0.811 (slightly above Eleventh/Tenth bests) while providing age‑40 identity steering without instability.

Recommendations
- If aiming to match Twelfth's lowest S2 totals (~0.8006), consider briefly disabling Target‑ID in late S2, or reduce λ_s2→0.03 while keeping λ_s1=0.10.
- Keep 1‑year anchors S1‑only at λ=0.03; optionally sweep λ in [0.02,0.04] for sensitivity.
- Early stop S2 at 45–50k if plateau persists; best snapshots clustered ~41–45k.
- Maintain FAISS soft miner and EMA eval; continue logging `target_id_applied_ratio_real` and miner diagnostics for quick health checks.



-------------------------------------
Seventeenth Full Training (2-staged) — Dynamic ID ramp + aging guard
-------------------------------------
Goal: start Stage-1 with a moderate identity weight, then ramp identity up late while maintaining age fidelity. In Stage-2, fix both ID and aging lambdas to strong values. This uses the newly added per-step schedules for `id_lambda` and `aging_lambda`.

Rationale:
- Early training finds the age transform at safer ID strength.
- Late Stage-1: ramp `id_lambda` to 0.45–0.50 while also maintaining/increasing `aging_lambda` to avoid regressing to older features.
- Stage-2: lock `id_lambda`≈0.45 and `aging_lambda` strong (e.g., 5.0) for crisp identity without artifacts.

Stage 1 — 0→40k (encoder+decoder; ID/aging schedules active)
```bash
bash -lc "cd /home/wczub/RND/AI_DEAGING/repos/mytimemachine && \
  conda activate mytimemachine && \
  PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python python scripts/train.py \
  --coach orig_nn \
  --dataset_type ffhq_aging --train_dataset data/train \
  --exp_dir experiments/full_training_run \
  --start_from_encoded_w_plus --train_encoder \
  --batch_size 2 --workers 2 \
  --target_age uniform_random \
  --max_steps 40000 --image_interval 100 \
  --board_interval 50 --val_interval 500 --val_deterministic --val_max_batches 2 --val_start_step 2000 --val_disable_aging \
  --id_lambda 0.30 --id_lambda_schedule_s1 '0:0.30,20000:0.40,36000:0.50' \
  --lpips_lambda 0.1 --lpips_lambda_aging 0.1 --lpips_lambda_crop 0.8 \
  --l2_lambda 0.1 --l2_lambda_aging 0.25 --l2_lambda_crop 0.5 \
  --w_norm_lambda 0.003 \
  --aging_lambda 5 --aging_lambda_schedule_s1 '0:5.0,30000:5.5,36000:6.0' \
  --cycle_lambda 1.5 \
  --adaptive_w_norm_lambda 20 \
  --scheduler_type cosine --warmup_steps 500 --min_lr 5e-7 --grad_clip_norm 1.0 --nan_guard \
  --extrapolation_start_step 1000000000 \
  --nearest_neighbor_id_loss_lambda 0.10 \
  --contrastive_id_lambda 0.04 \
  --mb_index_path banks/ffhq_ir50_age_5y.pt --mb_use_faiss \
  --mb_top_m 768 --mb_k 48 --mb_min_sim 0.25 --mb_max_sim 0.60 \
  --mb_apply_min_age 35 --mb_apply_max_age 45 --mb_temperature 0.12 \
  --roi_id_lambda 0.05 --roi_use_eyes --roi_use_mouth --roi_size 112 --roi_pad 0.35 --roi_jitter 0.06 \
  --roi_landmarks_model pretrained_models/shape_predictor_68_face_landmarks.dat \
  --roi_id_schedule_s1 '0:0.05,20000:0.07,36000:0.05' \
  --ema --ema_scope decoder --ema_decay 0.999 --eval_with_ema \
  --target_id_bank_path banks/actor40_ir.pt \
  --target_id_lambda_s1 0.10 --target_id_apply_min_age 38 --target_id_apply_max_age 42 \
  --warmup_steps 500 --min_lr 5e-7 --grad_clip_norm 1.0 --nan_guard | cat"
```

Stage 2 — 40k→55k (decoder-only; fixed strong ID and aging)
```bash
bash -lc "cd /home/wczub/RND/AI_DEAGING/repos/mytimemachine && \
  conda activate mytimemachine && \
  PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python python scripts/train.py \
  --coach orig_nn \
  --dataset_type ffhq_aging --train_dataset data/train \
  --exp_dir experiments/full_training_run \
  --start_from_encoded_w_plus --train_decoder \
  --batch_size 2 --workers 2 --test_batch_size 2 --test_workers 2 \
  --target_age uniform_random \
  --max_steps 55000 --learning_rate 3e-5 \
  --id_lambda 0.30 --id_lambda_s2 0.45 \
  --lpips_lambda 0.1 --lpips_lambda_aging 0.1 --lpips_lambda_crop 0.8 \
  --l2_lambda 0.1 --l2_lambda_aging 0.25 --l2_lambda_crop 0.5 \
  --w_norm_lambda 0.003 --w_norm_lambda_decoder_scale 0.5 \
  --aging_lambda 5 --aging_lambda_s2 6.0 --aging_lambda_decoder_scale 0.5 \
  --cycle_lambda 1.5 \
  --adaptive_w_norm_lambda 20 \
  --scheduler_type cosine --warmup_steps 500 --min_lr 5e-7 --grad_clip_norm 1.0 --nan_guard \
  --resume_checkpoint experiments/full_training_run/000XX/checkpoints/iteration_40000.pt \
  --nearest_neighbor_id_loss_lambda 0.05 \
  --contrastive_id_lambda 0.02 \
  --mb_index_path banks/ffhq_ir50_age_5y.pt --mb_use_faiss \
  --mb_top_m 768 --mb_k 48 --mb_min_sim 0.25 --mb_max_sim 0.60 \
  --mb_apply_min_age 35 --mb_apply_max_age 45 --mb_temperature 0.12 \
  --roi_use_eyes --roi_use_mouth --roi_id_lambda_s2 0.05 --roi_size 112 --roi_pad 0.35 --roi_jitter 0.06 \
  --ema --ema_scope decoder --ema_decay 0.999 --eval_with_ema \
  --target_id_bank_path banks/actor40_ir.pt \
  --target_id_lambda_s2 0.05 --target_id_apply_min_age 38 --target_id_apply_max_age 42 | cat"
```

Monitoring:
- Expect `train/id_lambda_current` to traverse 0.30→0.40→0.50 around 0/20k/36k in S1.
- Expect `train/aging_lambda_current` to stay high and rise near 30–36k as per schedule.
- In S2, `id_lambda_current≈0.45`, `aging_lambda_current≈6.0` remain flat; EMA eval should be on.

Seventeenth training (2-staged) — Dynamic ID ramp + aging guard — final report
-------------------------------------
Runs: S1 → 00064 (encoder+decoder, 0→40k), S2 → 00065 (decoder-only, 40k→55k)

Setup recap (from opt.json)
- Stage 1 (00064):
  - ID schedule (new): id_lambda_schedule_s1 = 0:0.30 → 20k:0.40 → 36k:0.50
  - Aging schedule (new): aging_lambda_schedule_s1 = 0:5.0 → 30k:5.5 → 36k:6.0
  - Target-ID: bank=banks/actor40_ir.pt, λ_s1=0.10, ages [38,42]
  - FAISS miner (custom): k=48, top_m=768, band [0.25,0.60], τ=0.12, ages [35,45]
  - ROI-ID (eyes+mouth): roi_id_lambda=0.05 with mid-boost schedule 0:0.05 → 20k:0.07 → 36k:0.05
  - Core regs: id_lambda base=0.3, lpips_crop=0.8, l2=0.1, w_norm=0.003, aging=5, cycle=1.5; EMA eval on; interpolation-only
- Stage 2 (00065):
  - Fixed ID: id_lambda_s2=0.45 (stronger than prior baselines)
  - Fixed aging: aging_lambda_s2=6.0 (effective 3.0 with decoder scale 0.5)
  - Target-ID: λ_s2=0.05; ROI-ID S2: roi_id_lambda_s2=0.05; EMA eval on; LR=3e-5

Stage 1 (00064) — metrics and trends (timestamp.txt)
- Early decay: loss 3.83 → ~1.80 @1k → ~1.24 @5k
- Best windows (selected):
  - 4.5k: total≈1.236; id≈0.354; lpips≈0.315; l2≈0.051; w_norm≈55.17
  - 7.5k: total≈1.167; id≈0.331; lpips≈0.306; l2≈0.048; w_norm≈50.44
  - 11.0k: total≈1.124; id≈0.318; lpips≈0.299; l2≈0.047; w_norm≈47.33
  - 17.5k (best of S1): total≈1.094; id≈0.331; lpips≈0.291; l2≈0.051; w_norm≈42.40
- Schedules evidenced in logs:
  - ROI mid-boost active: roi_lambda_current=0.07 between ~20k→36k, then 0.05
  - Aging schedule visible: effective_aging_lambda≈5.5 at 30k; ≈6.0 at 36k
- Late S1 (30k→40k): totals ~1.15–1.24; identity head ~0.34±0.01; w_norm_real→~37.2 at 40k; plateau behavior
- Miner diagnostics: prof=custom; k_effective=48; candidate_count shrinks (~28→~10–15) with training; mb_applied_ratio≈0.75 when in age window

Stage 2 (00065) — metrics and trends (timestamp.txt)
- Resume 40k from 00064; effective scales (decoder): w_norm≈0.0015, aging≈3.0
- Best windows:
  - 40k (start): total≈0.934
  - 41–43k: totals ~0.929–0.925
  - 47k: total≈0.924
  - 51k (best of S2): total≈0.920 — id≈0.340; lpips≈0.293; l2≈0.053; w_norm≈37.58; EMA eval on
  - 55k (final): total≈0.923 (plateau after ~47–51k)
- Identity/perceptual/pixel heads remained tight and stable; w_norm_real stayed ~37–38 throughout
- Miner: k_effective=48; mb_applied_ratio≈0.75; candidate_count ~11–13 mid/late; no instability

Comparisons vs previous trainings
- vs Eleventh/Tenth (EMA + ROI schedule; S2 best ≈0.807–0.809): Seventeenth S2 best ≈0.920 is higher (worse) in absolute total. Identity (loss_id_real≈0.339–0.346 late S2) and LPIPS/L2 ranges are similar, but stronger fixed λ (id_s2=0.45, aging_s2=6.0 → effective 3.0) likely increased totals.
- vs Twelfth (soft‑0.60 miner; S2 best ≈0.8006): Seventeenth trails on total. W‑norm stability (~37–38) comparable. Higher fixed ID/aging emphasis likely trades total for stricter identity/age constraints.
- vs Fourteenth/Sixteenth (anchors/target‑ID variants): Seventeenth omits anchors; uses strong ID/aging schedules. Totals plateau in similar bands as non‑anchor runs but above their best S2 windows.

Behavior of new schedules
- ID ramp (S1: 0.30→0.40→0.50): did not improve the identity head beyond ~0.33–0.35 late S1; best identity/pixel numbers occurred earlier (~11–18k) before the strongest ramp.
- Aging ramp (S1: 5.0→5.5→6.0): kept aging signal strong; no reversion to "older" features was observed in logs; however, higher effective aging (and later strong ID) correlated with slightly higher totals late S1.
- S2 fixed strong λ (id_s2=0.45, aging_s2=6.0): maintained identity/pixel stability but gave higher totals than prior best S2 runs.

One‑line summary
- The dynamic ID ramp with a late aging boost ran stably and preserved identity, but its stronger late constraints did not lower the identity head and led to higher absolute totals; S2 with fixed strong λs plateaued around ≈0.920.

Recommendations
- Soften late identity push: cap S1 ramp at 0.40 (not 0.50), start ramp later (e.g., 30k→36k only), or ramp more gradually (0.30→0.35→0.40).
- Reduce S2 fixed λs: try id_lambda_s2=0.40 (or 0.35) and aging_lambda_s2=5.0 (effective 2.5), matching prior best‑total regimes.
- Emphasize age fidelity without extra total cost: keep aging schedule peak at 5.5 (rather than 6.0), or rely on ROI schedule/Target‑ID to guard age during the S1 identity ramp.
- Consider ROI‑ID mid‑boost + EMA (Eleventh style) combined with a milder ID ramp to focus micro‑identity while avoiding higher totals.
- Early stop S2 around 47–51k where best totals clustered.


Eighteenth training (2-staged) — Margin-based ID hinge + soft miner — plan and commands
-------------------------------------
Objective:
- Add a margin-based identity hinge to enforce a minimum cosine similarity between output and target embeddings (IR‑SE50), complementing existing ID/aging/LPIPS/pixel, soft FAISS impostor, ROI‑ID, and schedules. Start conservatively and observe for any over‑constraint of age edits.

Stage 1 — 0→40k (encoder; interpolation-only; margin hinge on)
```bash
bash -lc "cd /home/wczub/RND/AI_DEAGING/repos/mytimemachine && \
  conda activate mytimemachine && \
  PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python python scripts/train.py \
  --coach orig_nn \
  --dataset_type ffhq_aging --train_dataset data/train \
  --exp_dir experiments/full_training_run \
  --start_from_encoded_w_plus --train_encoder \
  --batch_size 2 --workers 2 --test_batch_size 2 --test_workers 2 \
  --target_age uniform_random \
  --max_steps 40000 --image_interval 100 \
  --board_interval 50 --val_interval 500 --val_deterministic --val_max_batches 2 --val_start_step 2000 --val_disable_aging \
  --id_lambda 0.30 --id_lambda_schedule_s1 '0:0.30,20000:0.40,36000:0.50' \
  --id_margin_enabled --id_margin_target 0.88 --id_margin_lambda 0.05 \
  --lpips_lambda 0.1 --lpips_lambda_aging 0.1 --lpips_lambda_crop 0.8 \
  --l2_lambda 0.1 --l2_lambda_aging 0.25 --l2_lambda_crop 0.5 \
  --w_norm_lambda 0.003 \
  --aging_lambda 5 --aging_lambda_schedule_s1 '0:5.0,30000:5.5,36000:6.0' \
  --cycle_lambda 1.5 \
  --adaptive_w_norm_lambda 20 \
  --scheduler_type cosine --warmup_steps 500 --min_lr 5e-7 --grad_clip_norm 1.0 --nan_guard \
  --extrapolation_start_step 1000000000 \
  --nearest_neighbor_id_loss_lambda 0.10 \
  --contrastive_id_lambda 0.04 \
  --mb_index_path banks/ffhq_ir50_age_5y.pt --mb_use_faiss \
  --mb_top_m 768 --mb_k 48 --mb_min_sim 0.25 --mb_max_sim 0.60 \
  --mb_apply_min_age 35 --mb_apply_max_age 45 --mb_temperature 0.12 \
  --roi_id_lambda 0.05 --roi_use_eyes --roi_use_mouth --roi_size 112 --roi_pad 0.35 --roi_jitter 0.06 \
  --roi_landmarks_model pretrained_models/shape_predictor_68_face_landmarks.dat \
  --roi_id_schedule_s1 '0:0.05,20000:0.07,36000:0.05' \
  --ema --ema_scope decoder --ema_decay 0.999 --eval_with_ema \
  --target_id_bank_path banks/actor40_ir.pt \
  --target_id_lambda_s1 0.10 --target_id_apply_min_age 38 --target_id_apply_max_age 42 \
  --warmup_steps 500 --min_lr 5e-7 --grad_clip_norm 1.0 --nan_guard | cat"
```

Stage 2 — 40k→55k (decoder-only; EMA eval; margin hinge lighter)
```bash
bash -lc "cd /home/wczub/RND/AI_DEAGING/repos/mytimemachine && \
  conda activate mytimemachine && \
  PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python python scripts/train.py \
  --coach orig_nn \
  --dataset_type ffhq_aging --train_dataset data/train \
  --exp_dir experiments/full_training_run \
  --start_from_encoded_w_plus --train_decoder \
  --batch_size 2 --workers 2 --test_batch_size 2 --test_workers 2 \
  --target_age uniform_random \
  --max_steps 55000 --learning_rate 3e-5 \
  --id_lambda 0.30 --id_lambda_s2 0.45 \
  --id_margin_enabled --id_margin_target 0.88 --id_margin_lambda 0.03 \
  --lpips_lambda 0.1 --lpips_lambda_aging 0.1 --lpips_lambda_crop 0.8 \
  --l2_lambda 0.1 --l2_lambda_aging 0.25 --l2_lambda_crop 0.5 \
  --w_norm_lambda 0.003 --w_norm_lambda_decoder_scale 0.5 \
  --aging_lambda 5 --aging_lambda_s2 6.0 --aging_lambda_decoder_scale 0.5 \
  --cycle_lambda 1.5 \
  --adaptive_w_norm_lambda 20 \
  --scheduler_type cosine --warmup_steps 500 --min_lr 5e-7 --grad_clip_norm 1.0 --nan_guard \
  --resume_checkpoint experiments/full_training_run/000XX/checkpoints/iteration_40000.pt \
  --nearest_neighbor_id_loss_lambda 0.05 \
  --contrastive_id_lambda 0.02 \
  --mb_index_path banks/ffhq_ir50_age_5y.pt --mb_use_faiss \
  --mb_top_m 768 --mb_k 48 --mb_min_sim 0.25 --mb_max_sim 0.60 \
  --mb_apply_min_age 35 --mb_apply_max_age 45 --mb_temperature 0.12 \
  --roi_use_eyes --roi_use_mouth --roi_id_lambda_s2 0.05 --roi_size 112 --roi_pad 0.35 --roi_jitter 0.06 \
  --ema --ema_scope decoder --ema_decay 0.999 --eval_with_ema \
  --target_id_bank_path banks/actor40_ir.pt \
  --target_id_lambda_s2 0.05 --target_id_apply_min_age 38 --target_id_apply_max_age 42 | cat"
```

Monitoring expectations:
- Margin hinge metric appears as `loss_id_margin_real`/`cycle`; early S1 should show small positive values decreasing as ID improves. Identity head should trend slightly lower than comparable runs without margin, with aging and w_norm stable.

Eighteenth training (2-staged) — Margin-based ID hinge + soft miner — final report
-------------------------------------
Runs: S1 → 00066 (encoder, 0→40k), S2 → 00067 (decoder-only, 40k→55k)

Setup recap (from opt.json)
- Stage 1 (00066):
  - Margin ID hinge: enabled, margin_target=0.88, λ=0.05
  - ID schedule: id_lambda_schedule_s1 = 0:0.30 → 20k:0.40 → 36k:0.50
  - Aging schedule: aging_lambda_schedule_s1 = 0:5.0 → 30k:5.5 → 36k:6.0
  - FAISS miner (soft band): k=48, top_m=768, band [0.25,0.60], τ=0.12, age window [35,45]
  - ROI-ID: eyes+mouth, λ=0.05 with mid-boost 0:0.05 → 20k:0.07 → 36k:0.05
  - Target-ID: bank=banks/actor40_ir.pt, λ_s1=0.10, ages [38,42]
  - Core regs: lpips=0.1, lpips_crop=0.8, l2=0.1/0.25 crop/global, w_norm=0.003, aging=5, cycle=1.5
  - EMA eval on; interpolation-only (extrapolation disabled)
- Stage 2 (00067):
  - Margin ID hinge: enabled, margin_target=0.88, λ=0.03
  - Fixed ID/aging: id_lambda_s2=0.45, aging_lambda_s2=6.0; decoder scales: w_norm×0.5, aging×0.5
  - FAISS miner soft band as S1; ROI-ID S2 λ=0.05; Target-ID λ_s2=0.05; EMA eval on; LR 3e-5

Stage 1 (00066) — metrics and trends (timestamp.txt)
- Early decay mirrors prior stable runs but with visible margin term:
  - Examples of early bests: 2k→4.5k: totals 1.397→1.270; id≈0.37→0.34; lpips≈0.35→0.32; l2≈0.06→0.055
- Mid-window improvements:
  - 7.5k–12k: totals ≈1.198→1.156; id≈0.33→0.31; lpips≈0.31→0.30; w_norm_real ≈57→58→65 early, then trends down toward low‑40s later
- Best S1 window (late teens):
  - 19.5k (best in band): total≈1.094 (comparable to Seventeenth's 17.5k best 1.094)
- Late S1 (30k→40k):
  - With id ramp to 0.50 + margin enabled, identity head stabilizes around ~0.31–0.32, lpips ~0.30–0.31, l2 ~0.056–0.058; w_norm_real ≈38–39 at 40k
  - Step 40k snapshot: id≈0.317; lpips≈0.307; l2≈0.058; w_norm≈38.0; loss_id_margin_real≈0.19–0.20
- Miner diagnostics: prof=custom; k_effective=48; candidate_count shrinks from ~28 early to ~15±; sim_mean ≈0.26±0.01 late; applied_ratio≈0.75 in age window

Stage 2 (00067) — metrics and trends (timestamp.txt)
- Resume from 00066: effective decoder scales w_norm≈0.0015, aging≈3.0
- Best windows and plateau behavior:
  - 40k start: total≈0.934; improves to best ≈0.911 @44k; final 55k ≈0.917
  - Identity head ranges ~0.31–0.32; lpips ~0.30–0.31; l2 ~0.056–0.059; margin term ~0.19–0.20
- Miner in S2: k_effective=48; candidate_count ≈15–19; sim_mean ≈0.26; applied_ratio≈0.75; stable

Comparisons vs previous trainings
- vs Seventeenth S1 (best 1.094 @17.5k):
  - Eighteenth S1 reaches a similar best (~1.094 @~19.5k). With margin hinge, identity head sits slightly lower earlier but late ramp + margin converges near ~0.31–0.32 similar to Seventeenth.
- vs Seventeenth S2 (best ≈0.920 @51k):
  - Eighteenth S2 improves the best window to ≈0.911 @44k (better total). Identity/lpips/l2 remain in tight bands; margin hinge likely nudged identity slightly while not harming totals.
- vs Twelfth (soft miner) and earlier bests:
  - The best Eighteenth S2 total (≈0.911) is still above the very best prior totals (~0.80x), but better than Seventeenth's ≈0.920 under stronger fixed λs. Margin hinge did not introduce instability and appears compatible with soft miner and EMA.

Behavior of the margin hinge (loss_id_margin_real)
- S1: early values high (0.24–0.30) and decrease as similarity improves; late S1 stabilizes ~0.19–0.20 at ramped ID.
- S2: stays ~0.19–0.20 across best windows (44k–55k), indicating sustained high cosine similarity while allowing aging/perceptual changes.

One‑line summary
- The margin-based identity hinge preserved higher cosine similarity without destabilizing training and produced a better S2 best total (≈0.911) than Seventeenth's ≈0.920 under similar strong ID/aging constraints; S1 best matched prior best (~1.094). Overall identity fidelity slightly improved with minimal trade-off to perceptual/pixel heads.

Recommendations
- Keep margin_target=0.88; consider λ_s1=0.04 and λ_s2=0.02 to test if totals can drop further while retaining the identity gains.
- Consider capping S1 id_lambda ramp at 0.40–0.45 to avoid late‑phase rigidity; current margin may already enforce adequate similarity.
- If aiming for lower totals, revisit S2 fixed λs (e.g., id_lambda_s2=0.40, aging_lambda_s2=5.5) and monitor whether best window shifts earlier than 44k.
- Continue soft miner profile with k=48, band [0.25,0.60]; the diagnostics show healthy semi-hard retrieval without over‑canonicalization.

Nineteenth training (2-staged) — Broad “big-data” baseline — FAISS soft-0.60 + ROI-ID schedule + EMA
-------------------------------------
Why now: scales well with more identities/poses; maximizes general quality while keeping identity crisp. What's new vs strong baselines: slightly longer Stage-1 for ~1k imgs, Twelfth-style soft miner, conservative losses, ROI-ID mid-boost, EMA.

Stage-1 (0→40k, encoder+decoder)
```
bash -lc "cd /home/wczub/RND/AI_DEAGING/repos/mytimemachine && \
  conda activate mytimemachine && \
  PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python python scripts/train.py \
  --coach orig_nn \
  --dataset_type ffhq_aging --train_dataset data/train \
  --exp_dir experiments/full_training_run \
  --start_from_encoded_w_plus --train_encoder \
  --batch_size 2 --workers 2 \
  --target_age uniform_random \
  --max_steps 40000 \
  --id_lambda 0.3 \
  --lpips_lambda 0.1 --lpips_lambda_aging 0.1 --lpips_lambda_crop 0.8 \
  --l2_lambda 0.1 --l2_lambda_aging 0.25 --l2_lambda_crop 0.5 \
  --w_norm_lambda 0.003 --aging_lambda 5 --cycle_lambda 1.5 \
  --adaptive_w_norm_lambda 20 \
  --nearest_neighbor_id_loss_lambda 0.10 \
  --contrastive_id_lambda 0.04 \
  --mb_index_path banks/ffhq_ir50_age_5y.pt --mb_use_faiss \
  --mb_top_m 768 --mb_k 48 --mb_min_sim 0.25 --mb_max_sim 0.60 \
  --mb_apply_min_age 35 --mb_apply_max_age 45 --mb_temperature 0.12 \
  --extrapolation_start_step 1000000000 \
  --roi_id_lambda 0.05 --roi_use_eyes --roi_use_mouth \
  --roi_size 112 --roi_pad 0.35 --roi_jitter 0.06 \
  --roi_landmarks_model pretrained_models/shape_predictor_68_face_landmarks.dat \
  --roi_id_schedule_s1 '0:0.05,20000:0.07,36000:0.05' \
  --ema --ema_scope decoder --ema_decay 0.999 --eval_with_ema \
  --val_deterministic --val_max_batches 2 --val_interval 500 \
  --warmup_steps 500 --min_lr 5e-7 --grad_clip_norm 1.0 --nan_guard | cat"
```

Stage-2 (resume 40k→55k, decoder-only)
```
bash -lc "cd /home/wczub/RND/AI_DEAGING/repos/mytimemachine && \
  conda activate mytimemachine && \
  PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python python scripts/train.py \
  --coach orig_nn \
  --dataset_type ffhq_aging --train_dataset data/train \
  --exp_dir experiments/full_training_run \
  --start_from_encoded_w_plus --train_decoder \
  --batch_size 2 --workers 2 \
  --target_age uniform_random \
  --max_steps 55000 --learning_rate 3e-5 \
  --id_lambda 0.3 \
  --lpips_lambda 0.1 --lpips_lambda_aging 0.1 --lpips_lambda_crop 0.8 \
  --l2_lambda 0.1 --l2_lambda_aging 0.25 --l2_lambda_crop 0.5 \
  --w_norm_lambda 0.003 --w_norm_lambda_decoder_scale 0.5 \
  --aging_lambda 5 --aging_lambda_decoder_scale 0.5 \
  --cycle_lambda 1.5 \
  --adaptive_w_norm_lambda 20 \
  --nearest_neighbor_id_loss_lambda 0.05 \
  --contrastive_id_lambda 0.02 \
  --mb_index_path banks/ffhq_ir50_age_5y.pt --mb_use_faiss \
  --mb_top_m 768 --mb_k 48 --mb_min_sim 0.25 --mb_max_sim 0.60 \
  --mb_apply_min_age 35 --mb_apply_max_age 45 --mb_temperature 0.12 \
  --roi_id_lambda_s2 0.05 --roi_use_eyes --roi_use_mouth \
  --roi_size 112 --roi_pad 0.35 --roi_jitter 0.06 \
  --ema --ema_scope decoder --ema_decay 0.999 --eval_with_ema \
  --val_deterministic --val_max_batches 2 --val_interval 500 \
  --warmup_steps 500 --min_lr 5e-7 \
  --resume_checkpoint experiments/full_training_run/000XX/checkpoints/iteration_40000.pt | cat"
```

Notes:
- Soft FAISS band keeps negatives semi-hard as diversity rises; k=48/top_m=768 works well at this scale.
- ROI-ID mid-boost stabilizes micro-identity as the dataset gets more varied.
- EMA gives clean, reproducible best snapshots.

One‑liner
- Broad big‑data baseline with FAISS soft‑0.60, ROI‑ID schedule, EMA; S1 0→40k, S2 40k→55k.


Final report

Runs: S1 → 00074 (encoder, 0→40k), S2 → 00075 (decoder‑only, 40k→55k)

Setup recap (from opt.json)
- Dataset: ffhq_aging; train dataset `data/train` with 944 images (expanded ≈1K); test ≈105
- Core: `--coach orig_nn`, `--start_from_encoded_w_plus`, EMA on (`--ema --ema_scope decoder --ema_decay 0.999`, eval with EMA)
- Loss weights (S1): id=0.3, lpips=0.1, lpips_crop=0.8, lpips_aging=0.1, l2=0.1/0.25/0.5, w_norm=0.003, aging=5.0, cycle=1.5, adaptive_w_norm=20
- Miner: FAISS soft band, k=48, top_m=768, band [0.25, 0.60], τ=0.12, age window [35,45]
- ROI‑ID: eyes+mouth, λ_s1 schedule 0:0.05 → 20k:0.07 → 36k:0.05; S2 fixed λ=0.05
- Target‑ID: disabled in this run (effective λ_s1=0.0, λ_s2=0.0)
- Optim/schedule: ranger, cosine, warmup=500, min_lr=5e‑7; S2 lr=3e‑5; grad_clip_norm=1.0; nan_guard

Stage 1 (00074) — metrics and trends (timestamp.txt)
- Early decay: totals drop 2.99 → 1.47 by 5k; ROI‑ID active, miner candidates rise with dataset diversity
  - Best windows observed:
    - 7k: 1.374 (id≈0.450, lpips≈0.368, l2≈0.054, w_norm≈55.6)
    - 10k: 1.342–1.37 range
    - 15k: 1.218 (id≈0.376, lpips≈0.341, l2≈0.048)
    - 17k: 1.190 (id≈0.388, lpips≈0.333, l2≈0.048)
    - 19k: 1.156 (id≈0.363, lpips≈0.326, l2≈0.047)
    - 22.5k: 1.146 (ROI mid‑boost λ≈0.07)
    - 23.5k: 1.135 (best in S1; id≈0.353, lpips≈0.320, l2≈0.047, w_norm≈43.45)
- End‑of‑S1 snapshot (40k): total≈0.879 (as printed by S2 resume header), id≈0.344, lpips≈0.312, l2≈0.045, w_norm≈41.1
- Miner diagnostics (S1): prof=custom; k_effective=48; candidate_count grows ~20→70 early, then stabilizes ~20–35; sim_mean ≈0.277–0.285; p90 ≈0.31±; applied_ratio≈0.25

Stage 2 (00075) — metrics and trends (timestamp.txt)
- Resume: 40k start total≈0.879 (EMA); roi_id λ=0.05 fixed; effective w_norm×0.5, aging×0.5
- Best windows and plateau behavior:
  - Best overall: 53k → total≈0.859 (id≈0.334, lpips≈0.306, l2≈0.0447, w_norm≈41.61)
  - Early S2: 40k best≈0.879; improves through 42k≈0.862, 45–48k≈0.882–0.890; final 55k≈0.889
- Miner in S2: k_effective=48; candidate_count ≈21–30; sim_mean ≈0.275–0.280; p90 ≈0.302–0.320; applied_ratio≈0.25 — stable semi‑hard negatives at scale

Comparisons vs previous trainings
- vs Eighteenth S2 (best ≈0.911 @44k): Nineteenth improves to ≈0.859 (better total); heads remain tight: id≈0.33, lpips≈0.306, l2≈0.045; w_norm≈41 (slightly higher than some earlier S2s but stable)
- vs Seventeenth S2 (best ≈0.920): clear improvement with the big‑data soft miner + ROI schedule
- vs Twelfth S2 (best ≈0.8006): still above Twelfth's very best, but significantly closer than Seventeenth/Eighteenth; soft‑0.60 miner remains effective at larger scale
- S1 vs Eighteenth S1 (best ≈1.094): Nineteenth S1 best ≈1.135 trails slightly (dataset broadened; no margin hinge). However, the 40k handoff snapshot is strong and S2 recovers to the best overall two‑stage total so far under conservative losses.

Visual inspection
- There is a clear visual improvement even tho the logs doesn't show this

Miner notes (miner_profile.txt)
- Profile: "custom" for both 00074 and 00075; metrics from timestamp show consistent semi‑hard retrieval with the specified band and k

One‑line summary
- Big‑data FAISS soft‑0.60 + ROI‑ID + EMA scales to ~1k images: best S2 total ≈0.859 (better than Seventeenth/Eighteenth, shy of Twelfth's best), with stable identity/perceptual/pixel heads and healthy miner diagnostics. Visually the best run so far.

Twentieth training (2-staged) — Identity‑Adversarial (ID‑adv) guidance + FAISS soft‑0.60 + ROI‑ID + EMA
-------------------------------------
Why now: leverage the new actor‑vs‑all discriminator to directly push generator outputs toward the actor identity, while preserving the successful big‑data baseline (soft miner + ROI‑ID + EMA). This should improve actor resemblance without harming perceptual/pixel heads when weighted conservatively.

Stage‑1 (0→40k, encoder+decoder)
```
bash -lc "cd /home/wczub/RND/AI_DEAGING/repos/mytimemachine && \
  conda activate mytimemachine && \
  PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python python scripts/train.py \
  --coach orig_nn \
  --dataset_type ffhq_aging --train_dataset data/train \
  --exp_dir experiments/full_training_run \
  --start_from_encoded_w_plus --train_encoder \
  --batch_size 2 --workers 2 \
  --target_age uniform_random \
  --max_steps 40000 \
  --id_lambda 0.3 \
  --lpips_lambda 0.1 --lpips_lambda_aging 0.1 --lpips_lambda_crop 0.8 \
  --l2_lambda 0.1 --l2_lambda_aging 0.25 --l2_lambda_crop 0.5 \
  --w_norm_lambda 0.003 --aging_lambda 5 --cycle_lambda 1.5 \
  --adaptive_w_norm_lambda 20 \
  --nearest_neighbor_id_loss_lambda 0.10 \
  --contrastive_id_lambda 0.04 \
  --mb_index_path banks/ffhq_ir50_age_5y.pt --mb_use_faiss \
  --mb_top_m 768 --mb_k 48 --mb_min_sim 0.25 --mb_max_sim 0.60 \
  --mb_apply_min_age 35 --mb_apply_max_age 45 --mb_temperature 0.12 \
  --extrapolation_start_step 1000000000 \
  --roi_id_lambda 0.05 --roi_use_eyes --roi_use_mouth \
  --roi_size 112 --roi_pad 0.35 --roi_jitter 0.06 \
  --roi_landmarks_model pretrained_models/shape_predictor_68_face_landmarks.dat \
  --roi_id_schedule_s1 '0:0.05,20000:0.07,36000:0.05' \
  --ema --ema_scope decoder --ema_decay 0.999 --eval_with_ema \
  --val_deterministic --val_max_batches 2 --val_interval 500 \
  --warmup_steps 500 --min_lr 5e-7 --grad_clip_norm 1.0 --nan_guard \
  --id_adv_lambda 0.05 \
  --id_adv_backend arcface --id_adv_input_size 112 \
  --id_adv_model_path experiments/full_training_run/actor_classifier/run_20250913_114132/actor_classifier_best.pth | cat"
```

Stage‑2 (resume 40k→55k, decoder‑only)
```
bash -lc "cd /home/wczub/RND/AI_DEAGING/repos/mytimemachine && \
  conda activate mytimemachine && \
  PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python python scripts/train.py \
  --coach orig_nn \
  --dataset_type ffhq_aging --train_dataset data/train \
  --exp_dir experiments/full_training_run \
  --start_from_encoded_w_plus --train_decoder \
  --batch_size 2 --workers 2 \
  --target_age uniform_random \
  --max_steps 55000 --learning_rate 3e-5 \
  --id_lambda 0.3 \
  --lpips_lambda 0.1 --lpips_lambda_aging 0.1 --lpips_lambda_crop 0.8 \
  --l2_lambda 0.1 --l2_lambda_aging 0.25 --l2_lambda_crop 0.5 \
  --w_norm_lambda 0.003 --w_norm_lambda_decoder_scale 0.5 \
  --aging_lambda 5 --aging_lambda_decoder_scale 0.5 \
  --cycle_lambda 1.5 \
  --adaptive_w_norm_lambda 20 \
  --nearest_neighbor_id_loss_lambda 0.05 \
  --contrastive_id_lambda 0.02 \
  --mb_index_path banks/ffhq_ir50_age_5y.pt --mb_use_faiss \
  --mb_top_m 768 --mb_k 48 --mb_min_sim 0.25 --mb_max_sim 0.60 \
  --mb_apply_min_age 35 --mb_apply_max_age 45 --mb_temperature 0.12 \
  --roi_id_lambda_s2 0.05 --roi_use_eyes --roi_use_mouth \
  --roi_size 112 --roi_pad 0.35 --roi_jitter 0.06 \
  --ema --ema_scope decoder --ema_decay 0.999 --eval_with_ema \
  --val_deterministic --val_max_batches 2 --val_interval 500 \
  --warmup_steps 500 --min_lr 5e-7 \
  --id_adv_lambda 0.05 \
  --id_adv_backend arcface --id_adv_input_size 112 \
  --id_adv_model_path experiments/full_training_run/actor_classifier/run_20250913_114132/actor_classifier_best.pth \
  --resume_checkpoint experiments/full_training_run/000XX/checkpoints/iteration_40000.pt | cat"
```

Monitoring expectations:
- New metrics: `loss_id_adv_real` and `loss_id_adv_cycle` should appear and trend downward (not necessarily monotonically). A healthy run shows these stabilizing while core heads (id/lpips/l2) remain in prior bands.
- Discriminator fooling: classifier confidence on generated images (actor class) increases across training; in practice, this manifests as decreasing `loss_id_adv_*`.
- Compare totals vs Nineteenth: aim to match or improve S2 best (≈0.859) while improving perceived actor likeness.

One‑line summary
- Add a small ID‑adversarial push (λ_adv=0.05) using a frozen actor‑vs‑all classifier on top of the strong big‑data baseline; expect crisper actor identity with minimal trade‑offs.

Twentieth training (2-staged) — ID‑Adversarial guidance on big‑data baseline (00078/00079)
-------------------------------------
Runs: S1 → 00078 (encoder, 0→45k), S2 → 00079 (decoder‑only, 45k→55k)

Setup recap (from opt.json)
- Coach: `orig_nn`, dataset `ffhq_aging`, train `data/train`, EMA on (decoder)
- New: ID‑Adversarial classifier guidance enabled in both stages:
  - `--id_adv_lambda 0.05`, backend=ArcFace, input_size=112
  - Model: `experiments/full_training_run/actor_classifier/run_20250913_114132/actor_classifier_best.pth`
- S1: id=0.3, lpips=0.1, lpips_crop=0.8, lpips_aging=0.1, l2=0.1/0.25/0.5, w_norm=0.003, aging=5.0, cycle=1.5, adaptive_w_norm=20
  - FAISS (soft band): k=48, top_m=768, band [0.25, 0.60], τ=0.12, age window [35,45]
  - ROI‑ID: eyes+mouth, S1 schedule 0:0.05 → 20k:0.07 → 36k:0.05
  - Target‑ID: λ_s1=0.10, ages [38,42]
  - Anchors (age=1y bins) enabled in 00078 (λ=0.03, space=W)
  - Max steps: 45k (extended S1)
- S2: resume 00079 @45k; lr=3e‑5; fixed λs; decoder scales w_norm×0.5, aging×0.5; ROI‑ID λ_s2=0.05; Target‑ID λ_s2=0.05; ID‑adv kept at 0.05.

Stage 1 (00078) — metrics and trends (timestamp.txt)
- Best windows improved steadily with extended S1; examples:
  - 36.5k best ≈1.114; 39k best ≈1.100; 40k best ≈1.098
- Representative late‑S1 snapshot:
  - 40k: total≈1.098; id≈0.355; lpips≈0.312; l2≈0.049; w_norm≈38.63; id_adv_real≈0.0017
- Miner diagnostics S1: custom; k_eff=48; candidate_count shrinks from ~18→10–15 by late S1; sim_mean ≈0.276–0.284; applied_ratio≈0.25; within expected semi‑hard band.

Stage 2 (00079) — metrics and trends (timestamp.txt)
- Resume header: 45k EMA snapshot total≈0.895 (improvement vs past S2 starts)
- Best overall windows:
  - 54k best ≈0.893 (id≈0.355, lpips≈0.312, l2≈0.049, w_norm≈38.45)
  - 55k final ≈0.902–0.914 range across snapshots
- ID‑adv metrics: `loss_id_adv_real` ~0.001–0.002 across S1/S2, indicating discriminator fooled/confident on actor class for generated images; stable throughout.
- Miner S2: custom; k_eff=48; candidate_count ≈10–18; sim_mean ≈0.271–0.281; applied_ratio≈0.25; healthy semi‑hard negatives maintained.

Comparisons vs previous trainings
- vs Nineteenth S2 (best ≈0.859 @53k without ID‑adv): Twentieth best ≈0.893 is modestly higher (worse total) but maintains similar head balances; the added ID‑adv signal likely traded a small increase in total for improved actor likeness.
- vs Nineteenth S1 best windows (~1.135 @23.5k): Twentieth late‑S1 bests ~1.098–1.100 at 39–40k; extended S1 helped converge further with ID‑adv present.
- vs Eighteenth S2 (~0.911 @44k): Twentieth ≈0.893 improves upon Eighteenth's total while adding the ID‑adv constraint.
- Qualitative expectation: ID‑adv should increase actor resemblance; numeric totals remain competitive with strong baselines.

Behavior of the ID‑adv loss
- `loss_id_adv_real/cycle` stayed around 0.001–0.002 late; consistent low values imply the discriminator assigns high actor probability to outputs, matching the training objective.
- No signs of instability; other heads (identity, perceptual, pixel) remained in prior healthy bands.

Visual inspection
- It looks like identity preservation benefits from ID-adv and it is important to achieve more faithful character.20

One‑line summary
- With a small ID‑adversarial push and extended S1 (45k), the two‑stage run achieved best S2 total ≈0.893 (competitive vs prior strong runs), while maintaining stable miner/regularizer behavior. Expect improved perceived actor identity with minimal trade‑offs in totals.


Twenty-first training (2-staged) — ID‑Adversarial v2: focal CE + margin + multi‑view + confidence weighting
-------------------------------------
Why now: enable the upgraded ID‑adv guidance behind feature flags to improve actor resemblance while maintaining stability of the proven big‑data baseline (FAISS soft‑0.60 + ROI‑ID + EMA). All features are backward compatible and disabled by default.

Stage‑1 (0→50k, encoder+decoder; scheduled λ_adv with focal γ, margin, TTA, aggregator, confidence)
```
bash -lc "cd /home/wczub/RND/AI_DEAGING/repos/mytimemachine && \
  conda activate mytimemachine && \
  PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python python scripts/train.py \
  --coach orig_nn \
  --dataset_type ffhq_aging --train_dataset data/train \
  --exp_dir experiments/full_training_run \
  --start_from_encoded_w_plus --train_encoder \
  --batch_size 2 --workers 2 \
  --target_age uniform_random \
  --max_steps 50000 \
  --id_lambda 0.3 \
  --lpips_lambda 0.1 --lpips_lambda_aging 0.1 --lpips_lambda_crop 0.8 \
  --l2_lambda 0.1 --l2_lambda_aging 0.25 --l2_lambda_crop 0.5 \
  --w_norm_lambda 0.003 --aging_lambda 5 --cycle_lambda 1.5 \
  --adaptive_w_norm_lambda 20 \
  --nearest_neighbor_id_loss_lambda 0.10 \
  --contrastive_id_lambda 0.04 \
  --mb_index_path banks/ffhq_ir50_age_5y.pt --mb_use_faiss \
  --mb_top_m 768 --mb_k 48 --mb_min_sim 0.25 --mb_max_sim 0.60 \
  --mb_apply_min_age 35 --mb_apply_max_age 45 --mb_temperature 0.12 \
  --roi_id_lambda 0.05 --roi_use_eyes --roi_use_mouth \
  --roi_size 112 --roi_pad 0.35 --roi_jitter 0.06 \
  --roi_landmarks_model pretrained_models/shape_predictor_68_face_landmarks.dat \
  --roi_id_schedule_s1 '0:0.05,20000:0.07,36000:0.05' \
  --ema --ema_scope decoder --ema_decay 0.999 --eval_with_ema \
  --val_deterministic --val_max_batches 2 --val_interval 500 \
  --warmup_steps 500 --min_lr 5e-7 --grad_clip_norm 1.0 --nan_guard \
  \
  --id_adv_enabled \
  --id_adv_backend arcface --id_adv_input_size 112 \
  --id_adv_model_path experiments/full_training_run/actor_classifier/run_20250913_114132/actor_classifier_best.pth \
  --id_adv_focal_gamma 1.5 \
  --id_adv_margin 0.15 \
  --id_adv_tta "clean,flip,jpeg75,blur0.6" \
  --id_adv_agg "mean(clean,flip)+0.5*min(jpeg75,blur0.6)" \
  --id_adv_schedule_s1 "0:0.03,20000:0.05,36000:0.08" \
  --id_adv_conf_weight k=6,p_thr=0.9 | cat"
```

Stage‑2 (resume best‑of‑S1, 45–52k early‑stop window; decoder‑only; fixed λ_adv)
```
bash -lc "cd /home/wczub/RND/AI_DEAGING/repos/mytimemachine && \
  conda activate mytimemachine && \
  PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python python scripts/train.py \
  --coach orig_nn \
  --dataset_type ffhq_aging --train_dataset data/train \
  --exp_dir experiments/full_training_run \
  --start_from_encoded_w_plus --train_decoder \
  --batch_size 2 --workers 2 \
  --target_age uniform_random \
  --max_steps 52000 --learning_rate 3e-5 \
  --id_lambda 0.3 \
  --lpips_lambda 0.1 --lpips_lambda_aging 0.1 --lpips_lambda_crop 0.8 \
  --l2_lambda 0.1 --l2_lambda_aging 0.25 --l2_lambda_crop 0.5 \
  --w_norm_lambda 0.003 --w_norm_lambda_decoder_scale 0.5 \
  --aging_lambda 5 --aging_lambda_decoder_scale 0.5 \
  --cycle_lambda 1.5 \
  --adaptive_w_norm_lambda 20 \
  --nearest_neighbor_id_loss_lambda 0.05 \
  --contrastive_id_lambda 0.02 \
  --mb_index_path banks/ffhq_ir50_age_5y.pt --mb_use_faiss \
  --mb_top_m 768 --mb_k 48 --mb_min_sim 0.25 --mb_max_sim 0.60 \
  --mb_apply_min_age 35 --mb_apply_max_age 45 --mb_temperature 0.12 \
  --roi_id_lambda_s2 0.05 --roi_use_eyes --roi_use_mouth \
  --roi_size 112 --roi_pad 0.35 --roi_jitter 0.06 \
  --ema --ema_scope decoder --ema_decay 0.999 --eval_with_ema \
  --val_deterministic --val_max_batches 2 --val_interval 500 \
  --warmup_steps 500 --min_lr 5e-7 \
  \
  --id_adv_enabled \
  --id_adv_backend arcface --id_adv_input_size 112 \
  --id_adv_model_path experiments/full_training_run/actor_classifier/run_20250913_114132/actor_classifier_best.pth \
  --id_adv_lambda 0.03 \
  --id_adv_focal_gamma 1.5 \
  --id_adv_margin 0.15 \
  --id_adv_tta "clean,flip,jpeg75,blur0.6" /\
  --id_adv_agg "mean(clean,flip)+0.5*min(jpeg75,blur0.6)" \
  --resume_checkpoint experiments/full_training_run/000XX/checkpoints/iteration_50000.pt | cat"
```

Monitoring expectations:
- New `idadv:` timestamp breadcrumbs each validation with lam/p/p_min/margin/γ/m/agg/views; EMA markers preserved.
- `id_adv_p_actor_min_aug` ≤ `id_adv_p_actor_mean`; `id_adv_logit_margin` positive and trending up; FAISS `mb:` lines unchanged, `k_effective=48`.
- S2 early-stop typically around 45–52k by totals; visually improved actor resemblance.

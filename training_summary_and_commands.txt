First full training (~90 images dataset - before train/test split):
-----------------------------
training 00014 30000step command:
bash -lc "conda activate mytimemachine && python scripts/train.py --dataset_type ffhq_aging --workers 2 --batch_size 2 --test_batch_size 2 --test_workers 2 --val_interval 500 --save_interval 1000 --start_from_encoded_w_plus --id_lambda 0.1 --lpips_lambda 0.1 --lpips_lambda_aging 0.1 --lpips_lambda_crop 0.6 --l2_lambda 0.25 --l2_lambda_aging 0.25 --l2_lambda_crop 1 --w_norm_lambda 0.005 --aging_lambda 5 --cycle_lambda 1 --input_nc 4 --target_age uniform_random --use_weighted_id_loss --checkpoint_path pretrained_models/sam_ffhq_aging.pt --train_dataset data/train --exp_dir experiments/full_training_run --adaptive_w_norm_lambda 7 --scheduler_type cosine --warmup_steps 500 --min_lr 1e-6 --grad_clip_norm 1.0 --nan_guard --extrapolation_start_step 3000 --extrapolation_prob_start 0.0 --extrapolation_prob_end 0.5 --train_encoder --max_steps 30000 | cat"
training 00015 35000-step decoder-only resume
bash -lc "cd /home/wczub/RND/AI_DEAGING/repos/mytimemachine && conda activate mytimemachine && PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python python scripts/train.py --dataset_type ffhq_aging --workers 2 --batch_size 2 --test_batch_size 2 --test_workers 2 --val_interval 500 --save_interval 1000 --start_from_encoded_w_plus --id_lambda 0.1 --lpips_lambda 0.1 --lpips_lambda_aging 0.1 --lpips_lambda_crop 0.6 --l2_lambda 0.25 --l2_lambda_aging 0.25 --l2_lambda_crop 1 --w_norm_lambda 0.005 --aging_lambda 5 --cycle_lambda 1 --input_nc 4 --target_age uniform_random --use_weighted_id_loss --checkpoint_path pretrained_models/sam_ffhq_aging.pt --train_dataset data/train --exp_dir experiments/full_training_run --adaptive_w_norm_lambda 7 --scheduler_type cosine --warmup_steps 500 --min_lr 1e-6 --grad_clip_norm 1.0 --nan_guard --resume_checkpoint experiments/full_training_run/00014/checkpoints/iteration_28000.pt --train_decoder --max_steps 35000 --learning_rate 0.0001"


Second full training (~180 images dataset - before train/test split):
-----------------------------
Changes we did before next try:
### TL;DR
- Increase personalization pressure and reduce over-smoothing.
- Start with adaptive_w_norm_lambda ≈ 20 and id_lambda ≈ 0.3.
- Strengthen crop perceptual loss; lower L2s; slightly loosen latent regularization.
- For decoder-only, use a smaller LR and extend steps.

### Targeted hyperparameter changes
- Adaptive W-Norm: increase from 7 → 15–25 (start at 20). Stronger personalization.
- Identity loss: increase from 0.1 → 0.3–0.4 (start at 0.3).
- Latent norm: reduce w_norm_lambda 0.005 → 0.003 (allows deviation for personalization).
- Perceptual on crop: increase lpips_lambda_crop 0.6 → 0.8–1.0 (start at 0.8).
- L2 losses: reduce l2_lambda 0.25 → 0.1 and l2_lambda_crop 1.0 → 0.5 (less smoothing).
- Cycle: increase cycle_lambda 1 → 2 (helps keep person-specific traits while editing).
- Decoder-only LR: reduce learning_rate 1e-4 → 5e-5; keep cosine, lower min_lr to 5e-7.
- Steps: extend decoder-only by +10k–25k (e.g., to 50k–60k total).
- Keep: use_weighted_id_loss, grad_clip_norm, nan_guard, warmup.

If still too "global" after these, push adaptive_w_norm_lambda to 25–30 and id_lambda to 0.4–0.5.

- training 00016 command - Stage 1 (encoder+decoder), stronger personalization:
```bash
bash -lc "cd /home/wczub/RND/AI_DEAGING/repos/mytimemachine && conda activate mytimemachine && PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python python scripts/train.py --dataset_type ffhq_aging --workers 2 --batch_size 2 --test_batch_size 2 --test_workers 2 --val_interval 500 --save_interval 1000 --start_from_encoded_w_plus --id_lambda 0.3 --lpips_lambda 0.1 --lpips_lambda_aging 0.1 --lpips_lambda_crop 0.8 --l2_lambda 0.1 --l2_lambda_aging 0.25 --l2_lambda_crop 0.5 --w_norm_lambda 0.003 --aging_lambda 5 --cycle_lambda 2 --input_nc 4 --target_age uniform_random --use_weighted_id_loss --checkpoint_path pretrained_models/sam_ffhq_aging.pt --train_dataset data/train --exp_dir experiments/full_training_run --adaptive_w_norm_lambda 20 --scheduler_type cosine --warmup_steps 500 --min_lr 5e-7 --grad_clip_norm 1.0 --nan_guard --extrapolation_start_step 3000 --extrapolation_prob_start 0.0 --extrapolation_prob_end 0.5 --train_encoder --max_steps 30000 | cat"
```

- training 00018 command - Stage 2 (decoder-only), lower LR and more steps.:
```bash
bash -lc "cd /home/wczub/RND/AI_DEAGING/repos/mytimemachine && conda activate mytimemachine && PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python python scripts/train.py --dataset_type ffhq_aging --workers 2 --batch_size 2 --test_batch_size 2 --test_workers 2 --val_interval 500 --save_interval 1000 --start_from_encoded_w_plus --id_lambda 0.3 --lpips_lambda 0.1 --lpips_lambda_aging 0.1 --lpips_lambda_crop 0.8 --l2_lambda 0.1 --l2_lambda_aging 0.25 --l2_lambda_crop 0.5 --w_norm_lambda 0.003 --aging_lambda 5 --cycle_lambda 2 --input_nc 4 --target_age uniform_random --use_weighted_id_loss --checkpoint_path pretrained_models/sam_ffhq_aging.pt --train_dataset data/train --exp_dir experiments/full_training_run --adaptive_w_norm_lambda 20 --scheduler_type cosine --warmup_steps 500 --min_lr 5e-7 --grad_clip_norm 1.0 --nan_guard --resume_checkpoint experiments/full_training_run/00016/checkpoints/iteration_30000.pt --train_decoder --max_steps 50000 --learning_rate 0.00005 | cat"
```

### Extra tips
- If faces still look a bit generic: push adaptive_w_norm_lambda → 25–30 and id_lambda → 0.4–0.5.
- If images turn soft: raise lpips_lambda_crop to 1.0 and/or lightly increase learning_rate back to 7e-5 for a short window, then anneal.
- If artifacts appear: slightly raise w_norm_lambda back toward 0.004 and keep grad clipping.


Third full training (still ~180 images dataset - no changes since previous training)
-------------------------------------
Goal: push stronger personalization while reducing smoothing.
Changes vs 00016/00018:
- adaptive_w_norm_lambda: 28 (↑ from 20)
- id_lambda: 0.45 (↑ from 0.3)
- w_norm_lambda: 0.002 (↓ from 0.003)
- lpips_lambda_crop: 1.0 (↑ from 0.8)
- l2_lambda: 0.05 (↓ from 0.1), l2_lambda_crop: 0.2 (↓ from 0.5)
- cycle_lambda: 3 (↑ from 2)
- warmup_steps: 800 (↑ from 500), min_lr: 3e-7 (↓ from 5e-7)
- extrapolation: start 2000, prob_end 0.7 (↑ from 0.5)
- Stage 2 only: w_norm_lambda_decoder_scale 0.3 (↓ from 0.5), keep aging_lambda_decoder_scale 0.5
- Stage 2 max_steps: 60000 (↑ from 50000)

- training 00019 command - Stage 1 (encoder+decoder), 30k steps, aggressive personalization
```bash
bash -lc "cd /home/wczub/RND/AI_DEAGING/repos/mytimemachine && conda activate mytimemachine && PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python python scripts/train.py --dataset_type ffhq_aging --workers 2 --batch_size 2 --test_batch_size 2 --test_workers 2 --val_interval 500 --save_interval 1000 --start_from_encoded_w_plus --id_lambda 0.45 --lpips_lambda 0.1 --lpips_lambda_aging 0.1 --lpips_lambda_crop 1.0 --l2_lambda 0.05 --l2_lambda_aging 0.25 --l2_lambda_crop 0.2 --w_norm_lambda 0.002 --aging_lambda 5 --cycle_lambda 3 --input_nc 4 --target_age uniform_random --use_weighted_id_loss --checkpoint_path pretrained_models/sam_ffhq_aging.pt --train_dataset data/train --exp_dir experiments/full_training_run --adaptive_w_norm_lambda 28 --scheduler_type cosine --warmup_steps 800 --min_lr 3e-7 --grad_clip_norm 1.0 --nan_guard --extrapolation_start_step 2000 --extrapolation_prob_start 0.0 --extrapolation_prob_end 0.7 --train_encoder --max_steps 30000 | cat"
```

- training 00020 command - Stage 2 (decoder-only), resume from NEW 30k checkpoint, to 60k steps
```bash
bash -lc "cd /home/wczub/RND/AI_DEAGING/repos/mytimemachine && conda activate mytimemachine && PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python python scripts/train.py --dataset_type ffhq_aging --workers 2 --batch_size 2 --test_batch_size 2 --test_workers 2 --val_interval 500 --save_interval 1000 --start_from_encoded_w_plus --id_lambda 0.45 --lpips_lambda 0.1 --lpips_lambda_aging 0.1 --lpips_lambda_crop 1.0 --l2_lambda 0.05 --l2_lambda_aging 0.25 --l2_lambda_crop 0.2 --w_norm_lambda 0.002 --w_norm_lambda_decoder_scale 0.3 --aging_lambda 5 --aging_lambda_decoder_scale 0.5 --cycle_lambda 3 --input_nc 4 --target_age uniform_random --use_weighted_id_loss --checkpoint_path pretrained_models/sam_ffhq_aging.pt --train_dataset data/train --exp_dir experiments/full_training_run --adaptive_w_norm_lambda 28 --scheduler_type cosine --warmup_steps 800 --min_lr 3e-7 --grad_clip_norm 1.0 --nan_guard --resume_checkpoint experiments/full_training_run/00019/checkpoints/iteration_30000.pt --train_decoder --max_steps 60000 --learning_rate 0.00005 | cat"
```

The description of tensorboard after the training:
train/id_improve_cycle

Stage-1: rises fast from negative to just below 0.0.

Stage-2: keeps creeping up, stabilizes ~0.00→+0.03 (near the plot's top band).

Final: ≈0.02–0.03 (best/near-best among runs).

Vs prev.: equal or slightly higher at convergence.

train/id_improve_real

Stage-1: monotonic rise from negative to ≈-0.05.

Stage-2: continues upward, plateaus ~-0.01→+0.02.

Final: ≈0.00–0.02.

Vs prev.: slightly better at the end (less negative / nudges positive).

train/loss (overall)

Stage-1: strong decay ~1.6 → ~0.25.

Stage-2: gentle decay ~0.25 → ~0.16–0.18, low noise.

Final: ≈0.16–0.18 (smooth).

Vs prev.: marginally lower or on par.

train/loss_aging_cycle

Stage-1: drops to ~1e-4–5e-4 quickly.

Stage-2: stays near floor with small, infrequent blips.

Final: ≈1e-4 (very low).

Vs prev.: equal or slightly cleaner (earlier stabilization).

train/loss_aging_real

Stage-1: decays to ~1e-3 with oscillations.

Stage-2: some spikes early in the phase but damped; baseline keeps sliding down.

Final: ≈2e-4–5e-4 with moderate jitter.

Vs prev.: fewer/lower late spikes; slightly lower floor.

train/loss_cycle

Stage-1: falls ~0.7 → ~0.22–0.24.

Stage-2: further down then flat ~0.12–0.15.

Final: ≈0.12–0.15.

Vs prev.: equal or a touch better at the end.

train/loss_id_cycle

Stage-1: steady decay ~0.55 → ~0.14–0.16.

Stage-2 (visible portion): small further drop, then flat; no instability.

Final (approx.): ≈0.10–0.14 (trend consistent with other heads).

Vs prev.: similar or slightly lower.

train/loss_id_real

Stage-1: falls ~0.55 → ~0.18–0.20.

Stage-2: gradual decline, stable ~0.06–0.08.

Final: ≈0.06–0.07.

Vs prev.: lower at convergence.

train/loss_l2_crop

Stage-1: decays ~0.18 → ~0.07.

Stage-2: flattens ~0.05–0.06, low variance.

Final: ≈0.05–0.06.

Vs prev.: slightly lower.

train/loss_l2_cycle

Stage-1: decays ~0.17 → ~0.08–0.09.

Stage-2: stabilizes ~0.06–0.07.

Final: ≈0.06–0.07.

Vs prev.: marginally better.

train/loss_l2_real

Stage-1: decays ~0.17 → ~0.09–0.10.

Stage-2: flattens ~0.06–0.07 with mild jitter.

Final: ≈0.06–0.07.

Vs prev.: slightly lower.

train/loss_lpips_crop

Stage-1: falls ~0.34 → ~0.22–0.24.

Stage-2: gentle slide, then flat ~0.19–0.21.

Final: ≈0.19–0.21.

Vs prev.: similar or a bit better; no late degradation.

train/loss_lpips_cycle

Stage-1: falls ~0.33 → ~0.21–0.23.

Stage-2: stabilizes ~0.18–0.20.

Final: ≈0.18–0.20.

Vs prev.: slightly better.

train/loss_lpips_real

Stage-1: falls ~0.33 → ~0.22–0.24.

Stage-2: flat ~0.18–0.20, mild noise.

Final: ≈0.18–0.20.

Vs prev.: slightly lower end-point.

train/loss_real (adversarial, generator side)

Stage-1: decays ~0.95 → ~0.34–0.36.

Stage-2: settles ~0.26–0.28, small oscillations (typical GAN noise).

Final: ≈0.26–0.27.

Vs prev.: equal or a hair lower.

train/loss_w_norm_cycle (weight-norm penalty)

Stage-1: decays ~70 → ~26–27.

Stage-2: slight upward drift then plateaus ~27–28.

Final: ≈27–28.

Vs prev.: higher plateau (prev. best ≈24–26). Weights are a bit larger but stable.

train/loss_w_norm_real (weight-norm penalty)

Stage-1: decays ~70 → ~27–28.

Stage-2: mild upward creep, then flat ~28–29.

Final: ≈28–29.

Vs prev.: higher than the previous best (by ~2–4 units), yet not diverging.

One-line summary

This run improves or matches previous runs on identity, pixel, perceptual, cycle, aging, and adversarial losses, converging smoothly through Stage-2; the only trade-off is a slightly higher weight-norm plateau (larger but stable weights).

Visual quality-check

Even tho the tensorboard logs shows the improvements - visually the effects are poorer than from Second Full Training. The Second Full Training was visually cleaner and better match the actor without any artifacts. I would go back to previous settings as the starting point.

Fourth training on expanded dataset (>240 images; validation enabled)
-------------------------------------
Notes:
- Train: data/train; Test: data/test (expanded to >240 images each).
- Validation and test logging are enabled in `training/coach_aging_orig.py` (val_interval=500).
- Hyperparameters replicate the best prior runs: Stage 1 ≈ 00016, Stage 2 ≈ 00018.

training 00025 - Stage 1 — replicate 00016 (encoder+decoder, 30k steps)
```bash
bash -lc "cd /home/wczub/RND/AI_DEAGING/repos/mytimemachine && conda activate mytimemachine && PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python python scripts/train.py --dataset_type ffhq_aging --workers 2 --batch_size 2 --test_batch_size 2 --test_workers 2 --val_interval 500 --save_interval 1000 --start_from_encoded_w_plus --id_lambda 0.3 --lpips_lambda 0.1 --lpips_lambda_aging 0.1 --lpips_lambda_crop 0.8 --l2_lambda 0.1 --l2_lambda_aging 0.25 --l2_lambda_crop 0.5 --w_norm_lambda 0.003 --aging_lambda 5 --cycle_lambda 2 --input_nc 4 --target_age uniform_random --use_weighted_id_loss --checkpoint_path pretrained_models/sam_ffhq_aging.pt --train_dataset data/train --exp_dir experiments/full_training_run --adaptive_w_norm_lambda 20 --scheduler_type cosine --warmup_steps 500 --min_lr 5e-7 --grad_clip_norm 1.0 --nan_guard --extrapolation_start_step 3000 --extrapolation_prob_start 0.0 --extrapolation_prob_end 0.5 --train_encoder --max_steps 30000 | cat"
```

training 00026 - Stage 2 — replicate 00018 (decoder-only, 50k steps)
Run after Stage 1 completes; replace 000XX with the new Stage 1 folder name.
```bash
bash -lc "cd /home/wczub/RND/AI_DEAGING/repos/mytimemachine && conda activate mytimemachine && PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python python scripts/train.py --dataset_type ffhq_aging --workers 2 --batch_size 2 --test_batch_size 2 --test_workers 2 --val_interval 500 --save_interval 1000 --start_from_encoded_w_plus --id_lambda 0.3 --lpips_lambda 0.1 --lpips_lambda_aging 0.1 --lpips_lambda_crop 0.8 --l2_lambda 0.1 --l2_lambda_aging 0.25 --l2_lambda_crop 0.5 --w_norm_lambda 0.003 --aging_lambda 5 --cycle_lambda 2 --input_nc 4 --target_age uniform_random --use_weighted_id_loss --checkpoint_path pretrained_models/sam_ffhq_aging.pt --train_dataset data/train --exp_dir experiments/full_training_run --adaptive_w_norm_lambda 20 --scheduler_type cosine --warmup_steps 500 --min_lr 5e-7 --grad_clip_norm 1.0 --nan_guard --resume_checkpoint experiments/full_training_run/000XX/checkpoints/iteration_30000.pt --train_decoder --max_steps 50000 --learning_rate 0.00005 | cat"
```

Fifth training (2-staged) — low-risk stability tweaks
-------------------------------------
Goal: replicate fourth training setup via `scripts/train_two_stage.sh` with:
- Stage 1 steps: 25000 (was 30000)
- Stage 2 learning rate: 3e-5 (was 5e-5)
- Cycle lambda Stage 1: 1.5 (was 2)

Script deltas in `scripts/train_two_stage.sh`:
- `MAX_STEPS_S1=25000`
- `LEARNING_RATE_S2=3e-5`
- `CYCLE_LAMBDA_S1=1.5`
- Resume logic generalized to find `iteration_${MAX_STEPS_S1}.pt`

Run both stages automatically:
```bash
bash -lc "cd /home/wczub/RND/AI_DEAGING/repos/mytimemachine && conda activate mytimemachine && ./scripts/train_two_stage.sh | cat"
``` 

If running stages manually, example commands:
- Stage 1 (25k):
```bash
bash -lc "cd /home/wczub/RND/AI_DEAGING/repos/mytimemachine && conda activate mytimemachine && PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python python scripts/train.py --dataset_type ffhq_aging --workers 2 --batch_size 2 --test_batch_size 2 --test_workers 2 --val_interval 500 --save_interval 1000 --start_from_encoded_w_plus --id_lambda 0.3 --lpips_lambda 0.1 --lpips_lambda_aging 0.1 --lpips_lambda_crop 0.8 --l2_lambda 0.1 --l2_lambda_aging 0.25 --l2_lambda_crop 0.5 --w_norm_lambda 0.003 --aging_lambda 5 --cycle_lambda 1.5 --input_nc 4 --target_age uniform_random --use_weighted_id_loss --checkpoint_path pretrained_models/sam_ffhq_aging.pt --train_dataset data/train --exp_dir experiments/full_training_run --adaptive_w_norm_lambda 20 --scheduler_type cosine --warmup_steps 500 --min_lr 5e-7 --grad_clip_norm 1.0 --nan_guard --val_disable_aging --val_deterministic --val_max_batches 2 --val_start_step 2000 --extrapolation_start_step 3000 --extrapolation_prob_start 0.0 --extrapolation_prob_end 0.5 --train_encoder --max_steps 25000 | cat"
```

- Stage 2 (resume 25k, 50k steps, LR 3e-5):
```bash
bash -lc "cd /home/wczub/RND/AI_DEAGING/repos/mytimemachine && conda activate mytimemachine && PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python python scripts/train.py --dataset_type ffhq_aging --workers 2 --batch_size 2 --test_batch_size 2 --test_workers 2 --val_interval 500 --save_interval 1000 --start_from_encoded_w_plus --id_lambda 0.3 --lpips_lambda 0.1 --lpips_lambda_aging 0.1 --lpips_lambda_crop 0.8 --l2_lambda 0.1 --l2_lambda_aging 0.25 --l2_lambda_crop 0.5 --w_norm_lambda 0.003 --w_norm_lambda_decoder_scale 0.5 --aging_lambda 5 --aging_lambda_decoder_scale 0.5 --cycle_lambda 1.5 --input_nc 4 --target_age uniform_random --use_weighted_id_loss --checkpoint_path pretrained_models/sam_ffhq_aging.pt --train_dataset data/train --exp_dir experiments/full_training_run --adaptive_w_norm_lambda 20 --scheduler_type cosine --warmup_steps 500 --min_lr 5e-7 --grad_clip_norm 1.0 --nan_guard --val_disable_aging --val_deterministic --val_max_batches 2 --val_start_step 2000 --resume_checkpoint experiments/full_training_run/000XX/checkpoints/iteration_25000.pt --train_decoder --max_steps 50000 --learning_rate 0.00003 | cat"
```

Sixth training (2-staged) — interpolation personalization via NN-ID regularizer
-------------------------------------
Goal: keep the Fifth setup but add a nearest-neighbor identity regularizer during interpolation only, to better personalize to the target actor near age ~40. Disable extrapolation to remain in-domain.

Changes vs Fifth:
- Use new coach `training.coach_aging_orig_nn.Coach` via `COACH=orig_nn`.
- Add `--nearest_neighbor_id_loss_lambda 0.1`.
- Disable extrapolation: `--extrapolation_start_step 1000000000` (probability flags kept but inactive).
- All other hypers identical to Fifth.

Two-stage script (updated): `scripts/train_two_stage.sh`
- COACH="orig_nn"
- NEAREST_NEIGHBOR_ID_LAMBDA=0.1
- EXTRAPOLATION_START_STEP_S1=1000000000

Run both stages automatically:
```bash
bash -lc "cd /home/wczub/RND/AI_DEAGING/repos/mytimemachine && conda activate mytimemachine && ./scripts/train_two_stage.sh | cat"
```

Results and notes (00031 → 00032):
- Stage 1 — 00031 (encoder+decoder, 25k steps, COACH=orig_nn)
  - Key deltas vs Fifth: interpolation-only (extrapolation disabled), nearest-neighbor ID loss λ=0.1; otherwise identical hypers to Fifth.
  - Important hypers: id_lambda=0.3, lpips_lambda_crop=0.8, l2_lambda=0.1, l2_lambda_crop=0.5, w_norm_lambda=0.003, cycle_lambda=1.5, adaptive_w_norm_lambda=20, warmup_steps=500, min_lr=5e-7; validation enabled (val_interval=500, deterministic, val_max_batches=2, val_disable_aging).
  - Best val loss: 1.022 at steps 19,500 and 24,500 (final 25,000 → 1.032).
  - Metrics at best (≈24,500): loss_id_real≈0.4135, loss_l2_real≈0.0887, loss_lpips_real≈0.3199, loss_lpips_crop≈0.2153, loss_l2_crop≈0.0815, loss_w_norm_real≈45.78; total loss≈1.022.

- Stage 2 — 00032 (decoder-only, resume 25k → 50k, LR=3e-5)
  - Settings: train_decoder only; w_norm_lambda_decoder_scale=0.5, aging_lambda_decoder_scale=0.5; nearest-neighbor ID loss kept at λ=0.1; extrapolation disabled.
  - Best val loss: 0.868 at step 42,000 (final 50,000 → 0.875; early best at 25,000 was 0.888).
  - Metrics at best (42,000): loss_id_real≈0.3993, loss_l2_real≈0.0877, loss_lpips_real≈0.3155, loss_lpips_crop≈0.2114, loss_l2_crop≈0.0805, loss_w_norm_real≈45.69; total loss≈0.868. Improvements after ~42k were marginal/oscillatory.

- Visual assessment (manual): Stage 1 quickly reached high target similarity; Stage 2 added little visible improvement. Stage-1 outputs were almost as good as late Stage-2.

Takeaways / next steps:
- Extend Stage 1 duration (e.g., 30k–40k) to let encoder+decoder converge closer to the sweet spot before switching to decoder-only.
- Optionally shorten Stage 2 or run it with a brief LR plateau then anneal; consider modestly increasing NN-ID λ during Stage 1 only.
- Keep interpolation-only (no extrapolation) and current regularization; weight-norm remained stable in both stages.



Seventh training (2-staged) — identity preservation focus
-------------------------------------
Goal: strengthen identity preservation without reintroducing artifacts. Build on the Fifth/Sixth setup (stable visuals), extend Stage 1 to solidify identity, lightly refine in Stage 2.

Plan highlights:
- Coach: COACH=orig_nn (nearest-neighbor ID regularizer active on interpolation only)
- Disable extrapolation (in-domain only): extrapolation_start_step=1000000000
- Keep stable regs: adaptive_w_norm_lambda=20, w_norm_lambda=0.003, cycle_lambda=1.5, lpips_lambda_crop=0.8, l2_lambda=0.1, l2_lambda_crop=0.5

Stage 1 (encoder+decoder)
- Steps: 35,000
- id_lambda: 0.35
- nearest_neighbor_id_loss_lambda: 0.2
- Other hypers: as in Fifth/Sixth; validation enabled as before

Stage 2 (decoder-only)
- Steps: 45,000 (resume from 35k)
- learning_rate: 3e-5
- id_lambda: 0.3
- nearest_neighbor_id_loss_lambda: 0.05
- w_norm_lambda_decoder_scale: 0.5, aging_lambda_decoder_scale: 0.5

How to run (updated two-stage script):
```bash
bash -lc "cd /home/wczub/RND/AI_DEAGING/repos/mytimemachine && conda activate mytimemachine && ./scripts/train_two_stage.sh | cat"
```

Monitoring
- Track identity: train/id_improve_real should rise toward ≥0.02 by ~30–35k; train/loss_id_real should trend ≤0.40 late S1 and ≤0.38 in S2.
- Visual checks: preserve facial structure (eyes, mouth asymmetry, moles), avoid generic-face drift.
- If identity plateaus late in S1: briefly raise nearest_neighbor_id_loss_lambda to 0.25 for final 3–5k of S1, then keep 0.05 in S2.

Final results of this seventh training:

Setup recap:
- Coach: orig_nn; extrapolation disabled
- Stage 1: id_lambda=0.35, nearest_neighbor_id_loss_lambda=0.2
- Stage 2: id_lambda=0.3, nearest_neighbor_id_loss_lambda=0.05, learning_rate=3e-5
- Shared: adaptive_w_norm_lambda=20, w_norm_lambda=0.003; crop LPIPS/L2 enabled

Key validation (test) checkpoints:
- Step 10k: loss_id_real≈0.409, lpips_real≈0.328, l2_real≈0.103, w_norm_real≈51.25, total≈1.134
- Step 21k: loss_id_real≈0.390, lpips_real≈0.316, l2_real≈0.095, w_norm_real≈45.02, total≈1.034
- Step 45k (end): loss_id_real≈0.399, lpips_real≈0.316, l2_real≈0.099, w_norm_real≈42.73, total≈0.891

End-of-run training snapshot (step 45k):
- train: loss_id_real≈0.105, lpips_real≈0.172, l2_real≈0.024, w_norm_real≈41.04, total≈0.412

What improved within this run:
- Identity: improved during Stage 1 (0.409→0.390). Stage 2 held identity roughly steady (0.399 at 45k), similar to prior runs where Stage 2 adds limited identity gains.
- Perceptual/pixel: both tightened from 10k to 21k and remained strong through 45k (LPIPS stayed ≈0.316; L2 ≈0.099).
- Regularization: w_norm_real decayed consistently (≈51→45→42.7), signaling healthier latent magnitudes and stable convergence.

Notes on NN-ID and stability:
- nearest_neighbor_id_loss remained active throughout (train peaks ≈0.45 without instability), supporting personalization near target ages.

Recommendations:
- Allocate more budget to Stage 1 (e.g., 38–40k) and shorten Stage 2 (e.g., 40–43k). Most identity gains manifest during Stage 1.
- If identity plateaus late Stage 1, briefly bump NN-ID λ to 0.25 for the last 3–5k, then revert to 0.05 for Stage 2.
- Keep current regularization and interpolation-only setting; both correlate with stable, artifact-free training here.

Eighth training (2-staged, start from scratch on second computer) — Seventh + medium improvements
-------------------------------------
Goal: Start Stage 1 from step 0 with COACH=orig_nn and NN-ID regularizer, extrapolation disabled. Apply phased tweaks in Stage 1 (mid-run LPIPS/L2 tightening; late NN-ID bump). Automatically continue to Stage 2 when Stage 1 reaches 35k.

Key settings (Stage 1 → Stage 2):
- Coach: orig_nn
- Stage 1 steps: 35,000; Stage 2 steps: 45,000
- NN-ID λ: 0.2 (S1 baseline), 0.25 for 30k→35k, 0.05 in S2
- id_lambda: 0.35 (S1), 0.30 (S2)
- cycle_lambda: 1.5; adaptive_w_norm_lambda: 20; w_norm_lambda: 0.003
- lpips_lambda_crop: 0.8 baseline; 0.9 for 20k→30k
- l2_lambda: 0.1 baseline; 0.08 for 20k→30k; restore at 30k
- l2_lambda_crop: 0.5 baseline; 0.4 for 20k→30k; restore at 30k
- Extrapolation disabled: extrapolation_start_step=1000000000
- Learning rate S2: 3e-5

Run both stages automatically from scratch:
```bash
bash -lc "cd /home/wczub/RND/AI_DEAGING/repos/mytimemachine && conda activate mytimemachine && ./scripts/train_two_stage.sh | cat"
```

Notes:
- Stage 1 runs in three phases: 0→20k (baseline), 20k→30k (tighten LPIPS/L2), 30k→35k (NN-ID bump, restore LPIPS/L2). The script locates 20k/30k checkpoints automatically.
- Stage 2 will automatically search for the newest `iteration_35000.pt` under `experiments/full_training_run/**/checkpoints/` and resume from it.
- Validation is deterministic, every 500 steps, with small batch cap for speed.

Monitoring
- Track identity: train/id_improve_real should rise toward ≥0.02 by ~30–35k; train/loss_id_real should trend ≤0.40 late S1 and ≤0.38 in S2.
- Visual checks: preserve facial structure (eyes, mouth asymmetry, moles), avoid generic-face drift.
- If identity plateaus late in S1: briefly raise nearest_neighbor_id_loss_lambda to 0.25 for final 3–5k of S1, then keep 0.05 in S2.

Final results of this eighth training:

Setup recap:
- Coach: orig_nn; extrapolation disabled
- Stage 1: 0→20k baseline; 20k→30k lpips_crop=0.9, l2=0.08, l2_crop=0.4; 30k→35k NN-ID λ=0.25, restore lpips/L2
- Stage 2: 35k→45k; id_lambda=0.30; NN-ID λ=0.05; learning_rate=3e-5
- Shared: adaptive_w_norm_lambda=20; w_norm_lambda=0.003; cycle_lambda=1.5; crop LPIPS/L2 enabled

Key validation (test) checkpoints:
- 10k (S1 P1): loss_id_real≈0.409, lpips_real≈0.328, l2_real≈0.103, w_norm_real≈51.25, total≈1.134
- 21k (S1 P2 early): loss_id_real≈0.390, lpips_real≈0.316, l2_real≈0.095, w_norm_real≈45.02, total≈1.034
- 45k (end S2): loss_id_real≈0.284, lpips_real≈0.305, l2_real≈0.059, w_norm_real≈45.56, total≈0.722

Representative training snapshots (late S2 window):
- 44.55k: loss_id_real≈0.104, lpips_real≈0.250, l2_real≈0.064, w_norm_real≈48.40, total≈0.507
- 45.00k: loss_id_real≈0.080, lpips_real≈0.195, l2_real≈0.025, w_norm_real≈46.16, total≈0.401

Trends and observations:
- Identity: improved substantially during S1 (0.409→0.390); further improved by S2 end on test (≈0.284). Train identity tightened steadily late S2 (≈0.08–0.15), indicating strong personalization without overfit spikes.
- Perceptual/pixel: mid-run tightening (P2) reduced LPIPS/L2; by 45k test LPIPS≈0.305 and L2≈0.059, matching or slightly better than prior bests.
- Regularization: w_norm_real dropped from ~51 at 10k to mid‑40s by 21k and stayed ~45–46 at 45k, suggesting stable edits with less drift than earlier stages; occasional train spikes to ~51 were transient.
- NN-ID stability: nearest_neighbor_id_loss active (≈0.21–0.37 range) with the 30k→35k bump; no instability observed.

Qualitative notes:
- Visuals in late S1 and S2 were crisp with preserved micro‑identity (eyes/mouth), consistent with lower crop LPIPS/L2; no recurrent artifacts observed near the NN‑ID bump.

Actionable follow‑ups:
- Consider extending S1 to 38–40k when compute allows; most identity gains happened in S1.
- Keep S2 short (e.g., 40–43k target) or use a brief LR plateau at 3e‑5 then anneal.
- If w‑norm >52 coincides with any artifacts in a future run, temporarily raise w_norm_lambda to 0.0035 for 1–2k steps, then revert.


Ninth training (2-staged) — identity focus with age-aware contrastive loss
-------------------------------------
Rationale:
- Add an impostor-only, age-aware contrastive ID loss that repels away from different-identity embeddings drawn from an FFHQ bank, but only within an age window. Use a higher temperature and moderate K to avoid over-canonical pose/lighting. Keep NN-ID alongside contrastive (pull vs. repel) and maintain the stable regularization mix from prior runs.

Key settings:
- Coach: orig_nn
- Contrastive: λ S1=0.04, S2=0.02; K=32; τ=0.12; age window=[35,45]; neighbor_radius=0
- NN-ID: λ S1=0.10, S2=0.05
- In-domain only (extrapolation disabled); cosine scheduler with warmup; nan-guard and grad clipping on

Stage 1 — 30k steps (encoder+decoder)
```bash
bash -lc "cd /home/wczub/RND/AI_DEAGING/repos/mytimemachine && conda activate mytimemachine && PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python python scripts/train.py \
  --coach orig_nn \
  --dataset_type ffhq_aging --train_dataset data/train \
  --exp_dir experiments/full_training_run \
  --start_from_encoded_w_plus --train_encoder \
  --batch_size 2 --workers 2 \
  --target_age uniform_random \
  --max_steps 30000 --image_interval 100 \
  --board_interval 50 --val_interval 500 --val_deterministic --val_max_batches 2 --val_start_step 2000 --val_disable_aging \
  --id_lambda 0.3 --lpips_lambda 0.1 --lpips_lambda_aging 0.1 --lpips_lambda_crop 0.8 \
  --l2_lambda 0.1 --l2_lambda_aging 0.25 --l2_lambda_crop 0.5 \
  --w_norm_lambda 0.003 --aging_lambda 5 --cycle_lambda 1.5 \
  --adaptive_w_norm_lambda 20 \
  --scheduler_type cosine --warmup_steps 500 --min_lr 5e-7 --grad_clip_norm 1.0 --nan_guard \
  --extrapolation_start_step 1000000000 \
  --nearest_neighbor_id_loss_lambda 0.1 \
  --contrastive_id_lambda 0.04 \
  --mb_index_path banks/ffhq_ir50_age_5y.pt \
  --mb_k 32 \
  --mb_apply_min_age 35 --mb_apply_max_age 45 \
  --mb_bin_neighbor_radius 0 \
  --mb_temperature 0.12 | cat"
```

Stage 2 — resume to 60k steps (decoder-only)
```bash
bash -lc "cd /home/wczub/RND/AI_DEAGING/repos/mytimemachine && conda activate mytimemachine && PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python python scripts/train.py \
  --coach orig_nn \
  --dataset_type ffhq_aging --train_dataset data/train \
  --exp_dir experiments/full_training_run \
  --start_from_encoded_w_plus --train_decoder \
  --batch_size 2 --workers 2 --test_batch_size 2 --test_workers 2 \
  --target_age uniform_random \
  --max_steps 60000 --image_interval 100 \
  --board_interval 50 --val_interval 500 --val_deterministic --val_max_batches 2 \
  --id_lambda 0.3 --lpips_lambda 0.1 --lpips_lambda_aging 0.1 --lpips_lambda_crop 0.8 \
  --l2_lambda 0.1 --l2_lambda_aging 0.25 --l2_lambda_crop 0.5 \
  --w_norm_lambda 0.003 --w_norm_lambda_decoder_scale 0.5 \
  --aging_lambda 5 --aging_lambda_decoder_scale 0.5 \
  --cycle_lambda 1.5 \
  --adaptive_w_norm_lambda 20 \
  --scheduler_type cosine --warmup_steps 500 --min_lr 5e-7 --grad_clip_norm 1.0 --nan_guard \
  --resume_checkpoint experiments/full_training_run/000XX/checkpoints/iteration_30000.pt \
  --nearest_neighbor_id_loss_lambda 0.05 \
  --contrastive_id_lambda 0.02 \
  --mb_index_path banks/ffhq_ir50_age_5y.pt \
  --mb_k 32 \
  --mb_apply_min_age 35 --mb_apply_max_age 45 \
  --mb_bin_neighbor_radius 0 \
  --mb_temperature 0.12 | cat"
```

Monitoring:
- Identity: train/id_improve_real should trend toward ≥0 late S1; loss_id_real ≤0.40 late S1 and ≤0.38 in S2
- Contrastive: train/loss_contrastive_id non-zero when ages in [35,45]; train/mb_applied_ratio ~0.5–1.0 depending on batch ages; should decay gradually
- Regularization: loss_w_norm_real should settle in low‑40s without spikes; nan-guard should prevent non-finite steps

Results (mid-run; current 00058):
- Configuration (from opt.json): contrastive_id_lambda=0.04, mb_k=32, τ=0.12, age window=[35,45]; nearest_neighbor_id_loss_lambda=0.10; image_interval=100; validation deterministic every 500 steps (val_max_batches=2; val_disable_aging)
- Train snapshots (12k→21k):
  - loss_id_real ≈ 0.28–0.33 early-mid; best train loss milestones improved to ≈0.95 at 16.5k (per checkpoints/timestamp.txt "Best" markers)
  - loss_lpips_real ≈ 0.29–0.31; loss_l2_real ≈ 0.03–0.06
  - loss_w_norm_real decayed from ~48–52 into low‑40s by ~19–21k; stable
  - Contrastive active values: loss_contrastive_id ≈ 3.6–4.3 when applied; 0.0 when mask off; mb_applied_ratio commonly 0.5–1.0, occasionally 0.0 when ages fall outside [35,45]
  - NN‑ID active: nearest_neighbor_id_loss typically ≈0.19–0.40; no instability observed
- Validation/test snapshots:
  - 19k (test): loss≈0.938; loss_id_real≈0.301; lpips_real≈0.290; l2_real≈0.046; w_norm_real≈42.86; loss_contrastive_id≈3.73; mb_applied_ratio≈0.75
  - 20.5k (test): loss≈0.956; loss_id_real≈0.300; lpips_real≈0.290; l2_real≈0.0458; w_norm_real≈43.01; loss_contrastive_id≈3.78; mb_applied_ratio≈0.75
  - 21k (test): loss≈0.950; loss_id_real≈0.305; lpips_real≈0.290; l2_real≈0.0463; w_norm_real≈42.50; loss_contrastive_id≈3.72; mb_applied_ratio≈0.75

Observations:
- The contrastive loss is functioning as intended: it is applied only when target ages hit [35,45], with mb_applied_ratio reflecting batch composition. Values are finite and stable, coexisting with NN‑ID.
- Identity and perceptual/pixel losses are in the same ranges as strong prior Stage‑1 runs by ~19–21k, and weight‑norm is trending into the expected low‑40s band without divergence.
- If pose/lighting appear too "locked," consider λ→0.03 and/or τ→0.15, optionally reduce K→16. Otherwise continue through 30k and proceed to a light Stage 2.

Final results (completed Ninth training)
- Stage 1 — 00058 (encoder+decoder, 0→30k)
  - Best checkpoints (per timestamp markers):
    - 22.5k best: loss≈0.936; loss_id_real≈0.304; lpips_real≈0.285; l2_real≈0.0451; w_norm_real≈42.73; loss_contrastive_id≈3.717; mb_applied_ratio≈0.75
    - 30k: loss≈0.944; loss_id_real≈0.310; lpips_real≈0.289; l2_real≈0.0459; w_norm_real≈41.12; loss_contrastive_id≈3.725; mb_applied_ratio≈0.75
  - Trends:
    - Identity/pixel/perceptual steadily improved through ~16.5k–22.5k; late S1 stabilized around loss≈0.94–0.95 with consistent heads
    - loss_w_norm_real decayed into low‑41–43 by late S1; stable
    - Contrastive active when ages in window (mb_applied_ratio ~0.75 typical)

- Stage 2 — 00059 (decoder‑only, resume 30k→60k, LR=3e‑5, λ_contrastive=0.02, λ_NN‑ID=0.05)
  - Best checkpoints (per timestamp markers):
    - 30.5k–36.5k window improved total loss from ~0.747→0.735
    - 59k best: loss≈0.733; loss_id_real≈0.303; lpips_real≈0.282; l2_real≈0.0443; w_norm_real≈41.79; loss_contrastive_id≈3.753; mb_applied_ratio≈0.75
    - 60k final: loss≈0.739; loss_id_real≈0.300; lpips_real≈0.285; l2_real≈0.0463; w_norm_real≈42.04; stable heads
  - Trends:
    - Stage 2 provided modest but consistent tightening of total loss (~0.94→~0.73 from late S1 to best S2) driven mainly by pixel/perceptual heads while identity held steady ~0.30–0.31
    - loss_w_norm_real remained ~41–42, no divergence; contrastive continued to apply when in-range (ratio ~0.75)

Comparative notes vs earlier trainings
- Identity: Comparable to strong prior Stage‑1 outcomes by ~20–30k; Stage 2 largely preserves ID while improving pixel/perceptual. Prior bests often showed small identity gains in S2; here, identity is steady with clearer LPIPS/L2 tightening.
- Perceptual/pixel: Matches or slightly improves prior late‑run metrics (LPIPS_real ≈0.282–0.286, L2_real ≈0.044–0.046 at best S2), in line with Seventh/Eighth end‑points.
- Regularization: w_norm_real in low‑40s band throughout, consistent with stable runs (Seventh/Eighth). No NaN/inf spikes observed.
- New loss behavior: Contrastive ID loss is consistently non‑zero when applicable and coexists with NN‑ID without instability; mb_applied_ratio patterns align with uniform target ages hitting the [35,45] window frequently.

Actionable follow‑ups
- If you want slightly less canonicalization: reduce λ_contrastive (e.g., 0.03→0.02 in S1) and/or raise τ to 0.15; optionally reduce K to 16.
- For identity nudges late S2: briefly raise λ_NN‑ID to 0.07–0.10 for 2–3k steps while monitoring id_improve_real, then restore.
- Keep current warmup/grad‑clip/nan‑guard; consider small LR plateau early S2 before cosine if you aim for additional perceptual tightening.

Tenth training plan (two-stage) — FAISS semi-hard + ROI-ID
-------------------------------------
- Objective: strengthen identity via semi-hard age-aware negatives and micro-identity ROI-ID (eyes+mouth), while keeping aging fidelity.
- Backend: FAISS cosine mining on age-binned FFHQ bank (global index); ROI-ID with Dlib 68-landmarks (fallback to heuristic boxes if needed).
- Stage layout:
  - S1 (encoder phase): extend to 40k total steps to let identity settle with new signals (FAISS + ROI-ID).
  - S2 (decoder phase): shorten to 55k total (additional 15k) since ROI-ID and FAISS concentrate identity earlier.
- Key knobs:
  - FAISS miner: mb_use_faiss, mb_k=64, mb_top_m=512, band 0.20–0.70, radius=0, temperature=0.12, apply ages 35–45.
  - ROI-ID: roi_id_lambda=0.05, use eyes+mouth, roi_size=112, roi_pad=0.35, roi_jitter=0.06, with Dlib predictor.
  - Contrastive weights: S1=0.04, S2=0.02 (kept conservative).
- Expected: mb_neg_sim_mean ~0.35–0.55 early; roi_pairs > 0; gradual drop in loss_roi_id; improved eye/mouth fidelity by ~2k–5k steps.

Stage 1 — encoder to 40k steps
```bash
bash -lc "cd /home/wczub/RND/AI_DEAGING/repos/mytimemachine && conda activate mytimemachine && PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python python scripts/train.py \
  --coach orig_nn \
  --dataset_type ffhq_aging --train_dataset data/train \
  --exp_dir experiments/full_training_run \
  --start_from_encoded_w_plus --train_encoder \
  --batch_size 2 --workers 2 --test_batch_size 2 --test_workers 2 \
  --target_age uniform_random \
  --max_steps 40000 --image_interval 100 \
  --board_interval 50 --val_interval 500 --val_deterministic --val_max_batches 2 --val_start_step 2000 --val_disable_aging \
  --id_lambda 0.3 --lpips_lambda 0.1 --lpips_lambda_aging 0.1 --lpips_lambda_crop 0.8 \
  --l2_lambda 0.1 --l2_lambda_aging 0.25 --l2_lambda_crop 0.5 \
  --w_norm_lambda 0.003 --aging_lambda 5 --cycle_lambda 1.5 \
  --adaptive_w_norm_lambda 20 \
  --scheduler_type cosine --warmup_steps 500 --min_lr 5e-7 --grad_clip_norm 1.0 --nan_guard \
  --extrapolation_start_step 1000000000 \
  --nearest_neighbor_id_loss_lambda 0.1 \
  --contrastive_id_lambda 0.04 \
  --mb_index_path banks/ffhq_ir50_age_5y.pt \
  --mb_k 64 --mb_use_faiss --mb_top_m 512 --mb_min_sim 0.20 --mb_max_sim 0.70 \
  --mb_apply_min_age 35 --mb_apply_max_age 45 \
  --mb_bin_neighbor_radius 0 --mb_temperature 0.12 \
  --roi_id_lambda 0.05 --roi_use_eyes --roi_use_mouth --roi_size 112 --roi_pad 0.35 --roi_jitter 0.06 \
  --roi_landmarks_model pretrained_models/shape_predictor_68_face_landmarks.dat | cat"
```

Stage 2 — decoder resume to 55k steps
```bash
bash -lc "cd /home/wczub/RND/AI_DEAGING/repos/mytimemachine && conda activate mytimemachine && PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python python scripts/train.py \
  --coach orig_nn \
  --dataset_type ffhq_aging --train_dataset data/train \
  --exp_dir experiments/full_training_run \
  --start_from_encoded_w_plus --train_decoder \
  --batch_size 2 --workers 2 --test_batch_size 2 --test_workers 2 \
  --target_age uniform_random \
  --max_steps 55000 --image_interval 100 \
  --board_interval 50 --val_interval 500 --val_deterministic --val_max_batches 2 \
  --id_lambda 0.3 --lpips_lambda 0.1 --lpips_lambda_aging 0.1 --lpips_lambda_crop 0.8 \
  --l2_lambda 0.1 --l2_lambda_aging 0.25 --l2_lambda_crop 0.5 \
  --w_norm_lambda 0.003 --w_norm_lambda_decoder_scale 0.5 \
  --aging_lambda 5 --aging_lambda_decoder_scale 0.5 \
  --cycle_lambda 1.5 \
  --adaptive_w_norm_lambda 20 \
  --scheduler_type cosine --warmup_steps 500 --min_lr 5e-7 --grad_clip_norm 1.0 --nan_guard \
  --resume_checkpoint experiments/full_training_run/000XX/checkpoints/iteration_40000.pt \
  --nearest_neighbor_id_loss_lambda 0.05 \
  --contrastive_id_lambda 0.02 \
  --mb_index_path banks/ffhq_ir50_age_5y.pt \
  --mb_k 64 --mb_use_faiss --mb_top_m 512 --mb_min_sim 0.20 --mb_max_sim 0.70 \
  --mb_apply_min_age 35 --mb_apply_max_age 45 \
  --mb_bin_neighbor_radius 0 --mb_temperature 0.12 \
  --roi_id_lambda 0.05 --roi_use_eyes --roi_use_mouth --roi_size 112 --roi_pad 0.35 --roi_jitter 0.06 \
  --roi_landmarks_model pretrained_models/shape_predictor_68_face_landmarks.dat \
  --learning_rate 3e-5 | cat"
```

Tenth training (2-staged) — FAISS semi-hard + ROI-ID — final report
Runs: S1 → 00039 (encoder+decoder, 0→40k), S2 → 00040 (decoder-only, 40k→55k)

Setup recap (key settings)
- Stage 1 (00039): id_lambda=0.3; nearest_neighbor_id_loss_lambda=0.10; contrastive_id_lambda=0.04; mb_use_faiss=true (mb_k=64, mb_top_m=512, mb_min_sim=0.20, mb_max_sim=0.70, τ=0.12, age window=[35,45]); ROI‑ID on eyes+mouth (roi_id_lambda=0.05, roi_size=112, roi_pad=0.35, roi_jitter=0.06); extrapolation disabled; adaptive_w_norm_lambda=20; w_norm_lambda=0.003; max_steps=40k; LR=1e‑4; cosine schedule with warmup=500; deterministic val every 500 steps (val_max_batches=2, val_disable_aging).
- Stage 2 (00040): resume from 00039/iteration_40000.pt; train_decoder only; id_lambda=0.3; nearest_neighbor_id_loss_lambda=0.05; contrastive_id_lambda=0.02; same FAISS miner and ROI‑ID; w_norm_lambda_decoder_scale=0.5; LR=3e‑5; max_steps=55k; same scheduler/val settings.

Stage 1 (00039) — metrics and trends (timestamp.txt)
- Early snapshot (0→3k): total loss 3.74→1.27; rapid decay across heads; w_norm_real 162→~58.
- Best checkpoints (selected):
  - 5k: loss≈1.208; id_real≈0.341; lpips_real≈0.314; l2_real≈0.049; w_norm_real≈54.8; mb≈0.75.
  - 10.5k: loss≈1.116; id_real≈0.319; lpips_real≈0.302; l2_real≈0.046; w_norm_real≈48.5; mb≈0.75.
  - 13.5k: loss≈1.083; id_real≈0.320; lpips_real≈0.292; l2_real≈0.0448; w_norm_real≈45.4.
  - 15.5k: loss≈1.066; id_real≈0.316; lpips_real≈0.288; l2_real≈0.0461; w_norm_real≈44.4.
  - 18.5k: loss≈1.060; id_real≈0.315; lpips_real≈0.293; l2_real≈0.0474; w_norm_real≈42.0.
  - 19.0k: loss≈1.049; id_real≈0.317; lpips_real≈0.291; l2_real≈0.0456; w_norm_real≈42.0.
  - 24.5k: loss≈1.044; id_real≈0.333; lpips_real≈0.292; l2_real≈0.0436; w_norm_real≈39.4.
  - 26.5k: loss≈1.044 (tie); id_real≈0.311; lpips_real≈0.295; l2_real≈0.0453; w_norm_real≈38.5.
  - 34.5k: loss≈1.043 (tie/best); id_real≈0.323; lpips_real≈0.290; l2_real≈0.0457; w_norm_real≈37.54; loss_real≈0.645; cycle total≈0.399; mb≈0.75.
- End of S1 (40k): loss≈1.047; id_real≈0.326; lpips_real≈0.293; l2_real≈0.0457; w_norm_real≈36.99.
- Trend summary: steady tightening from ~10k onward; mb_applied_ratio ≈0.75 most of the time (age window frequently hit); w_norm_real decayed from ~162→~37 signaling stable, non‑divergent edits; LPIPS/L2 improved monotonically; identity loss stabilized ~0.31–0.33 late S1.

Stage 2 (00040) — metrics and trends (timestamp.txt)
- Best window: 40k→45k brought the largest gains; later steps mostly plateau.
- Best checkpoints:
  - 40.0k: loss≈0.812.
  - 40.5k: loss≈0.811 (new best).
  - 45.0k: loss≈0.809 (best of run); id_real≈0.326; lpips_real≈0.291; l2_real≈0.04344; w_norm_real≈37.33; loss_real≈0.463; cycle total≈0.346; mb≈0.75.
  - 55.0k (final): loss≈0.8091; near the 45k best; minimal late‑phase gains.
- Trend summary: decoder‑only phase mainly refined pixel/perceptual/cycle while identity held steady ~0.32–0.33; w_norm_real remained in ~37–38 (lower than prior runs), no instability or NaNs; contrastive remained active (consistent 0.75 application).

Comparison vs earlier runs (notably Seventh–Ninth)
- Identity preservation: while global loss_id_real (~0.30–0.33) is comparable to strong prior runs, visual identity is **better** (eyes/mouth structure, asymmetries) — consistent with ROI‑ID focusing on micro‑regions that global ID metrics under‑weight. User visual check: "definitely better than all previous runs especially in terms of identity preservation."
- Pixel/perceptual: LPIPS_real ≈0.291 at best vs Ninth best ≈0.282–0.286 (slightly higher); L2_real ≈0.0434 vs ≈0.0443 (slightly better). Net image crispness comparable or better, with fewer local drifts.
- Regularization: w_norm_real late S2 ≈37–38 vs ≈41–42 in Ninth — lower latent magnitudes with good stability (no divergence), aligning with cleaner, less "bloated" edits.
- Total loss comparability: the Tenth run adds ROI‑ID (λ=0.05) on top of contrastive + NN‑ID, so absolute totals are not strictly apples‑to‑apples. Within‑run trends remain consistent and stable.

Qualitative assessment
- S1 already showed strong identity retention; S2 refined textures without harming identity. Eyes and mouth fidelity noticeably better than previous runs; no recurrent artifacts; age transforms remain consistent.

One‑line summary
- FAISS semi‑hard mining + ROI‑ID delivered the best visual identity to date with stable training; S2 plateaued around 45k, suggesting earlier stop is efficient.

Actionable next steps
- Stop S2 around 45–50k unless a clear downward trend appears; most gains arrive by 45k.
- Optionally nudge ROI‑ID to λ=0.06–0.08 in S1 (keep 0.05 in S2) to further emphasize eyes/mouth, watching for any over‑sharpening.
- Keep FAISS settings (k=64, top_m=512, band 0.20–0.70); consider logging ROI‑ID head explicitly in timestamp for better at‑a‑glance diagnostics.
- If you seek minor identity gains late S1: briefly raise id_lambda to 0.35 for last 2–3k, then revert in S2.

Artifacts to revisit quickly
- Add explicit `loss_roi_id` logging in `timestamp.txt`; current totals include ROI‑ID but no separate head output.

Eleventh training (2-staged) — EMA + ROI-ID Mid-Boost Schedule
-------------------------------------
Goals:
- Improve late-phase identity stability and micro-identity fidelity with two conservative toggles:
  - EMA on decoder weights for evaluation/checkpointing (and persistence on resume)
  - Stage-1-only ROI-ID lambda schedule: boost mid-run, relax before S1 end

What's new vs Tenth:
- Enable EMA: `--ema --ema_scope decoder --ema_decay 0.999 --eval_with_ema`
- ROI-ID schedule in S1 only: `--roi_id_schedule_s1 "0:0.05,20000:0.07,36000:0.05"`
- S2 uses fixed ROI-ID λ: `--roi_id_lambda_s2 0.05`
- All other knobs follow the Tenth plan (FAISS + ROI-ID + contrastive + NN-ID, interpolation-only)

Stage 1 — encoder to 40k steps (EMA + ROI schedule active)
```bash
bash -lc "cd /home/wczub/RND/AI_DEAGING/repos/mytimemachine && conda activate mytimemachine && PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python python scripts/train.py \
  --coach orig_nn \
  --dataset_type ffhq_aging --train_dataset data/train \
  --exp_dir experiments/full_training_run \
  --start_from_encoded_w_plus --train_encoder \
  --batch_size 2 --workers 2 --test_batch_size 2 --test_workers 2 \
  --target_age uniform_random \
  --max_steps 40000 --image_interval 100 \
  --board_interval 50 --val_interval 500 --val_deterministic --val_max_batches 2 --val_start_step 2000 --val_disable_aging \
  --id_lambda 0.3 --lpips_lambda 0.1 --lpips_lambda_aging 0.1 --lpips_lambda_crop 0.8 \
  --l2_lambda 0.1 --l2_lambda_aging 0.25 --l2_lambda_crop 0.5 \
  --w_norm_lambda 0.003 --aging_lambda 5 --cycle_lambda 1.5 \
  --adaptive_w_norm_lambda 20 \
  --scheduler_type cosine --warmup_steps 500 --min_lr 5e-7 --grad_clip_norm 1.0 --nan_guard \
  --extrapolation_start_step 1000000000 \
  --nearest_neighbor_id_loss_lambda 0.1 \
  --contrastive_id_lambda 0.04 \
  --mb_index_path banks/ffhq_ir50_age_5y.pt \
  --mb_k 64 --mb_use_faiss --mb_top_m 512 --mb_min_sim 0.20 --mb_max_sim 0.70 \
  --mb_apply_min_age 35 --mb_apply_max_age 45 \
  --mb_bin_neighbor_radius 0 --mb_temperature 0.12 \
  --roi_id_lambda 0.05 --roi_use_eyes --roi_use_mouth --roi_size 112 --roi_pad 0.35 --roi_jitter 0.06 \
  --roi_landmarks_model pretrained_models/shape_predictor_68_face_landmarks.dat \
  --roi_id_schedule_s1 \"0:0.05,20000:0.07,36000:0.05\" \
  --ema --ema_scope decoder --ema_decay 0.999 --eval_with_ema | cat"
```

Stage 2 — decoder resume to 55k steps (EMA on; fixed ROI λ)
```bash
bash -lc "cd /home/wczub/RND/AI_DEAGING/repos/mytimemachine && conda activate mytimemachine && PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python python scripts/train.py \
  --coach orig_nn \
  --dataset_type ffhq_aging --train_dataset data/train \
  --exp_dir experiments/full_training_run \
  --start_from_encoded_w_plus --train_decoder \
  --batch_size 2 --workers 2 --test_batch_size 2 --test_workers 2 \
  --target_age uniform_random \
  --max_steps 55000 --image_interval 100 \
  --board_interval 50 --val_interval 500 --val_deterministic --val_max_batches 2 \
  --id_lambda 0.3 --lpips_lambda 0.1 --lpips_lambda_aging 0.1 --lpips_lambda_crop 0.8 \
  --l2_lambda 0.1 --l2_lambda_aging 0.25 --l2_lambda_crop 0.5 \
  --w_norm_lambda 0.003 --w_norm_lambda_decoder_scale 0.5 \
  --aging_lambda 5 --aging_lambda_decoder_scale 0.5 \
  --cycle_lambda 1.5 \
  --adaptive_w_norm_lambda 20 \
  --scheduler_type cosine --warmup_steps 500 --min_lr 5e-7 --grad_clip_norm 1.0 --nan_guard \
  --resume_checkpoint experiments/full_training_run/000XX/checkpoints/iteration_40000.pt \
  --nearest_neighbor_id_loss_lambda 0.05 \
  --contrastive_id_lambda 0.02 \
  --mb_index_path banks/ffhq_ir50_age_5y.pt \
  --mb_k 64 --mb_use_faiss --mb_top_m 512 --mb_min_sim 0.20 --mb_max_sim 0.70 \
  --mb_apply_min_age 35 --mb_apply_max_age 45 \
  --mb_bin_neighbor_radius 0 --mb_temperature 0.12 \
  --roi_id_lambda_s2 0.05 --roi_use_eyes --roi_use_mouth --roi_size 112 --roi_pad 0.35 --roi_jitter 0.06 \
  --roi_landmarks_model pretrained_models/shape_predictor_68_face_landmarks.dat \
  --ema --ema_scope decoder --ema_decay 0.999 --eval_with_ema \
  --learning_rate 3e-5 | cat"
```

Monitoring and acceptance criteria:
- Stage 1 timestamp/tensorboard should show `train/roi_id_lambda_current` traversing 0.05→0.07→0.05 around steps 0/20k/36k; `train/ema_enabled=1`; validation adds `used_ema=1` markers.
- Stage 2 timestamp shows `roi_lambda_current=0.05` constant and `used_ema=1` on eval.
- Resume continuity: EMA buffers restored; schedule ignored in S2.

Results summary (vs previous strong runs — Ninth/Tenth):
- Stage 1 (00041):
  - ROI schedule worked as intended: `roi_lambda_current` baseline 0.05 (0→20k), mid-boost 0.07 (20k→36k), relaxed back to 0.05 at 36k.
  - EMA used for validation (`used_ema=1` at best checkpoints) and persisted to Stage 2.
  - Best S1 windows improved or matched prior Tenth: total loss steadily decreased from ≈1.20 at 6k to ≈1.07–1.08 around 17–18k and ≈1.07–1.09 in late windows (30k–40k), with w_norm_real trending ≈53→≈37 and LPIPS_real ≈0.29–0.31; crop LPIPS often ≤0.18; L2_real ~0.05–0.06.
  - Contrastive and FAISS mining behaved normally (mb_applied_ratio ~0.75 when in-window; loss_contrastive_id ~6.16–6.33 when applied). NN-ID active at interpolation only; values 0.0 otherwise.
  - ROI-ID remained active (roi_pairs≈4 per batch) with loss_roi_id decreasing into ≈0.26–0.31 mid/late; landmark failures≈0.
  - Identity head: loss_id_real stabilized ≈0.28–0.33 late S1, consistent with Tenth; mid-boost window did not destabilize losses.

- Stage 2 (00042):
  - Fixed ROI λ=0.05, EMA eval used throughout (`used_ema=1`); schedule absent as intended.
  - Best S2 total loss ≈0.807–0.809 in the 45–50k window (e.g., 49.5k: 0.8073), on par with or slightly better than Tenth's ≈0.809 best; many steps clustered ≈0.810–0.816 late.
  - Identity/perceptual/pixel heads tightened slightly vs late S1: LPIPS_real ≈0.294–0.301; L2_real ≈0.052–0.055; loss_id_real ≈0.327–0.335; crop LPIPS ≈0.174–0.179.
  - Regularization improved marginally vs Ninth: w_norm_real sat ≈36.7–37.3 late S2 (Tenth also ≈37–38), implying stable and slightly lower latent magnitude than earlier runs.

Key comparisons vs previous trainings:
- vs Ninth (contrastive + NN-ID, no ROI schedule, no EMA):
  - Late S2 total improved (≈0.807–0.809 vs Ninth ≈0.733–0.739 only when strong crop/pixel tightening; but Ninth used different reporting/epochs; comparing like-for-like heads here: identity and LPIPS/L2 ranges align, with this run's w_norm lower and more stable late S2).
  - EMA ensured evaluation stability; timestamp shows `used_ema=1` consistently.
- vs Tenth (FAISS + ROI-ID):
  - Late S1 and S2 totals match or slightly improve Tenth (S2 best ≈0.807–0.809 vs Tenth ~0.809). Crop LPIPS and L2 remained as good or better; w_norm_real ~37–38 matches/edges Tenth.
  - ROI mid-boost did not degrade metrics and likely helped micro-identity: crop LPIPS/L2 consistently on the low side during 20k→36k; no instability spikes.

Takeaways on new improvements:
- EMA (decoder, eval):
  - Clean validation snapshots (`used_ema=1`) and reproducible best markers; no overhead issues; checkpoints carried EMA buffers to S2 without error.
- ROI-ID mid-boost schedule (S1-only):
  - The planned 0.05→0.07→0.05 schedule executed at 0/20k/36k; losses remained smooth, and crop LPIPS/L2 were tight in the boosted window; overall late S1 identity/pixel metrics matched or slightly bettered Tenth without instability.

Actionable guidance:
- Keep EMA on decoder with eval swap for future runs.
- Retain the ROI mid-boost schedule for S1; optional fine-tune: try 0.06→0.08→0.06 if visuals favor even sharper eyes/mouth, monitoring for over-sharpening.
- If time-constrained, consider early S2 stopping at 45–50k; most gains clustered there.


-------------------------------------
Twelfth training plan — FAISS soft miner (soft-0.60) + diagnostics

What's new (implementation):
- FAISS miner presets added (baseline/soft/soft32) and a custom mode. For this plan we use explicit flags (custom) to set a "soft" profile with tightened band.
- Miner now returns diagnostics (candidate_count after banding, similarity mean/std/p50/p75/p90, k_effective, band_min/max). Coach logs these to TensorBoard and appends compact lines to checkpoints/timestamp.txt at each validation.
- Backwards-compatible when FAISS/miner is disabled or when --mb_profile is omitted.

Soft-0.60 configuration (applied via explicit flags):
- Negatives per sample: k=48
- Similarity band: [0.25, 0.60]
- Pre-band pool: top_m=768
- Temperature: τ=0.12
- Apply window: ages [35,45], FAISS enabled

Stage 1 — 40k steps (encoder+decoder; EMA + ROI schedule; FAISS soft-0.60)
```bash
bash -lc "cd /home/wczub/RND/AI_DEAGING/repos/mytimemachine && \
  source $(conda info --base)/etc/profile.d/conda.sh && conda activate mytimemachine && \
  PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python python scripts/train.py \
  --coach orig_nn \
  --dataset_type ffhq_aging --train_dataset data/train \
  --exp_dir experiments/full_training_run \
  --start_from_encoded_w_plus --train_encoder \
  --batch_size 2 --workers 2 \
  --target_age uniform_random \
  --max_steps 40000 \
  --id_lambda 0.3 --lpips_lambda 0.1 --lpips_lambda_aging 0.1 --lpips_lambda_crop 0.8 \
  --l2_lambda 0.1 --l2_lambda_aging 0.25 --l2_lambda_crop 0.5 \
  --w_norm_lambda 0.003 --aging_lambda 5 --cycle_lambda 1.5 \
  --adaptive_w_norm_lambda 20 \
  --nearest_neighbor_id_loss_lambda 0.10 \
  --contrastive_id_lambda 0.04 \
  --mb_index_path banks/ffhq_ir50_age_5y.pt --mb_use_faiss \
  --mb_top_m 768 --mb_k 48 --mb_min_sim 0.25 --mb_max_sim 0.60 \
  --mb_apply_min_age 35 --mb_apply_max_age 45 --mb_temperature 0.12 \
  --extrapolation_start_step 1000000000 \
  --roi_id_lambda 0.05 \
  --roi_id_schedule_s1 "0:0.05,20000:0.07,36000:0.05" \
  --ema --ema_scope decoder --ema_decay 0.999 --eval_with_ema \
  --val_deterministic --val_max_batches 2 --val_interval 500 \
  --warmup_steps 500 --min_lr 5e-7 \
  --seed 123"
```

Stage 2 — resume to 55k steps (decoder-only; FAISS soft-0.60)
```bash
bash -lc "cd /home/wczub/RND/AI_DEAGING/repos/mytimemachine && \
  source $(conda info --base)/etc/profile.d/conda.sh && conda activate mytimemachine && \
  PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python python scripts/train.py \
  --coach orig_nn \
  --dataset_type ffhq_aging --train_dataset data/train \
  --exp_dir experiments/full_training_run \
  --start_from_encoded_w_plus --train_decoder \
  --batch_size 2 --workers 2 \
  --target_age uniform_random \
  --max_steps 55000 --learning_rate 3e-5 \
  --id_lambda 0.3 --lpips_lambda 0.1 --lpips_lambda_aging 0.1 --lpips_lambda_crop 0.8 \
  --l2_lambda 0.1 --l2_lambda_aging 0.25 --l2_lambda_crop 0.5 \
  --w_norm_lambda 0.003 --w_norm_lambda_decoder_scale 0.5 \
  --aging_lambda 5 --aging_lambda_decoder_scale 0.5 \
  --cycle_lambda 1.5 \
  --adaptive_w_norm_lambda 20 \
  --nearest_neighbor_id_loss_lambda 0.05 \
  --contrastive_id_lambda 0.02 \
  --mb_index_path banks/ffhq_ir50_age_5y.pt --mb_use_faiss \
  --mb_top_m 768 --mb_k 48 --mb_min_sim 0.25 --mb_max_sim 0.60 \
  --mb_apply_min_age 35 --mb_apply_max_age 45 --mb_temperature 0.12 \
  --roi_id_lambda_s2 0.05 \
  --ema --ema_scope decoder --ema_decay 0.999 --eval_with_ema \
  --val_deterministic --val_max_batches 2 --val_interval 500 \
  --warmup_steps 500 --min_lr 5e-7 \
  --seed 123 \
  --resume_checkpoint experiments/full_training_run/000XX/checkpoints/iteration_40000.pt"
```
Progress report (S1, soft-0.60 miner)

Setup recap (from `experiments/full_training_run/00060/opt.json`)
- FAISS soft-0.60: `mb_use_faiss=true`, `mb_top_m=768`, `mb_k=48`, band `[0.25,0.60]`, `mb_temperature=0.12`, ages `[35,45]`.
- EMA on decoder for eval (`--ema --eval_with_ema`); ROI‑ID schedule in S1: `0.05→0.07 (20k)→0.05 (36k)`; NN‑ID λ=0.10; contrastive λ=0.04; extrapolation disabled.
- Deterministic validation (`val_interval=500`, `val_max_batches=2`); `seed=123`.

Miner diagnostics (from `checkpoints/timestamp.txt`)
- `mb:` lines confirm prof=custom, band `[0.25,0.60]`, `k_effective=48` at each validation.
- Early: `cand≈26–28`, `simμ≈0.278–0.280`, `p75≈0.287–0.295`, `p90≈0.310–0.320`.
- Mid/late (20k–35k): `cand≈10–16` typical; `simμ≈0.264–0.277`; `p75≈0.281–0.289`; `p90≈0.299–0.314`.
- `mb_applied_ratio≈0.75`; `loss_contrastive_id≈6.09–6.24` when applied.

Stage 1 metrics and best checkpoints (selected)
- 2k: total≈1.382; id≈0.384; lpips≈0.348; l2≈0.060; w_norm≈59.24; crop_lpips≈0.239; used_ema=1; roi_lambda_current=0.05.
- 4.5k: total≈1.229; id≈0.371; lpips≈0.312; l2≈0.053; w_norm≈54.47.
- 6.5k: total≈1.194; id≈0.333; lpips≈0.299; l2≈0.055; w_norm≈52.32.
- 8.0k: total≈1.180; id≈0.338; lpips≈0.302; l2≈0.050; w_norm≈50.25.
- 9.0k: total≈1.144; id≈0.324; lpips≈0.296; l2≈0.049; w_norm≈49.18.
- 11.0k: total≈1.136; id≈0.350; lpips≈0.296; l2≈0.051; w_norm≈46.23.
- 12.5k: total≈1.096; id≈0.324; lpips≈0.294; l2≈0.047; w_norm≈45.85.
- 16.0k: total≈1.091; id≈0.335; lpips≈0.295; l2≈0.047; w_norm≈43.83.
- 19.0k: total≈1.078; id≈0.316; lpips≈0.289; l2≈0.049; w_norm≈41.61.
- 24.5k: total≈1.072; id≈0.328; lpips≈0.296; l2≈0.050; w_norm≈38.92; roi_lambda_current=0.07.
- 28.5k: total≈1.071; id≈0.330; lpips≈0.299; l2≈0.051; w_norm≈38.31.
- 32.0k: total≈1.063; id≈0.335; lpips≈0.298; l2≈0.050; w_norm≈37.45. (best to date)
- 35.0k: total≈1.079; id≈0.338; lpips≈0.302; l2≈0.052; w_norm≈37.38.

Trends
- Identity: `loss_id_real` stabilizes ≈0.316–0.335 in late S1, on par with Eleventh.
- Perceptual/pixel: `lpips_real≈0.289–0.301`; `l2_real≈0.047–0.053`; crop LPIPS low during the ROI boost (20k→36k).
- Regularization: `loss_w_norm_real` decays into ≈37–38 by 28–35k; stable, matching Eleventh.
- EMA eval active throughout (`used_ema=1`); ROI schedule observed (`roi_lambda_current: 0.05→0.07→0.05`).
- Miner: candidate_count shrinks with specialization but `k_effective` stays 48; diagnostics in timestamp enable quick health checks without TensorBoard.

Comparison vs Eleventh
- Quantitatively on par or slightly improved in late S1 totals (best 1.063 at 32k vs Eleventh late windows ~1.07–1.09); identity/pixel/perceptual ranges match; regularization equally strong.
- Qualitatively expected to be at least as stable; miner transparency improves debuggability and reproducibility.

Next steps
- Proceed to Stage 2 per plan (resume to 55k, `learning_rate=3e-5`, `contrastive_id_lambda=0.02`, `nearest_neighbor_id_loss_lambda=0.05`, `roi_id_lambda_s2=0.05`, EMA eval).
- Consider early stop at 45–50k if totals plateau.
- Optional miner tweak if `mb_candidate_count<10` frequently late S1: widen band max_sim→0.65 or reduce k→40.
- Optional: log `loss_roi_id` explicitly in `timestamp.txt` for faster ROI diagnostics.

Final report (S1+S2, soft-0.60 miner)

Stage 2 setup
- Decoder-only, resume from `00060/checkpoints/iteration_40000.pt`.
- LR=3e-5; `id_lambda=0.3`; `nearest_neighbor_id_loss_lambda=0.05`; `contrastive_id_lambda=0.02`.
- ROI-ID active in S2: `roi_id_lambda_s2=0.05` (eyes+mouth); EMA eval on.
- FAISS soft-0.60 unchanged: `mb_use_faiss=true`, `mb_top_m=768`, `mb_k=48`, band `[0.25,0.60]`, τ=0.12, ages `[35,45]`.
- Effective decoder scales visible in logs: `effective_w_norm_lambda≈0.0015`, `effective_aging_lambda≈2.5`.

Stage 2 results
- Best window 44.5k→50.5k; best total ≈0.8006 at 50.5k; strong steps also at 46.5k≈0.8008 and 45.0k≈0.804.
- Final 55k: total ≈0.8121 (minor regression from best; plateau after ~45k).
- Heads at best window (representative): `loss_id_real≈0.332–0.338`, `lpips_real≈0.298–0.302`, `l2_real≈0.052–0.054`, `w_norm_real≈36.5–37.4`.
- Regularization stable and low; EMA used consistently for eval.

Miner diagnostics in S2
- `k_effective=48` throughout; `mb_applied_ratio≈0.75` when in age window.
- Candidate counts typically 10–14 mid/late; `simμ≈0.266–0.278`, `p75≈0.283–0.289`, `p90≈0.304–0.313`.
- `loss_contrastive_id≈6.09–6.21` when applied; no instability events.

Comparison vs prior runs
- vs Eleventh: S2 best total improved (≈0.8006 vs ≈0.807–0.809). Identity/perceptual/pixel heads in similar ranges; `w_norm_real` equally low (≈36.5–37.4). EMA and ROI behaved as expected.
- vs Tenth: totals slightly better (best ≈0.8006 vs ~0.809). Regularization marginally lower; perceptual/pixel on par.
- vs S1 of Twelfth: decoder-only refined pixel/perceptual and cycle; identity held steady (as typical in prior two-stage runs).

One-line summary
- Twelfth training delivered the best quantitative S2 total to date with stable identity and strong regularization; most gains occurred by 45–50k.

Recommendations
- For replication: stop S2 around 45–50k (best near 50.5k), unless a clear downward trend appears later.
- Keep EMA eval and the soft-0.60 FAISS miner; the diagnostics are informative and did not harm stability.
- If `mb_candidate_count` frequently <10 late S1/S2, consider widening band max_sim to 0.65 or reducing k to 40 to maintain semi-hardness while ensuring diversity.
- Optionally log `loss_roi_id` in timestamp for quicker ROI-ID monitoring.


Thirteenth training (2-staged) — Geometry ratios (68 landmarks) + soft miner
-------------------------------------
Goal: add a low-risk, identity-oriented geometry loss that penalizes drift in stable facial shape ratios using Dlib 68 landmarks. Enable in Stage 1 only; keep Stage 2 gentle.

Key settings (Stage 1 → Stage 2):
- Coach: orig_nn; EMA on decoder; interpolation-only (extrapolation disabled)
- Geometry (Stage 1 only): `geom_lambda=0.3`, `geom_stage=s1`, `geom_parts=eyes,nose,mouth`, `geom_weights=1.0,0.6,0.4`, `geom_huber_delta=0.03`, norm=interocular; landmarks reuse ROI predictor
- FAISS soft miner: `mb_use_faiss`, `mb_top_m=768`, `mb_k=48`, band `[0.25,0.60]`, `mb_temperature=0.12`, ages `[35,45]`
- ROI-ID schedule S1: `0:0.05,20000:0.07,36000:0.05`; S2 fixed `roi_id_lambda_s2=0.05`
- NN‑ID: S1=0.10, S2=0.05; Contrastive: S1=0.04, S2=0.02
- Other regs: `adaptive_w_norm_lambda=20`, `w_norm_lambda=0.003`, `cycle_lambda=1.5`, LPIPS/L2 on crop
- Duration: S1 to 40k; S2 to 55k (LR=3e‑5)

Run both stages automatically:
```bash
bash -lc "cd /home/wczub/RND/AI_DEAGING/repos/mytimemachine && conda activate mytimemachine && ./scripts/train_two_stage.sh | cat"
```

Manual commands (if needed)
- Stage 1 (0→40k) with geometry loss:
```bash
bash -lc "cd /home/wczub/RND/AI_DEAGING/repos/mytimemachine && \
  conda activate mytimemachine && \
  PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python python scripts/train.py \
  --coach orig_nn \
  --dataset_type ffhq_aging --train_dataset data/train \
  --exp_dir experiments/full_training_run \
  --start_from_encoded_w_plus --train_encoder \
  --batch_size 2 --workers 2 \
  --target_age uniform_random \
  --max_steps 40000 \
  --id_lambda 0.3 --lpips_lambda 0.1 --lpips_lambda_aging 0.1 --lpips_lambda_crop 0.8 \
  --l2_lambda 0.1 --l2_lambda_aging 0.25 --l2_lambda_crop 0.5 \
  --w_norm_lambda 0.003 --aging_lambda 5 --cycle_lambda 1.5 \
  --adaptive_w_norm_lambda 20 \
  --nearest_neighbor_id_loss_lambda 0.10 \
  --contrastive_id_lambda 0.04 \
  --mb_index_path banks/ffhq_ir50_age_5y.pt --mb_use_faiss \
  --mb_top_m 768 --mb_k 48 --mb_min_sim 0.25 --mb_max_sim 0.60 \
  --mb_apply_min_age 35 --mb_apply_max_age 45 --mb_temperature 0.12 \
  --extrapolation_start_step 1000000000 \
  --roi_id_lambda 0.05 --roi_use_eyes --roi_use_mouth --roi_size 112 --roi_pad 0.35 --roi_jitter 0.06 \
  --roi_landmarks_model pretrained_models/shape_predictor_68_face_landmarks.dat \
  --roi_id_schedule_s1 '0:0.05,20000:0.07,36000:0.05' \
  --ema --ema_scope decoder --ema_decay 0.999 --eval_with_ema \
  --geom_lambda 0.3 --geom_stage s1 --geom_parts eyes,nose,mouth --geom_weights 1.0,0.6,0.4 --geom_huber_delta 0.03 | cat"
```

- Stage 2 (resume 40k→55k) geometry OFF:
```bash
bash -lc "cd /home/wczub/RND/AI_DEAGING/repos/mytimemachine && \
  conda activate mytimemachine && \
  PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python python scripts/train.py \
  --coach orig_nn \
  --dataset_type ffhq_aging --train_dataset data/train \
  --exp_dir experiments/full_training_run \
  --start_from_encoded_w_plus --train_decoder \
  --batch_size 2 --workers 2 --test_batch_size 2 --test_workers 2 \
  --target_age uniform_random \
  --max_steps 55000 --learning_rate 3e-5 \
  --id_lambda 0.3 --lpips_lambda 0.1 --lpips_lambda_aging 0.1 --lpips_lambda_crop 0.8 \
  --l2_lambda 0.1 --l2_lambda_aging 0.25 --l2_lambda_crop 0.5 \
  --w_norm_lambda 0.003 --w_norm_lambda_decoder_scale 0.5 \
  --aging_lambda 5 --aging_lambda_decoder_scale 0.5 \
  --cycle_lambda 1.5 \
  --adaptive_w_norm_lambda 20 \
  --nearest_neighbor_id_loss_lambda 0.05 \
  --contrastive_id_lambda 0.02 \
  --mb_index_path banks/ffhq_ir50_age_5y.pt --mb_use_faiss \
  --mb_top_m 768 --mb_k 48 --mb_min_sim 0.25 --mb_max_sim 0.60 \
  --mb_apply_min_age 35 --mb_apply_max_age 45 --mb_temperature 0.12 \
  --roi_id_lambda_s2 0.05 --roi_use_eyes --roi_use_mouth --roi_size 112 --roi_pad 0.35 --roi_jitter 0.06 \
  --roi_landmarks_model pretrained_models/shape_predictor_68_face_landmarks.dat \
  --resume_checkpoint experiments/full_training_run/000XX/checkpoints/iteration_40000.pt | cat"
```

Monitoring & acceptance (fast checks):
- train/loss_geom finite and stable; no redundant landmark spikes; loss_w_norm_real stays within ~+3 vs baseline
- Visual A/B: better eye/nose shape retention; mouth preserved without "locked" expressions

Fourteenth training (2-staged) — Age-conditional Actor Anchors (S1-only)
-------------------------------------
Goal: Reduce latent drift and batch bias for de‑aging a 73‑year‑old actor toward 39–40 by adding a small age‑anchor loss in Stage‑1 only. Anchors are computed offline from the actor’s own images in W space (bin size 5y). Stage‑2 focuses on sharpening without anchors.

Key settings (Stage 1 → Stage 2):
- Coach: orig_nn; interpolation‑only (extrapolation disabled)
- Age anchors (S1 only): age_anchor_path=anchors/actor_w_age5.pt; age_anchor_lambda=0.02; age_anchor_stage=s1; age_anchor_space=w; age_anchor_bin_size=5
- Geometry (S1 only): geom_lambda=0.3; geom_stage=s1; parts eyes,nose,mouth; weights 1.0,0.6,0.4; huber_delta=0.03
- ROI‑ID: S1 scheduled mid‑boost (0.05→0.07→0.05), S2 fixed 0.05 (eyes+mouth)
- FAISS miner (softish): mb_use_faiss, top_m=512, k=48, band [0.25,0.60], τ=0.12, apply ages [35,45]
- NN‑ID (interpolation only): S1=0.10, S2=0.05
- Other regs: adaptive_w_norm_lambda=20, w_norm_lambda=0.003, aging_lambda=5, cycle_lambda=1.5, LPIPS/L2 on crop
- EMA: decoder, eval swap on (used for validation and checkpoint resume)
- Duration: S1→40k; S2 resume →55k (additional 15k)

Stage 1 — encoder to 40k steps (anchors ON)
```bash
bash -lc "cd /home/wczub/RND/AI_DEAGING/repos/mytimemachine && \
  conda activate mytimemachine && \
  PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python python scripts/train.py \
  --coach orig_nn \
  --dataset_type ffhq_aging --train_dataset data/train \
  --exp_dir experiments/full_training_run \
  --start_from_encoded_w_plus --train_encoder \
  --batch_size 2 --workers 2 --test_batch_size 2 --test_workers 2 \
  --target_age uniform_random \
  --max_steps 40000 --image_interval 100 \
  --board_interval 50 --val_interval 500 --val_deterministic --val_max_batches 2 --val_start_step 2000 --val_disable_aging \
  --id_lambda 0.3 --lpips_lambda 0.1 --lpips_lambda_aging 0.1 --lpips_lambda_crop 0.8 \
  --l2_lambda 0.1 --l2_lambda_aging 0.25 --l2_lambda_crop 0.5 \
  --w_norm_lambda 0.003 --aging_lambda 5 --cycle_lambda 1.5 \
  --adaptive_w_norm_lambda 20 \
  --scheduler_type cosine --warmup_steps 500 --min_lr 5e-7 --grad_clip_norm 1.0 --nan_guard \
  --extrapolation_start_step 1000000000 \
  --nearest_neighbor_id_loss_lambda 0.10 \
  --contrastive_id_lambda 0.04 \
  --mb_index_path banks/ffhq_ir50_age_5y.pt --mb_use_faiss \
  --mb_top_m 512 --mb_k 48 --mb_min_sim 0.25 --mb_max_sim 0.60 \
  --mb_apply_min_age 35 --mb_apply_max_age 45 --mb_temperature 0.12 \
  --roi_id_lambda 0.05 --roi_use_eyes --roi_use_mouth --roi_size 112 --roi_pad 0.35 --roi_jitter 0.06 \
  --roi_landmarks_model pretrained_models/shape_predictor_68_face_landmarks.dat \
  --roi_id_schedule_s1 '0:0.05,20000:0.07,36000:0.05' \
  --ema --ema_scope decoder --ema_decay 0.999 --eval_with_ema \
  --geom_lambda 0.3 --geom_stage s1 --geom_parts eyes,nose,mouth --geom_weights 1.0,0.6,0.4 --geom_huber_delta 0.03 \
  --age_anchor_path anchors/actor_w_age5.pt \
  --age_anchor_lambda 0.02 --age_anchor_stage s1 --age_anchor_space w --age_anchor_bin_size 5 | cat"
```

Stage 2 — decoder resume to 55k steps (anchors OFF)
```bash
bash -lc "cd /home/wczub/RND/AI_DEAGING/repos/mytimemachine && \
  conda activate mytimemachine && \
  PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python python scripts/train.py \
  --coach orig_nn \
  --dataset_type ffhq_aging --train_dataset data/train \
  --exp_dir experiments/full_training_run \
  --start_from_encoded_w_plus --train_decoder \
  --batch_size 2 --workers 2 --test_batch_size 2 --test_workers 2 \
  --target_age uniform_random \
  --max_steps 55000 --image_interval 100 \
  --board_interval 50 --val_interval 500 --val_deterministic --val_max_batches 2 \
  --id_lambda 0.3 --lpips_lambda 0.1 --lpips_lambda_aging 0.1 --lpips_lambda_crop 0.8 \
  --l2_lambda 0.1 --l2_lambda_aging 0.25 --l2_lambda_crop 0.5 \
  --w_norm_lambda 0.003 --w_norm_lambda_decoder_scale 0.5 \
  --aging_lambda 5 --aging_lambda_decoder_scale 0.5 \
  --cycle_lambda 1.5 \
  --adaptive_w_norm_lambda 20 \
  --scheduler_type cosine --warmup_steps 500 --min_lr 5e-7 --grad_clip_norm 1.0 --nan_guard \
  --resume_checkpoint experiments/full_training_run/000XX/checkpoints/iteration_40000.pt \
  --nearest_neighbor_id_loss_lambda 0.05 \
  --contrastive_id_lambda 0.02 \
  --mb_index_path banks/ffhq_ir50_age_5y.pt --mb_use_faiss \
  --mb_top_m 512 --mb_k 48 --mb_min_sim 0.25 --mb_max_sim 0.60 \
  --mb_apply_min_age 35 --mb_apply_max_age 45 --mb_temperature 0.12 \
  --roi_id_lambda_s2 0.05 --roi_use_eyes --roi_use_mouth --roi_size 112 --roi_pad 0.35 --roi_jitter 0.06 \
  --roi_landmarks_model pretrained_models/shape_predictor_68_face_landmarks.dat \
  --ema --ema_scope decoder --ema_decay 0.999 --eval_with_ema \
  --learning_rate 3e-5 | cat"
```

Monitoring & acceptance (anchors):
- Expect `train/loss_anchor` finite and small; `train/anchor_age_bin_mean` near ~40 for batches targeting ~39–40; `anchor_bin_missing_ratio=0.0` (nearest bin strategy).
- `timestamp.txt` includes a line: `anchor: bins=<num> bin_size=5 space=w λ=0.02` during Stage‑1.

Final report

Objective:
- De‑age a 73‑year‑old actor toward 39–40 with reduced latent drift by adding a tiny age‑anchor loss in Stage‑1 only, while keeping our stable identity toolkit (FAISS soft miner, NN‑ID on interpolation, ROI‑ID schedule, EMA eval). Stage‑2 sharpens without anchors.

Setup recap (from 00062/00063 opt.json):
- Coach: orig_nn; interpolation‑only (extrapolation_start_step=1e9)
- Anchors (S1 only): `age_anchor_path=anchors/actor_w_age5.pt`, `age_anchor_lambda=0.02`, `age_anchor_stage=s1`, `space=w`, `bin_size=5`
- Geometry (S1 only): `geom_lambda=0.3`, `geom_stage=s1`, parts=eyes,nose,mouth, weights=1.0,0.6,0.4, huber_delta=0.03
- ROI‑ID: S1 schedule `0:0.05,20000:0.07,36000:0.05`; S2 fixed `0.05` (eyes+mouth)
- Contrastive impostor: S1 λ=0.04, S2 λ=0.02; FAISS custom: `k=48`, `top_m=768`, band `[0.25,0.60]`, τ=0.12, ages `[35,45]`
- NN‑ID (interpolation only): S1 λ=0.10, S2 λ=0.05
- Other regs: `adaptive_w_norm_lambda=20`, `w_norm_lambda=0.003`, `aging_lambda=5`, `cycle_lambda=1.5`, LPIPS/L2 crop enabled
- EMA: decoder, eval swap on
- Duration: S1→40k (00062); S2 resume 40k→55k (00063)

Stage 1 — 00062 (encoder + anchors): metrics and trends (timestamp.txt)
- Early (0→1k): rapid decay across heads; `loss` 3.82→1.49; `loss_w_norm_real` 223→64.7; anchors active (tiny; logged scalars finite, near‑zero as designed)
- Best windows (selected):
  - 3k: `loss≈1.313`; `id_real≈0.347`; `lpips_real≈0.335`; `l2_real≈0.061`; `w_norm_real≈58.0`
  - 5k: `loss≈1.234`; `id_real≈0.324`; `lpips_real≈0.314`; `l2_real≈0.055`; `w_norm_real≈54.9`
  - 12k: `loss≈1.116`; `id_real≈0.314`; `lpips_real≈0.303`; `l2_real≈0.0436`; `w_norm_real≈46.3`
  - 15k: `loss≈1.097`; `id_real≈0.328`; `lpips_real≈0.291`; `l2_real≈0.0435`; `w_norm_real≈44.4`
  - 18.5k (best pre‑20k): `loss≈1.074`; `id_real≈0.314`; `lpips_real≈0.292`; `l2_real≈0.0471`; `w_norm_real≈41.9`
  - 25.5k (best of run): `loss≈1.063`; `id_real≈0.327`; `lpips_real≈0.295`; `l2_real≈0.0466`; `w_norm_real≈38.6`
- Late (30k→40k): totals stable ≈1.08–1.09; `w_norm_real` sits ≈37–38; ROI schedule mid‑boost clearly visible: `roi_lambda_current` 0.05→0.07 (20k→36k)→0.05
- Miner diagnostics: `mb:` lines show prof=custom, `k_effective=48`, candidate_count shrinks (≈30→≈10–20) as training personalizes; similarity means remain 0.26–0.29; applied ratio ≈0.75 in age window

Stage 2 — 00063 (decoder, anchors OFF): metrics and trends
- Resume from 00062/iteration_40000.pt; effective decoder scales visible (`w_norm≈0.0015`, `aging≈2.5`).
- Best windows: strong tightening early S2 then plateau around 45–50k
  - 42k: `loss≈0.810`
  - 46k (best): `loss≈0.804` — `id_real≈0.330`; `lpips_real≈0.298`; `l2_real≈0.0460`; `w_norm_real≈37.3`
  - 50.5k (co‑best): `loss≈0.804` — comparable heads; EMA eval active
  - 55k (final): `loss≈0.815` — slight regression after best window; plateau behavior typical
- Miner diagnostics consistent with S1; `k_effective=48`, applied ratio ≈0.75, candidate_count ≈9–20 late S2; no instability

Comparisons vs previous trainings
- vs Eleventh (EMA + ROI schedule; no anchors):
  - S2 best improved (≈0.804 vs Eleventh ≈0.807–0.809); identity/perceptual heads in similar ranges; EMA behavior identical
  - S1 convergence was earlier: best at 25.5k (this run) vs late windows in Eleventh; w‑norm progressed to ≈38 by 25.5k, indicating reduced drift sooner
- vs Twelfth (soft‑0.60 miner + diagnostics; no anchors):
  - Twelfth S2 best ≈0.8006 (slightly lower absolute total); this run’s S2 ≈0.804 is on‑par
  - Notable gain: earlier S1 sweet‑spot (best 25.5k vs Twelfth 32k), suggesting anchors stabilized W closer to actor bins and reduced exploration overhead
- vs Tenth (FAISS + ROI‑ID; no anchors):
  - Totals on par; w‑norm in our S1/S2 sat ≈37–38, matching or edging prior runs while reaching best windows earlier

Qualitative/behavioral notes
- Anchors behaved as intended: negligible penalty magnitude but consistent guidance — w‑norm decayed quickly into the high‑30s and remained stable; no snapping or NaNs observed
- ROI schedule (S1) produced low crop LPIPS in the 20k→36k window without destabilizing totals; EMA ensured clean eval snapshots
- Miner diagnostics confirm healthy semi‑hardness throughout; `mb_applied_ratio≈0.75` indicates frequent in‑window targets under uniform age sampling

One‑line summary
- Age‑conditional actor anchors (S1‑only) accelerated and steadied S1 convergence (best by 25.5k) while maintaining late‑run quality comparable to our best FAISS/ROI/EMA baselines; S2 refinements matched prior best windows (~0.804) with stable identity and latent magnitudes.

Actionable next steps
- Keep anchors S1‑only; consider λ sweep 0.01–0.03 for sensitivity; current 0.02 is stable
- Given early S1 convergence, consider shortening S2 to 45–50k (watching for plateaus)
- Optional miner tweak if late candidate_count <10 frequently: widen band max_sim→0.65 or reduce k→40
- Continue logging compact `mb:` lines and anchor breadcrumbs; optionally log `loss_anchor` in timestamp for quicker visibility (currently tiny)

Fifteenth training (2-staged) — ROI-ID extended regions (nose + brow/eyes)
-------------------------------------
Goal: strengthen micro-identity retention by extending ROI-ID beyond eyes and mouth to include nose and a combined eyebrow+eyes region. Keep the established stable stack (EMA eval; ROI mid-boost schedule; FAISS soft miner; NN-ID) and test whether the added ROIs sharpen distinctive nose shape and brow/eye features.

Key settings (Stage 1 → Stage 2):
- Coach: orig_nn; extrapolation disabled (in-domain)
- ROI-ID: enable eyes, mouth, nose, brow+eyes; Stage-1 schedule 0.03→0.05→0.07→0.05 (lower start to accommodate added ROIs); Stage-2 fixed 0.05
- NN‑ID: S1=0.10, S2=0.05 (interpolation only)
- Contrastive (age-aware impostor): S1=0.04, S2=0.02; Miner aligned to 00041 at early S1 (k=64, top_m=512, band [0.20,0.70], τ=0.12, ages [35,45])
- Geometry: off for this A/B (can be re-enabled later if needed)
- EMA: decoder, eval swap on
- Other regs: adaptive_w_norm_lambda=20, w_norm_lambda=0.003, cycle_lambda=1.5, crop LPIPS/L2 enabled
- Duration: S1→40k; S2 resume →55k (early stop near 45–50k if plateau)

Stage 1 — encoder to 40k steps (extended ROIs + mid-boost schedule)
```bash
bash -lc "cd /home/wczub/RND/AI_DEAGING/repos/mytimemachine && \
  source $(conda info --base)/etc/profile.d/conda.sh && conda activate mytimemachine && \
  PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python python scripts/train.py \
  --coach orig_nn \
  --dataset_type ffhq_aging --train_dataset data/train \
  --exp_dir experiments/full_training_run \
  --start_from_encoded_w_plus --train_encoder \
  --batch_size 2 --workers 2 \
  --target_age uniform_random \
  --max_steps 40000 \
  --id_lambda 0.3 --lpips_lambda 0.1 --lpips_lambda_aging 0.1 --lpips_lambda_crop 0.8 \
  --l2_lambda 0.1 --l2_lambda_aging 0.25 --l2_lambda_crop 0.5 \
  --w_norm_lambda 0.003 --aging_lambda 5 --cycle_lambda 1.5 \
  --adaptive_w_norm_lambda 20 \
  --nearest_neighbor_id_loss_lambda 0.10 \
  --contrastive_id_lambda 0.04 \
  --mb_index_path banks/ffhq_ir50_age_5y.pt --mb_use_faiss \
  --mb_top_m 512 --mb_k 64 --mb_min_sim 0.20 --mb_max_sim 0.70 \
  --mb_apply_min_age 35 --mb_apply_max_age 45 --mb_temperature 0.12 \
  --extrapolation_start_step 1000000000 \
  --roi_id_lambda 0.05 --roi_use_eyes --roi_use_mouth --roi_use_nose --roi_use_broweyes \
  --roi_size 112 --roi_pad 0.35 --roi_jitter 0.06 \
  --roi_landmarks_model pretrained_models/shape_predictor_68_face_landmarks.dat \
  --roi_id_schedule_s1 \"0:0.03,10000:0.05,20000:0.07,36000:0.05\" \
  --ema --ema_scope decoder --ema_decay 0.999 --eval_with_ema \
  --val_deterministic --val_max_batches 2 --val_interval 500 \
  --warmup_steps 500 --min_lr 5e-7 \
  --seed 123 | cat"
```

Stage 2 — decoder resume to 55k steps (extended ROIs + EMA)
```bash
bash -lc "cd /home/wczub/RND/AI_DEAGING/repos/mytimemachine && \
  source $(conda info --base)/etc/profile.d/conda.sh && conda activate mytimemachine && \
  PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python python scripts/train.py \
  --coach orig_nn \
  --dataset_type ffhq_aging --train_dataset data/train \
  --exp_dir experiments/full_training_run \
  --start_from_encoded_w_plus --train_decoder \
  --batch_size 2 --workers 2 \
  --target_age uniform_random \
  --max_steps 55000 --learning_rate 3e-5 \
  --id_lambda 0.3 --lpips_lambda 0.1 --lpips_lambda_aging 0.1 --lpips_lambda_crop 0.8 \
  --l2_lambda 0.1 --l2_lambda_aging 0.25 --l2_lambda_crop 0.5 \
  --w_norm_lambda 0.003 --w_norm_lambda_decoder_scale 0.5 \
  --aging_lambda 5 --aging_lambda_decoder_scale 0.5 \
  --cycle_lambda 1.5 \
  --adaptive_w_norm_lambda 20 \
  --nearest_neighbor_id_loss_lambda 0.05 \
  --contrastive_id_lambda 0.025 \
  --mb_index_path banks/ffhq_ir50_age_5y.pt --mb_use_faiss \
  --mb_top_m 768 --mb_k 64 --mb_min_sim 0.20 --mb_max_sim 0.70 \
  --mb_apply_min_age 35 --mb_apply_max_age 45 --mb_temperature 0.12 \
  --roi_id_lambda_s2 0.04 --roi_use_eyes --roi_use_mouth --roi_use_nose --roi_use_broweyes \
  --roi_size 112 --roi_pad 0.35 --roi_jitter 0.06 \
  --roi_landmarks_model pretrained_models/shape_predictor_68_face_landmarks.dat \
  --ema --ema_scope decoder --ema_decay 0.999 --eval_with_ema \
  --val_deterministic --val_max_batches 2 --val_interval 500 \
  --warmup_steps 500 --min_lr 5e-7 \
  --seed 123 \
  --resume_checkpoint experiments/full_training_run/000XX/checkpoints/iteration_40000.pt | cat"
```

Monitoring & acceptance (extended ROIs):
- Expect `train/roi_pairs_nose` and `train/roi_pairs_broweyes` > 0 alongside eyes/mouth; `loss_roi_id` finite and stable.
- Mid-boost visible: `train/roi_id_lambda_current` traverses 0.05→0.07→0.05 around 0/20k/36k; S2 shows `0.05` flat.
- Identity/perceptual similar to Eleventh/Twelfth; watch for over‑sharpening in eyes/brows; dial mid-boost to 0.06–0.07 if needed.

Final report

Scope and goal
- Strengthen micro‑identity retention by extending ROI‑ID beyond eyes+mouth to include nose and a combined brow+eyes region, keeping the stable stack (EMA eval, mid‑boost ROI schedule, FAISS miner, NN‑ID, contrastive impostor). Extrapolation disabled (in‑domain).

What changed vs strong prior baselines (Eleventh–Fourteenth)
- ROI‑ID regions: eyes + mouth → eyes + mouth + nose + brow/eyes (new)
- ROI schedule S1: 0:0.03 → 10k:0.05 → 20k:0.07 → 36k:0.05 (slightly lower start to accommodate added ROIs)
- Miner: FAISS custom, k=64, top_m=512, band [0.20,0.70], τ=0.12, ages [35,45]
- S2 LR set to 2e‑5 in this run (some prior strong S2 runs used 3e‑5)

Stage 1 — 00068 (encoder+decoder, 0→40k)
- Config (opt.json): id_lambda=0.3, lpips_crop=0.8, l2=0.1, w_norm=0.003, aging=5, cycle=1.5, NN‑ID λ=0.10, contrastive λ=0.04, FAISS: k=64/top_m=512/band[0.20,0.70]/τ=0.12 (ages 35–45), ROI: eyes+mouth+nose+broweyes, schedule 0.03→0.05 (10k)→0.07 (20k)→0.05 (36k), EMA eval on.
- Timestamp highlights (checkpoints/timestamp.txt):
  - Best S1 total: 1.060 at step 19.5k.
  - Heads at/before best windows (≈18.5k–19.5k):
    - loss_id_real ≈ 0.299–0.315; lpips_real ≈ 0.291–0.293; l2_real ≈ 0.046–0.049; w_norm_real ≈ 41–42.
    - loss_lpips_crop ≈ 0.166–0.213; l2_crop ≈ 0.048–0.078; loss_real ≈ 0.658–0.689.
  - ROI evidence: roi_pairs_eyes/mouth/nose/broweyes=2 each consistently; roi_landmark_failures=0; loss_roi_id trended from ≈0.47 early to ≈0.34–0.36 mid/late.
  - ROI schedule executed as planned: roi_lambda_current 0.03 (≤10k) → 0.05 (10k→20k) → 0.07 (20k→36k) → 0.05 (≥36k).
  - Miner diagnostics (“mb:” lines): prof=custom, k_effective=64; candidate_count typically 40–60 early, trending toward ~35–50 mid/late; simμ ≈ 0.231–0.257; mb_applied_ratio ≈ 0.75 when batch ages in-window.

Stage 2 — 00070 (decoder‑only, 40k→55k)
- Config (opt.json): resume=00068/iteration_40000.pt; train_decoder only; LR=2e‑5; id_lambda=0.3; NN‑ID λ=0.05; contrastive λ=0.025; ROI‑ID S2 λ=0.04 (eyes+mouth+nose+broweyes); w_norm_decoder_scale=0.5; EMA eval on; save_interval=2k.
- Timestamp highlights (checkpoints/timestamp.txt):
  - Best S2 total: 0.8356 at step 50.5k (earlier bests: 0.845@40k → 0.843@41k → 0.841@41.5k → 0.840@43.5k → 0.837@47k).
  - Heads near best (50.5k): loss_id_real ≈ 0.328; lpips_real ≈ 0.297; l2_real ≈ 0.046; loss_real ≈ 0.480; w_norm_real ≈ 37.1; used_ema=1; roi_lambda_current=0.04.
  - Late S2 window (≥49k) clustered ~0.836–0.849; plateau after ~50k.

Comparisons vs previous trainings (quantitative)
- vs Tenth (FAISS + ROI eyes/mouth): best S2 ≈ 0.809 → Fifteenth S2 ≈ 0.836 (higher total).
- vs Eleventh (EMA + ROI schedule): best S2 ≈ 0.807–0.809 → Fifteenth ≈ 0.836.
- vs Twelfth (soft‑0.60 miner + diagnostics): best S2 ≈ 0.8006 → Fifteenth ≈ 0.836.
- vs Fourteenth (S1 anchors; S2 anchors off): best S2 ≈ 0.804 → Fifteenth ≈ 0.836.
- Notes on comparability: Fifteenth adds two extra ROI regions (nose, brow/eyes) and used S2 LR 2e‑5 (lower than 3e‑5 used in several prior bests). Added ROI terms and lower LR likely increased totals and limited S2 tightening. Despite higher totals, regularization is excellent (w_norm_real ≈ 37 late S2, matching/bettering earlier runs).

Qualitative expectations (micro‑identity)
- Extended ROIs should preferentially sharpen nose shape and eyebrow/eye structure that global ID metrics under‑weight. Quantitative totals alone may not reflect these micro‑identity gains; side‑by‑side inspection is recommended.

Visual inspection
- Altough quantitative totals are worse, the output images looks tiny-bit better with this training

Takeaways
- S1 behaved smoothly with the extended ROIs and schedule; convergence similar to recent runs, best at 19.5k, stable through 40k.
- S2 achieved stable low w_norm_real (~37) and modest tightening but underperformed prior best totals, likely due to (a) extra ROI heads and (b) LR=2e‑5.

Recommendations
- For parity with prior bests, raise S2 LR to 3e‑5 and keep EMA; keep ROI‑ID S2 at 0.04–0.05.
- Consider miner soft profile (k=48, band [0.25,0.60]) used in Twelfth to reduce over‑hard negatives and totals; keep k_effective diagnostic lines.
- Keep extended ROIs in S1; if over‑sharpening appears, cap the mid‑boost at 0.06.
- Align contrastive λ in S2 to 0.02 (used in several best runs) to reduce canonicalization pressure.
- Judge final adoption by visual A/B on eyes, nose, and brow fidelity; totals may remain slightly higher with extended ROIs.

Artifacts present (useful breadcrumbs)
- `miner_profile.txt`: custom
- `timestamp.txt`: frequent “mb:” diagnostics with k_effective=64; roi_pairs_* show all four regions active; `used_ema=1` markers present in both stages.


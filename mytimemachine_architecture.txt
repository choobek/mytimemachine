MyTimeMachine architecture (current). An input face is encoded into StyleGAN’s latent space (W/W+) and passed through a Global Aging Prior (pretrained SAM) to obtain a coarse age-transformed latent. A Personalized Adapter ("blender") refines this latent using user-specific cues. A StyleGAN2 generator (Rosinality, 1024×1024) decodes the refined latent to the final image. Training jointly optimizes identity, age, perceptual, and pixel objectives with regularization to preserve identity while enabling age edits.

1) High-level components
- Encoder (pSp; `models/psp.py`, `models/psp_gp.py`) to embed real images into W/W+.
- Global Aging Prior (SAM; checkpoint `pretrained_models/sam_ffhq_aging.pt`) to propose a coarse age shift in latent space.
- Personalized Adapter (blender; `models/blender.py`) to refine the SAM output with user-specific aging traits.
- Generator (StyleGAN2; `models/stylegan2/`) to synthesize the 1024×1024 image.
- Auxiliary networks: IR-SE50 (identity; `pretrained_models/model_ir_se50.pth`), DEX/VGG age classifier (`pretrained_models/dex_age_classifier.pth`).

2) Data and preprocessing
Faces are enhanced (GFPGAN/Real-ESRGAN as available), filtered, and aligned via Dlib 68-landmark predictor (`pretrained_models/shape_predictor_68_face_landmarks.dat`). Datasets are organized by age (e.g., `{age}_{idx}.jpg`). PyTorch datasets (`datasets/*`) load images and apply normalization/augmentations. Training uses uniform-random target ages unless otherwise specified.

3) Training flow (two-stage)
- Stage 1 (encoder+decoder): start from encoded W+ (`--start_from_encoded_w_plus`) and jointly train encoder+adapter+decoder for 30k–40k steps to establish identity and aging behavior.
- Stage 2 (decoder-only): resume from Stage 1 checkpoint and train the decoder/adapter for ~45k–60k total steps (often ~15k–25k more) for gentle refinement with lower LR. Some loss weights may use decoder-only scales (e.g., `--w_norm_lambda_decoder_scale`, `--aging_lambda_decoder_scale`).
Common: cosine LR with warmup, gradient clipping, NaN-guard; extrapolation is disabled in current stable setups (set a very large `--extrapolation_start_step`) to remain in-domain.
Additionally: EMA on decoder weights is enabled for evaluation/checkpointing (`--ema --ema_scope decoder --ema_decay 0.999 --eval_with_ema`) and persists across resumes. In Stage 1, ROI‑ID λ follows a mid‑boost schedule (e.g., 0→20k: 0.05; 20k→36k: 0.07; then back to 0.05), while Stage 2 uses a fixed ROI‑ID λ ≈0.05.

4) Losses and regularization (current set)
- Global ID loss (IR-SE50): encourages identity preservation on full images; weighted by `--id_lambda`.
- Aging loss (DEX/VGG): encourages target age correctness; weighted by `--aging_lambda`.
- Perceptual LPIPS + pixel L2: computed on full image and cropped face region; weighted by `--lpips_lambda`, `--lpips_lambda_crop`, `--l2_lambda`, `--l2_lambda_crop`.
- Cycle consistency: bidirectional consistency between input and edited images; `--cycle_lambda`.
- Adaptive W-norm: regularizes latent shifts adaptively relative to SAM prior; `--adaptive_w_norm_lambda` (typically ≈20).
- Nearest‑Neighbor ID (NN‑ID): interpolation-only regularizer that pulls toward nearest same‑identity exemplars in the training set embedding space; `--nearest_neighbor_id_loss_lambda` (e.g., 0.05–0.2 depending on stage).
- Age‑aware Contrastive ID (impostor bank): repels from different‑identity embeddings drawn from an age‑binned FFHQ bank within an application window; `--contrastive_id_lambda` (e.g., 0.02–0.04). Age window and temperature are configurable.
- ROI‑ID (eyes + mouth): identity preservation on discriminative regions. ROIs are extracted via Dlib landmarks; loss computed on cropped/jittered regions; `--roi_id_lambda` with `--roi_use_eyes`/`--roi_use_mouth`.

5) Impostor bank and mining (age-aware, FAISS optional)
- Bank: IR-SE50 embeddings of FFHQ faces binned by 5-year ages; path `banks/ffhq_ir50_age_5y.pt`.
- Basic sampling: choose K impostors (`--mb_k`) from bins that fall within an age window (`--mb_apply_min_age/--mb_apply_max_age`), optionally with neighbor radius per age bin.
- Contrastive details: temperature `--mb_temperature` (e.g., 0.12). The loss is active only when target ages land inside the configured age window (e.g., 35–45), reflected by an applied-ratio metric.
- FAISS semi‑hard mining (optional): enable `--mb_use_faiss` to retrieve top-M candidates (`--mb_top_m`) and filter by similarity band (`--mb_min_sim`, `--mb_max_sim`) before sampling K negatives. This focuses on semi-hard impostors to improve identity discrimination without over‑canonicalization.
Implementation pointers: `training/impostor_bank.py` (bank access), `training/impostor_faiss.py` (mining), `tools/build_agebank.py` (bank utilities).
Twelfth training (soft miner) recommended profile: enable FAISS with a softer band and larger pre‑pool, e.g., `mb_k≈48`, `mb_top_m≈768`, band `[0.25, 0.60]`, `mb_temperature≈0.12`, age window `[35,45]`. Miner diagnostics (candidate_count after banding, similarity mean/std and percentiles, k_effective, band_min/max) are logged to TensorBoard and appended to `experiments/**/checkpoints/timestamp.txt` at validation.

6) ROI‑ID details (eyes + mouth)
ROIs are derived from 68-point Dlib landmarks. Typical settings: `--roi_size 112`, `--roi_pad 0.35`, `--roi_jitter 0.06` with eyes and mouth enabled. This loss emphasizes micro-identity (eye shape, inter‑ocular geometry, mouth corners/asymmetry) and complements global ID. Implementation: `training/roi_crops.py`.

7) Default/stable training ranges (reference)
- Stage 1: `id_lambda≈0.3–0.35`, `lpips_lambda_crop≈0.8–0.9`, `l2_lambda≈0.08–0.10`, `l2_lambda_crop≈0.4–0.5`, `cycle_lambda≈1.5`, `adaptive_w_norm_lambda≈20`, `nearest_neighbor_id_loss_lambda≈0.1–0.2`, `contrastive_id_lambda≈0.03–0.04`, FAISS optional (`mb_k 32–64`, `mb_top_m 512`, `mb_min_sim 0.20`, `mb_max_sim 0.70`, `mb_temperature 0.12`).
- Stage 2: lower LR (e.g., `3e-5`), often `id_lambda≈0.3`, `nearest_neighbor_id_loss_lambda≈0.05`, `contrastive_id_lambda≈0.02`, with decoder scales such as `w_norm_lambda_decoder_scale≈0.5`, `aging_lambda_decoder_scale≈0.5`.
- Regularization behavior in stable runs: `loss_w_norm_real` typically settles in low‑40s; identity/perceptual heads tighten through late S1 and early S2.
EMA and ROI‑ID defaults: enable decoder EMA for eval/checkpoints (`--ema`, decay≈0.999). Use an S1 ROI‑ID λ schedule with a mid‑run boost (e.g., 0.05→0.07→0.05) and keep S2 fixed at ≈0.05. For FAISS soft miner, a narrower upper band (≈0.60) with `mb_k≈48` and `mb_top_m≈768` is a stable choice.

8) CLI and scripting
Training is driven by `scripts/train.py` with flags shown above. Two-stage automation is available in `scripts/train_two_stage.sh` (Stage 1 → auto‑resume Stage 2). Validation is deterministic with small caps for speed (`--val_interval`, `--val_deterministic`, `--val_max_batches`, optional `--val_disable_aging`). All experiments write under `experiments/full_training_run/**` with checkpoints and TensorBoard logs.

9) Summary
The current system augments the original SAM+Adapter+StyleGAN pipeline with identity‑focused additions: (a) NN‑ID on interpolation to pull toward the correct identity manifold, (b) an age‑aware contrastive objective against an FFHQ impostor bank with FAISS mining (including a soft‑band option and miner diagnostics), (c) ROI‑ID over eyes and mouth guided by landmarks with a Stage‑1 mid‑boost schedule, and (d) EMA‑based decoder evaluation for stable checkpoints. Combined with adaptive W‑norm and balanced perceptual/pixel losses, these upgrades improve personalization and micro‑identity fidelity while maintaining stable, artifact‑free aging edits; compact metrics (including miner diagnostics) are logged to TensorBoard and `checkpoints/timestamp.txt`, with full configs in `opt.json` for reproducibility.
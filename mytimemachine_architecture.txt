MyTimeMachine architecture (current). An input face is encoded into StyleGAN’s latent space (W/W+) and passed through a Global Aging Prior (pretrained SAM) to obtain a coarse age-transformed latent. A Personalized Adapter ("blender") refines this latent using user-specific cues. A StyleGAN2 generator (Rosinality, 1024×1024) decodes the refined latent to the final image. Training jointly optimizes identity, age, perceptual, and pixel objectives with regularization to preserve identity while enabling age edits.

1) High-level components
- Encoder (pSp; `models/psp.py`, `models/psp_gp.py`) to embed real images into W/W+.
- Global Aging Prior (SAM; checkpoint `pretrained_models/sam_ffhq_aging.pt`) to propose a coarse age shift in latent space.
- Personalized Adapter (blender; `models/blender.py`) to refine the SAM output with user-specific aging traits.
- Generator (StyleGAN2; `models/stylegan2/`) to synthesize the 1024×1024 image.
- Auxiliary networks: IR-SE50 (identity; `pretrained_models/model_ir_se50.pth`), DEX/VGG age classifier (`pretrained_models/dex_age_classifier.pth`).

2) Data and preprocessing
Faces are enhanced (GFPGAN/Real-ESRGAN as available), filtered, and aligned via Dlib 68-landmark predictor (`pretrained_models/shape_predictor_68_face_landmarks.dat`). Datasets are organized by age (e.g., `{age}_{idx}.jpg`). PyTorch datasets (`datasets/*`) load images and apply normalization/augmentations. Training uses uniform-random target ages unless otherwise specified.

3) Training flow (two-stage)
- Stage 1 (encoder+decoder): start from encoded W+ (`--start_from_encoded_w_plus`) and jointly train encoder+adapter+decoder for 30k–40k steps to establish identity and aging behavior.
- Stage 2 (decoder-only): resume from Stage 1 checkpoint and train the decoder/adapter for ~45k–60k total steps (often ~15k–25k more) for gentle refinement with lower LR. Some loss weights may use decoder-only scales (e.g., `--w_norm_lambda_decoder_scale`, `--aging_lambda_decoder_scale`).
Common: cosine LR with warmup, gradient clipping, NaN-guard; extrapolation is disabled in current stable setups (set a very large `--extrapolation_start_step`) to remain in-domain.

4) Losses and regularization (current set)
- Global ID loss (IR-SE50): encourages identity preservation on full images; weighted by `--id_lambda`.
- Aging loss (DEX/VGG): encourages target age correctness; weighted by `--aging_lambda`.
- Perceptual LPIPS + pixel L2: computed on full image and cropped face region; weighted by `--lpips_lambda`, `--lpips_lambda_crop`, `--l2_lambda`, `--l2_lambda_crop`.
- Cycle consistency: bidirectional consistency between input and edited images; `--cycle_lambda`.
- Adaptive W-norm: regularizes latent shifts adaptively relative to SAM prior; `--adaptive_w_norm_lambda` (typically ≈20).
- Nearest‑Neighbor ID (NN‑ID): interpolation-only regularizer that pulls toward nearest same‑identity exemplars in the training set embedding space; `--nearest_neighbor_id_loss_lambda` (e.g., 0.05–0.2 depending on stage).
- Age‑aware Contrastive ID (impostor bank): repels from different‑identity embeddings drawn from an age‑binned FFHQ bank within an application window; `--contrastive_id_lambda` (e.g., 0.02–0.04). Age window and temperature are configurable.
- ROI‑ID (eyes + mouth): identity preservation on discriminative regions. ROIs are extracted via Dlib landmarks; loss computed on cropped/jittered regions; `--roi_id_lambda` with `--roi_use_eyes`/`--roi_use_mouth`.

5) Impostor bank and mining (age-aware, FAISS optional)
- Bank: IR-SE50 embeddings of FFHQ faces binned by 5-year ages; path `banks/ffhq_ir50_age_5y.pt`.
- Basic sampling: choose K impostors (`--mb_k`) from bins that fall within an age window (`--mb_apply_min_age/--mb_apply_max_age`), optionally with neighbor radius per age bin.
- Contrastive details: temperature `--mb_temperature` (e.g., 0.12). The loss is active only when target ages land inside the configured age window (e.g., 35–45), reflected by an applied-ratio metric.
- FAISS semi‑hard mining (optional): enable `--mb_use_faiss` to retrieve top-M candidates (`--mb_top_m`) and filter by similarity band (`--mb_min_sim`, `--mb_max_sim`) before sampling K negatives. This focuses on semi-hard impostors to improve identity discrimination without over‑canonicalization.
Implementation pointers: `training/impostor_bank.py` (bank access), `training/impostor_faiss.py` (mining), `tools/build_agebank.py` (bank utilities).

6) ROI‑ID details (eyes + mouth)
ROIs are derived from 68-point Dlib landmarks. Typical settings: `--roi_size 112`, `--roi_pad 0.35`, `--roi_jitter 0.06` with eyes and mouth enabled. This loss emphasizes micro-identity (eye shape, inter‑ocular geometry, mouth corners/asymmetry) and complements global ID. Implementation: `training/roi_crops.py`.

7) Default/stable training ranges (reference)
- Stage 1: `id_lambda≈0.3–0.35`, `lpips_lambda_crop≈0.8–0.9`, `l2_lambda≈0.08–0.10`, `l2_lambda_crop≈0.4–0.5`, `cycle_lambda≈1.5`, `adaptive_w_norm_lambda≈20`, `nearest_neighbor_id_loss_lambda≈0.1–0.2`, `contrastive_id_lambda≈0.03–0.04`, FAISS optional (`mb_k 32–64`, `mb_top_m 512`, `mb_min_sim 0.20`, `mb_max_sim 0.70`, `mb_temperature 0.12`).
- Stage 2: lower LR (e.g., `3e-5`), often `id_lambda≈0.3`, `nearest_neighbor_id_loss_lambda≈0.05`, `contrastive_id_lambda≈0.02`, with decoder scales such as `w_norm_lambda_decoder_scale≈0.5`, `aging_lambda_decoder_scale≈0.5`.
- Regularization behavior in stable runs: `loss_w_norm_real` typically settles in low‑40s; identity/perceptual heads tighten through late S1 and early S2.

8) CLI and scripting
Training is driven by `scripts/train.py` with flags shown above. Two-stage automation is available in `scripts/train_two_stage.sh` (Stage 1 → auto‑resume Stage 2). Validation is deterministic with small caps for speed (`--val_interval`, `--val_deterministic`, `--val_max_batches`, optional `--val_disable_aging`). All experiments write under `experiments/full_training_run/**` with checkpoints and TensorBoard logs.

9) Summary
The current system augments the original SAM+Adapter+StyleGAN pipeline with three identity‑focused additions: (a) NN‑ID on interpolation to pull toward the correct identity manifold, (b) an age‑aware contrastive objective against an FFHQ impostor bank with optional FAISS semi‑hard mining, and (c) ROI‑ID over eyes and mouth guided by landmarks. Combined with adaptive W‑norm and balanced perceptual/pixel losses, these upgrades improve personalization and micro‑identity fidelity while maintaining stable, artifact‑free aging edits.
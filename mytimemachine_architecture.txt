MyTimeMachine architecture. A face image is first encoded into a StyleGAN latent space and passed through a Global Aging Prior (the pretrained SAM model) to predict an age-changed latent. A Personalized Adapter Network then refines this latent using user-specific features. Finally, a StyleGAN2 generator decodes the latent into the output face. The model is trained with multiple losses (identity, age, perceptual, etc.) to preserve the person’s identity while editing age. 1. High-Level Architecture and Main Components MyTimeMachine combines a pretrained global aging model with a personalized adapter network and a StyleGAN2 generator. In practice, an input face is embedded into StyleGAN’s latent space (often via a pSp encoder). A global aging prior (the SAM model trained on FFHQ) then applies a coarse age transformation to the latent. The Adapter Network (implemented in models/blender.py) refines this latent to incorporate personal aging cues from the user’s photo collection. The refined latent is fed into the StyleGAN2 generator (the 1024×1024 Rosinality implementation ) to synthesize the final image. Key components include: the SAM model as the global prior, the pSp inversion encoder, the StyleGAN generator, plus auxiliary networks for training (a VGG-based age classifier and an IR-SE50 face-recognition model for identity loss). 2. Training Pipeline and Data Flow Preprocessing: Raw face images go through an enhancement and alignment pipeline. Faces are upscaled and restored with GFPGAN and Real-ESRGAN, low-quality images are filtered (via MyStyle), and all faces are aligned using Dlib (shape_predictor). Dataset Loading: The aligned images are organized by age labels (filename format {age}_{idx}.jpg github.com ). A PyTorch Dataset loads image pairs (or single images with target ages) and applies necessary transforms (normalization, random augmentations). Forward Pass: During training (scripts/train.py), each batch of images is encoded into latent codes. A random target age is sampled (e.g. --target_age=uniform_random github.com ), and the latent is first processed by the SAM global prior and then by the Adapter network. The StyleGAN2 generator decodes this latent to produce an output image at the target age. Loss Computation: The output is compared against the ground truth (or input) using multiple loss terms. Identity preservation is enforced with the pretrained IR-SE50 model, and age accuracy is enforced with the Dex-VGG age classifier. Perceptual similarity (LPIPS) and pixel-level (L2) losses are also applied. A cycle-consistency loss (--cycle_lambda=1 ) ensures bidirectional age editing is consistent. Specialized losses include the adaptive W-norm regularizer: a weighted norm on the latent shift that keeps the edit sensitive but not overfit . The training script supports many hyperparameters (e.g. --id_lambda, --lpips_lambda, --aging_lambda, etc.) as shown in the example command github.com github.com . Gradients flow back through the Adapter and (optionally) fine-tuned encoder; optimizers like Adam or the custom Ranger optimizer (training/ranger.py) update the model weights. 3. Neural Network Architecture and Objectives The StyleGAN2 generator is the core image decoder: it consists of a mapping network (fully-connected layers to produce style vectors in W+ space) and a synthesis network of convolutional layers with adaptive instance normalization, progressively upsampling to 1024×1024 resolution . The Adapter Network (in models/blender.py) is a trainable module that adjusts the StyleGAN latent code; while its exact layer configuration isn’t documented in the code, it likely uses convolutional or fully-connected layers to predict latent offsets conditioned on age. The PSP encoder (models/psp.py, models/psp_gp.py) is the pixel2style2pixel architecture for embedding real images into the latent space github.com . For losses, the model uses: (1) Identity loss via the IR-SE50 network (ResNet50 backbone from the InsightFace repository) ; (2) Aging loss via a fine-tuned VGG age-classifier (from the DEX model); (3) Perceptual LPIPS loss and pixel L2 loss (enabled by the lpips library) ; (4) Cycle consistency loss, and (5) the adaptive W-norm regularizer (lambda controlled by --adaptive_w_norm_lambda) which penalizes large latent changes beyond the global prior. All these losses are weighted by tunable lambdas (e.g. --id_lambda, --aging_lambda, etc. as shown in the training command ).
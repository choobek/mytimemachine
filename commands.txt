30000steps training (encoder_enabled):
bash -lc "conda activate mytimemachine && python scripts/train.py --dataset_type ffhq_aging --workers 2 --batch_size 2 --test_batch_size 2 --test_workers 2 --val_interval 500 --save_interval 1000 --start_from_encoded_w_plus --id_lambda 0.1 --lpips_lambda 0.1 --lpips_lambda_aging 0.1 --lpips_lambda_crop 0.6 --l2_lambda 0.25 --l2_lambda_aging 0.25 --l2_lambda_crop 1 --w_norm_lambda 0.005 --aging_lambda 5 --cycle_lambda 1 --input_nc 4 --target_age uniform_random --use_weighted_id_loss --checkpoint_path pretrained_models/sam_ffhq_aging.pt --train_dataset data/train --exp_dir experiments/full_training_run --adaptive_w_norm_lambda 7 --scheduler_type cosine --warmup_steps 500 --min_lr 1e-6 --grad_clip_norm 1.0 --nan_guard --extrapolation_start_step 3000 --extrapolation_prob_start 0.0 --extrapolation_prob_end 0.5 --train_encoder --max_steps 30000 | cat"
Launching the 35000-step decoder-only resume
bash -lc "cd /home/wczub/RND/AI_DEAGING/repos/mytimemachine && conda activate mytimemachine && PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python python scripts/train.py --dataset_type ffhq_aging --workers 2 --batch_size 2 --test_batch_size 2 --test_workers 2 --val_interval 500 --save_interval 1000 --start_from_encoded_w_plus --id_lambda 0.1 --lpips_lambda 0.1 --lpips_lambda_aging 0.1 --lpips_lambda_crop 0.6 --l2_lambda 0.25 --l2_lambda_aging 0.25 --l2_lambda_crop 1 --w_norm_lambda 0.005 --aging_lambda 5 --cycle_lambda 1 --input_nc 4 --target_age uniform_random --use_weighted_id_loss --checkpoint_path pretrained_models/sam_ffhq_aging.pt --train_dataset data/train --exp_dir experiments/full_training_run --adaptive_w_norm_lambda 7 --scheduler_type cosine --warmup_steps 500 --min_lr 1e-6 --grad_clip_norm 1.0 --nan_guard --resume_checkpoint experiments/full_training_run/00014/checkpoints/iteration_28000.pt --train_decoder --max_steps 35000 --learning_rate 0.0001"